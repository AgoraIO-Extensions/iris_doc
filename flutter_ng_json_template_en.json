[
    {
        "id": "enum_earmonitoringfiltertype",
        "name": "EarMonitoringFilterType",
        "description": "The audio filter of in-ear monitoring.\n",
        "parameters": [
            {
                "earMonitoringFilterNone": "1: Do not add an audio filter to the in-ear monitor.\n "
            },
            {
                "earMonitoringFilterBuiltInAudioFilters": "2: Add an audio filter to the in-ear monitor.\nIf you implement functions such as voice beautifier and audio effect, users can hear the voice after adding these effects.\n "
            },
            {
                "earMonitoringFilterNoiseSuppression": "4: Enable noise suppression to the in-ear monitor.\n "
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "api_stoplastmileprobetest",
        "name": "stopLastmileProbeTest",
        "description": "Stops the last mile network probe test.\n",
        "parameters": [],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "callback_onfirstremotevideoframe",
        "name": "onFirstRemoteVideoFrame",
        "description": "Occurs when the renderer receives the first frame of the remote video.\n",
        "parameters": [
            {
                "uid": "The ID of the remote user sending the video stream."
            },
            {
                "connection": "The connection information. See RtcConnection ."
            },
            {
                "width": "The width (px) of the video stream."
            },
            {
                "height": "The height (px) of the video stream."
            },
            {
                "elapsed": "The time elapsed (ms) from the local user calling joinChannelWithOptions until the SDK triggers this callback."
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "enum_rawaudioframeopmodetype",
        "name": "RawAudioFrameOpModeType",
        "description": "The use mode of the audio data.\n",
        "parameters": [
            {
                "rawAudioFrameOpModeReadOnly": "0: Read-only mode: "
            },
            {
                "rawAudioFrameOpModeReadWrite": "2: Read and write mode: "
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "enum_videosourcetype",
        "name": "VideoSourceType",
        "description": "The capture type of the custom video source.\n",
        "parameters": [
            {
                " ": "(Default) The primary camera."
            },
            {
                "videoSourceCamera": "The camera."
            },
            {
                "videoSourceCameraSecondary": "The secondary camera."
            },
            {
                "videoSourceScreenPrimary": "The primary screen."
            },
            {
                "videoSourceScreen": "The screen."
            },
            {
                "videoSourceScreenSecondary": "The secondary screen."
            },
            {
                "videoSourceCustom": "The custom video source."
            },
            {
                "videoSourceMediaPlayer": "The video source from the media player."
            },
            {
                "videoSourceRtcImagePng": "The video source is a PNG image."
            },
            {
                "videoSourceRtcImageJpeg": "The video source is a JPEG image."
            },
            {
                "videoSourceRtcImageGif": "The video source is a GIF image."
            },
            {
                "videoSourceRemote": "The video source is remote video acquired by the network."
            },
            {
                "videoSourceTranscoded": "A transcoded video source."
            },
            {
                "videoSourceUnknown": "An unknown video source."
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "api_createagorartcengine",
        "name": "createAgoraRtcEngine",
        "description": "Creates the RtcEngineEx object.\nCurrently, the Agora RTC SDK v4.0.0 supports creating only one RtcEngineEx object for an app.",
        "parameters": [],
        "returns": "RtcEngineEx object.",
        "is_hide": false
    },
    {
        "id": "api_configrhythmplayer",
        "name": "configRhythmPlayer",
        "description": "Configures the virtual metronome.\nThis method is for Android and iOS only.\n After enabling the virtual metronome, the SDK plays the specified audio effect file from the beginning, and controls the playback duration of each file according to beatsPerMinute you set in AgoraRhythmPlayerConfig . For example, if you set beatsPerMinute as 60, the SDK plays one beat every second. If the file duration exceeds the beat duration, the SDK only plays the audio within the beat duration.\n After calling startRhythmPlayer , you can call this method to reconfigure the virtual metronome.",
        "parameters": [
            {
                "config": "The metronome configuration. See AgoraRhythmPlayerConfig ."
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "callback_onlocaluserregistered",
        "name": "onLocalUserRegistered",
        "description": "Occurs when the local user registers a user account.\nAfter the local user successfully calls registerLocalUserAccount to register the user account or calls joinChannelWithUserAccount to join a channel, the SDK triggers the callback and informs the local user's UID and User Account.",
        "parameters": [
            {
                "uid": "The ID of the local user."
            },
            {
                "userAccount": "The user account of the local user."
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "callback_onchannelmediarelayevent",
        "name": "onChannelMediaRelayEvent",
        "description": "Reports events during the media stream relay.\n",
        "parameters": [
            {
                "code": "The event code of channel media relay. See ChannelMediaRelayEvent .\n "
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "enum_connectionstatetype",
        "name": "ConnectionStateType",
        "description": "Connection states.\n",
        "parameters": [
            {
                "connectionStateDisconnected": "1: The SDK is disconnected from the Agora edge server. The state indicates the SDK is in one of the following phases:\n Theinitial state before calling the joinChannelWithOptions method.\n The app calls the leaveChannel method. "
            },
            {
                "connectionStateConnecting": "2: The SDK is connecting to the Agora edge server. This state indicates that the SDK is establishing a connection with the specified channel after the app calls joinChannelWithOptions.\n If the SDK successfully joins the channel, it triggers the onConnectionStateChanged callback and the connection state switches to connectionStateConnected.\n After the connection is established, the SDK also initializes the media and triggers onJoinChannelSuccess when everything is ready. "
            },
            {
                "connectionStateConnected": "3: The SDK is connected to the Agora edge server. This state also indicates that the user has joined a channel and can now publish or subscribe to a media stream in the channel. If the connection to the channel is lost because, for example, if the network is down or switched, the SDK automatically tries to reconnect and triggers: \n onConnectionStateChanged callback, notifying that the current network state becomes connectionStateReconnecting."
            },
            {
                "connectionStateReconnecting": "4: The SDK keeps reconnecting to the Agora edge server. The SDK keeps rejoining the channel after being disconnected from a joined channel because of network issues.\n If the SDK cannot rejoin the channel within 10 seconds, it triggers onConnectionLost , stays in the connectionStateReconnecting state, and keeps rejoining the channel.\n If the SDK fails to rejoin the channel 20 minutes after being disconnected from the Agora edge server, the SDK triggers the onConnectionStateChanged callback, switches to the connectionStateFailed state, and stops rejoining the channel. "
            },
            {
                "connectionStateFailed": "5: The SDK fails to connect to the Agora edge server or join the channel. This state indicates that the SDK stops trying to rejoin the channel. You must call leaveChannel to leave the channel.\n You can call joinChannelWithOptions to rejoin the channel.\n If the SDK is banned from joining the channel by the Agora edge server through the RESTful API, the SDK triggers the onConnectionStateChanged callback. "
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "api_setcamerazoomfactor",
        "name": "setCameraZoomFactor",
        "description": "Set the camera zoom ratio.\nThis method is for Android and iOS only.\n Call this method before calling joinChannelWithOptions , enableVideo , or enableLocalVideo , depending on which method you use to turn on your local camera.",
        "parameters": [
            {
                "factor": "The camera zoom ratio. The value ranges between 1.0 and the maximum zoom supported by the device. You can get the maximum zoom ratio supported by the device by calling the getCameraMaxZoomFactor method."
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "api_irtcengineex_enableloopbackrecordingex",
        "name": "enableLoopbackRecordingEx",
        "description": "Enables loopback audio capture.\nIf you enable loopback audio capture, the output of the sound card is mixed into the audio stream sent to the other end.",
        "parameters": [
            {
                "connection": "The connection information. See RtcConnection ."
            },
            {
                "enabled": "Sets whether to enable loopback audio capture:\n true: Enable loopback audio capture.\n false: (Default) Disable loopback audio capture.\n "
            },
            {
                "deviceName": "The device name."
            }
        ],
        "returns": "0: Success.\n < 0: Failure.",
        "is_hide": false
    },
    {
        "id": "api_setdirectcdnstreamingvideoconfiguration",
        "name": "setDirectCdnStreamingVideoConfiguration",
        "description": "Sets the video profile of the media streams directly pushed to the CDN by the host.\nThis method is only valid for video captured by camera, screen sharing, or self-captured. When you set publishCameraTrack or publishCustomVideoTrack in the as to capture videos, you can call this method to set the video profile.",
        "parameters": [
            {
                "config": "Video profile. See VideoEncoderConfiguration ."
            }
        ],
        "returns": "",
        "is_hide": true
    },
    {
        "id": "class_remotevoicepositioninfo",
        "name": "RemoteVoicePositionInfo",
        "description": "The spatial position of the remote user or the media player.\n",
        "parameters": [
            {
                "position": "The coordinates in the world coordinate system. This parameter is an array of length 3, and the three values represent the front, right, and top coordinates in turn."
            },
            {
                "forward": "The unit vector of the x axis in the coordinate system. This parameter is an array of length 3, and the three values represent the front, right, and top coordinates in turn."
            }
        ],
        "returns": "",
        "is_hide": true
    },
    {
        "id": "api_startrhythmplayer",
        "name": "startRhythmPlayer",
        "description": "Enables the virtual metronome.\nIn music education, physical education and other scenarios, teachers usually need to use a metronome so that students can practice with the correct beat. The meter is composed of a downbeat and upbeats. The first beat of each measure is called a downbeat, and the rest are called upbeats.\n In this method, you need to set the paths of the upbeat and downbeat files, the number of beats per measure, the tempo, and whether to send the sound of the metronome to remote users. This method is for Android and iOS only.\n After enabling the virtual metronome, the SDK plays the specified audio effect file from the beginning, and controls the playback duration of each file according to beatsPerMinute you set in AgoraRhythmPlayerConfig . For example, if you set beatsPerMinute as 60, the SDK plays one beat every second. If the file duration exceeds the beat duration, the SDK only plays the audio within the beat duration.",
        "parameters": [
            {
                "sound1": "The absolute path or URL address (including the filename extensions) of the file for the downbeat. For example: C:\\music\\audio.mp4. For the audio file formats supported by this method, see What formats of audio files does the Agora RTC SDK support."
            },
            {
                "sound2": "The absolute path or URL address (including the filename extensions) of the file for the upbeats. For example: C:\\music\\audio.mp4. For the audio file formats supported by this method, see What formats of audio files does the Agora RTC SDK support."
            },
            {
                "config": "The metronome configuration. See AgoraRhythmPlayerConfig ."
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "api_setaudioeffectpreset",
        "name": "setAudioEffectPreset",
        "description": "Sets an SDK preset audio effect.\nCall this method to set an SDK preset audio effect for the local user who sends an audio stream. This audio effect does not change the gender characteristics of the original voice. After setting an audio effect, all users in the channel can hear the effect.\n To get better audio effect quality, Agora recommends calling and setting scenario in setAudioProfile as audioScenarioGameStreaming(3) before calling this method. You can call this method either before or after joining a channel.\n Do not set the profile parameter in setAudioProfile to audioProfileSpeechStandard(1)audioProfileIot or (6), or the method does not take effect.\n This method works best with the human voice. Agora does not recommend using this method for audio containing music.\n If you call setAudioEffectPreset, you need to set preset as roomAcoustics3dVoice or pitchCorrection, and do not call setAudioEffectParameters ; otherwise, setAudioEffectPreset is overridden.\n After calling setAudioEffectPreset, Agora recommends not calling the following methods, or the settings in setAudioEffectPreset are overridden: \n setVoiceBeautifierPreset \n setLocalVoicePitch \n setLocalVoiceEqualization \n setLocalVoiceReverb \n setVoiceBeautifierParameters \n setVoiceConversionPreset",
        "parameters": [
            {
                "preset": "The options for SDK preset audio effects. See AudioEffectPreset ."
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "class_size",
        "name": "SIZE",
        "description": "The target size of the thumbnail or icon.\n",
        "parameters": [
            {
                "width": "The target width (px) for the thumbnail or icon. The default value is 0."
            },
            {
                "height": "The target height (px) for the thumbnail or icon. The default value is 0."
            }
        ],
        "returns": "",
        "is_hide": true
    },
    {
        "id": "api_iaudiodevicemanager_enumeraterecordingdevices",
        "name": "enumerateRecordingDevices",
        "description": "Enumerates the audio capture devices.\n",
        "parameters": [],
        "returns": "A AudioDeviceInfo array, which includes all the audio capture devices, if the method call succeeds.",
        "is_hide": false
    },
    {
        "id": "api_imediaplayer_setrendermode",
        "name": "setRenderMode",
        "description": "Sets the render mode of the media player.\n",
        "parameters": [
            {
                "renderMode": "Sets the render mode of the view. See RenderModeType .\n "
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "api_startchannelmediarelay",
        "name": "startChannelMediaRelay",
        "description": "Starts relaying media streams across channels. This method can be used to implement scenarios such as co-host across channels.\nAfter a successful method call, the SDK triggers the onChannelMediaRelayStateChanged and onChannelMediaRelayEvent callbacks, and these callbacks return the state and events of the media stream relay.\n If the onChannelMediaRelayStateChanged callback returns relayStateRunning (2) and relayOk (0), and the onChannelMediaRelayEvent callback returns relayEventPacketSentToDestChannel (4), it means that the SDK starts relaying media streams between the source channel and the destination channel.\n If the onChannelMediaRelayStateChanged callback returns relayStateFailure (3), an exception occurs during the media stream relay. \n Call this method after joining the channel.\n This method takes effect only when you are a host in a live streaming channel.\n After a successful method call, if you want to call this method again, ensure that you call the stopChannelMediaRelay method to quit the current relay.\n You need to contact support before implementing this function.\n We do not support string type of UID in this API.",
        "parameters": [
            {
                "configuration": "The configuration of the media stream relay. See ChannelMediaRelayConfiguration ."
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "api_getscreencapturesources",
        "name": "getScreenCaptureSources",
        "description": "Gets a list of shareable screens and windows.\nYou can call this method before sharing a screen or window to get a list of shareable screens and windows, which enables a user to use thumbnails in the list to choose a particular screen or window to share. This list also contains important information such as window ID and screen ID, with which you can call startScreenCaptureByWindowId \n This method applies to Windows only.",
        "parameters": [
            {
                "thumbSize": "The target size of the screen or window thumbnail (the width and height are in pixels).  The SDK scales the original image to make the length of the longest side of the image the same as that of the target size without distorting the original image. For example, if the original image is 400 × 300 and thumbSize is 100 × 100, the actual size of the thumbnail is 100 × 75. If the target size is larger than the original size, the thumbnail is the original image and the SDK does not scale it."
            },
            {
                "iconSize": "The target size of the icon corresponding to the application program (the width and height are in pixels).  The SDK scales the original image to make the length of the longest side of the image the same as that of the target size without distorting the original image. For example, if the original image is 400 × 300 and iconSize is 100 × 100, the actual size of the icon is 100 × 75. If the target size is larger than the original size, the icon is the original image and the SDK does not scale it."
            },
            {
                "includeScreen": "Whether the SDK returns the screen information in addition to the window information:\n true: The SDK returns the screen and window information.\n false: The SDK returns the window information only.\n "
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "callback_ivideoframeobserver_getobservedframeposition",
        "name": "getObservedFramePosition",
        "description": "Sets the frame position for the video observer.\nAfter successfully registering the video data observer, the SDK uses this callback to determine whether to trigger onCaptureVideoFrame , onRenderVideoFrame and onPreEncodeVideoFrame callback at each specific video frame processing position, so that you can observe the locally collected video data, the video data sent by the remote end, and the video data before encoding. You can set one or more positions you need to observe by modifying the return value according to your scenarios: POSITION_POST_CAPTURER(1 << 0): The position after capturing the video data, which corresponds to the onCaptureVideoFrame callback.\n POSITION_PRE_RENDERER(1 << 1): The position before receiving the remote video data, which corresponds to the onRenderVideoFrame callback.\n POSITION_PRE_ENCODER(1 << 2): The position before encoding the video data, which corresponds to the onPreEncodeVideoFrame callback. \n Use '|' (the OR operator) to observe multiple frame positions.\n This callback observes POSITION_POST_CAPTURER(1 << 0) and POSITION_PRE_RENDERER(1 << 1) by default.\n To conserve the system consumption, you can reduce the number of frame positions that you want to observe.",
        "parameters": [],
        "returns": "",
        "is_hide": true
    },
    {
        "id": "callback_onrhythmplayerstatechanged",
        "name": "onRhythmPlayerStateChanged",
        "description": "",
        "parameters": [
            {
                "null": ""
            }
        ],
        "returns": "",
        "is_hide": true
    },
    {
        "id": "callback_onusermuteaudio",
        "name": "onUserMuteAudio",
        "description": "Occurs when a remote user (in the communication profile)/ host (in the live streaming profile) joins the channel.\nThe SDK triggers this callback when the remote user stops or resumes sending the audio stream by calling the muteLocalAudioStream method.\n This callback can be inaccurate when the number of users (in the communication profile) or hosts (in the live broadcasting profile) in a channel exceeds 17.",
        "parameters": [
            {
                "connection": "The connection information. See RtcConnection ."
            },
            {
                "remoteUid": "The user ID."
            },
            {
                "muted": "Whether the remote user's audio stream is muted/unmuted:\n true: Muted.\n false: Unmuted.\n "
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "api_pushreverseaudioframe",
        "name": "pushReverseAudioFrame1",
        "description": "Pushes an external audio frame, which is processed by a custom echo cancellation module.\nTo prevent the local user from hearing the user's own echo, after the local user calls pushCaptureAudioFrame1 , the remote user needs to call pullAudioFrame and pushReverseAudioFrame1 to push the audio frames collected by himself and the received audio frames to the custom echo cancellation module for processing.\n 调用该方法前，你需要将 ChannelMediaOptions 中的 publishCustomAudioTrackAec 设为 true 并将 publishCustomAudioTrack 设为 false。",
        "parameters": [],
        "returns": "0: Success.\n < 0: Failure.",
        "is_hide": true
    },
    {
        "id": "api_setupremotevideoex",
        "name": "setupRemoteVideoEx",
        "description": "Initializes the video view of a remote user.\nThis method initializes the video view of a remote stream on the local device. It affects only the video view that the local user sees. Call this method to bind the remote video stream to a video view and to set the rendering and mirror modes of the video view.\n The application specifies the uid of the remote video in the VideoCanvas method before the remote user joins the channel.\n If the remote uid is unknown to the application, set it after the application receives the onUserJoined callback. If the Video Recording function is enabled, the Video Recording Service joins the channel as a dummy client, causing other clients to also receive the onUserJoined callback. Do not bind the dummy client to the application view because the dummy client does not send any video streams.\n To unbind the remote user from the view, set the view parameter to NULL.\n Once the remote user leaves the channel, the SDK unbinds the remote user.",
        "parameters": [
            {
                "connection": "The connection information. See RtcConnection ."
            },
            {
                "canvas": "The remote video view settings. See VideoCanvas .\n "
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "api_removepublishstreamurl",
        "name": "removePublishStreamUrl",
        "description": "Removes an RTMP or RTMPS stream from the CDN.\nDeprecated:\n This method is deprecated. This method is deprecated. Use stopRtmpStream instead. After a successful method call, the SDK triggers the onRtmpStreamingStateChanged callback on the local client to report the result of deleting the URL. Before calling this method, make sure that the media push function has been enabled. \n This method takes effect only when you are a host in live interactive streaming.\n Call this method after joining a channel.\n This method removes only one media push URL each time it is called. To remove multiple URLs, call this method multiple times.",
        "parameters": [
            {
                "url": "The media push URL in the RTMP or RTMPS format. The maximum length of this parameter is 1024 bytes. The media push URL must not contain special characters, such as Chinese characters."
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "class_encodedaudioframeinfo",
        "name": "EncodedAudioFrameInfo",
        "description": "Audio information after encoding.\n",
        "parameters": [
            {
                "codec": "Audio Codec type: AudioCodecType ."
            },
            {
                "sampleRateHz": "Audio sample rate (Hz)."
            },
            {
                "samplesPerChannel": "The number of audio samples per channel."
            },
            {
                "numberOfChannels": "The number of audio channels."
            },
            {
                "advancedSettings": "This function is not currently supported."
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "enum_audiocodectype",
        "name": "AudioCodecType",
        "description": "The codec type of audio.\n",
        "parameters": [
            {
                "audioCodecOpus": "1: OPUS."
            },
            {
                "audioCodecAaclc": "8: LC-AAC."
            },
            {
                "audioCodecHeaac": "9: HE-AAC."
            },
            {
                "audioCodecHeaac2": "11: HE-AAC v2."
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "api_pausealleffects",
        "name": "pauseAllEffects",
        "description": "Pauses playing all audio effect files.\n",
        "parameters": [],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "api_updatedirectcdnstreamingmediaoptions",
        "name": "updateDirectCdnStreamingMediaOptions",
        "description": "",
        "parameters": [
            {
                "null": ""
            }
        ],
        "returns": "",
        "is_hide": true
    },
    {
        "id": "api_complain",
        "name": "complain",
        "description": "Allows a user to complain about the call quality after a call ends.\nThis method allows users to complain about the quality of the call. Call this method after the user leaves the channel.",
        "parameters": [
            {
                "callId": "The current call ID. You can get the call ID by calling getCallId ."
            },
            {
                "description": ""
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "callback_onlocalvideostatechanged",
        "name": "onLocalVideoStateChanged",
        "description": "Occurs when the local video stream state changes.\nWhen the state of the local video stream changes, the SDK triggers this callback to report the current state. This callback indicates the state of the local video stream and allows you to troubleshoot issues when exceptions occur.\n The SDK triggers the onLocalVideoStateChanged callback with the state code of localVideoStreamStateFailed and error code of localVideoStreamErrorCaptureFailure in the following situations:\n The app switches to the background, and the system gets the camera resource.\n The camera starts normally, but does not output vide frames for four consecutive seconds. When the camera outputs the captured video frames, if the video frames are the same for 15 consecutive frames, the SDK triggers the onLocalVideoStateChanged callback with the state code of localVideoStreamStateCapturing and error code of localVideoStreamErrorCaptureFailure. Note that the video frame duplication detection is only available for video frames with a resolution greater than 200 × 200, a frame rate greater than or equal to 10 fps, and a bitrate less than 20 Kbps.\n For some device models, the SDK does not trigger this callback when the state of the local video changes while the local video capturing device is in use, so you have to make your own timeout judgment.",
        "parameters": [
            {
                "connection": "The connection information. See RtcConnection ."
            },
            {
                "state": "The state of the local video. See LocalVideoStreamState .\n "
            },
            {
                "errorCode": "The detailed error information. See LocalVideoStreamError .\n "
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "callback_onuserstatechanged",
        "name": "onUserStateChanged",
        "description": "",
        "parameters": [
            {
                "null": ""
            }
        ],
        "returns": "",
        "is_hide": true
    },
    {
        "id": "api_setplaybackspeed",
        "name": "setPlaybackSpeed",
        "description": "Sets the channel mode of the current audio file.\nCall this method after calling open .",
        "parameters": [
            {
                "speed": "The playback speed. Agora recommends that you limit this value to between 50 and 400, defined as follows:\n 50: Half the original speed.\n 100: The original speed.\n 400: 4 times the original speed.\n "
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "callback_idirectcdnstreamingeventhandler_ondirectcdnstreamingstatechanged",
        "name": "onDirectCdnStreamingStateChanged",
        "description": "Occurs when the CDN streaming state changes.\nWhen the host directly pushes streams to the CDN, if the streaming state changes, the SDK triggers this callback to report the changed streaming state, error codes, and other information. You can troubleshoot issues by referring to this callback.",
        "parameters": [
            {
                "state": "The current CDN streaming state. See DIRECT_CDN_STREAMING_STATE ."
            },
            {
                "error": "The reason for the CDN streaming error. See DIRECT_CDN_STREAMING_ERROR ."
            },
            {
                "message": "The information about the changed streaming state."
            }
        ],
        "returns": "",
        "is_hide": true
    },
    {
        "id": "callback_iaudioframeobserver_onframe",
        "name": "onFrame",
        "description": "Occurs each time the player receives an audio frame.\nAfter registering the audio frame observer, the callback occurs every time the player receives an audio frame, reporting the detailed information of the audio frame.",
        "parameters": [],
        "returns": "",
        "is_hide": true
    },
    {
        "id": "api_stopchannelmediarelay",
        "name": "stopChannelMediaRelay",
        "description": "Stops the media stream relay. Once the relay stops, the host quits all the destination channels.\nAfter a successful method call, the SDK triggers the onChannelMediaRelayStateChanged callback. If the callback reports relayStateIdle (0) and relayOk (0), the host successfully stops the relay.\n If the method call fails, the SDK triggers the onChannelMediaRelayStateChanged callback with the relayErrorServerNoResponse (2) or relayErrorServerConnectionLost (8) status code. You can call the leaveChannel method to leave the channel, and the media stream relay automatically stops.",
        "parameters": [],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "enum_channelmediarelayevent",
        "name": "ChannelMediaRelayEvent",
        "description": "The event code of channel media relay.\n",
        "parameters": [
            {
                "relayEventNetworkDisconnected": "0: The user disconnects from the server due to a poor network connection."
            },
            {
                "relayEventNetworkConnected": "1: The user is connected to the server."
            },
            {
                "relayEventPacketJoinedSrcChannel": "2: The user joins the source channel."
            },
            {
                "relayEventPacketJoinedDestChannel": "3: The user joins the destination channel."
            },
            {
                "relayEventPacketSentToDestChannel": "4: The SDK starts relaying the media stream to the destination channel."
            },
            {
                "relayEventPacketReceivedVideoFromSrc": "5: The server receives the audio stream from the source channel."
            },
            {
                "relayEventPacketReceivedAudioFromSrc": "6: The server receives the audio stream from the source channel."
            },
            {
                "relayEventPacketUpdateDestChannel": "7: The destination channel is updated."
            },
            {
                "relayEventPacketUpdateDestChannelRefused": "8: The destination channel update fails due to internal reasons."
            },
            {
                "relayEventPacketUpdateDestChannelNotChange": "9: The destination channel does not change, which means that the destination channel fails to be updated."
            },
            {
                "relayEventPacketUpdateDestChannelIsNull": "10: The destination channel name is NULL."
            },
            {
                "relayEventVideoProfileUpdate": "11: The video profile is sent to the server."
            },
            {
                "relayEventPauseSendPacketToDestChannelSuccess": "12: The SDK successfully pauses relaying the media stream to destination channels."
            },
            {
                "relayEventPauseSendPacketToDestChannelFailed": "13: The SDK fails to pause relaying the media stream to destination channels."
            },
            {
                "relayEventResumeSendPacketToDestChannelSuccess": "14: The SDK successfully resumes relaying the media stream to destination channels."
            },
            {
                "relayEventResumeSendPacketToDestChannelFailed": "15: The SDK fails to resume relaying the media stream to destination channels."
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "callback_onremoteaudiostats",
        "name": "onRemoteAudioStats",
        "description": "Reports the statistics of the audio stream sent by each remote users.\nThe SDK triggers this callback once every two seconds. If a channel includes multiple users, the SDK triggers this callback as many times.",
        "parameters": [
            {
                "connection": "The connection information. See RtcConnection ."
            },
            {
                "stats": "Statistics of the received remote audio stream. See RemoteAudioStats ."
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "callback_iaudiospectrumobserver_onlocalaudiospectrum",
        "name": "onLocalAudioSpectrum",
        "description": "Gets the statistics of a local audio spectrum.\nAfter successfully calling registerAudioSpectrumObserver to implement the onLocalAudioSpectrum in IAudioSpectrumObserver and calling enableAudioSpectrumMonitor to enable audio spectrum monitoring, the SDK will trigger the callback as the time interval you set to report the received remote audio data spectrum.",
        "parameters": [
            {}
        ],
        "returns": "Whether you have received the spectrum data:\n true: Spectrum data is received.\n false: No spectrum data is received.",
        "is_hide": true
    },
    {
        "id": "api_getaudiodevicemanager",
        "name": "getAudioDeviceManager",
        "description": "Gets the AudioDeviceManager object to manage audio devices.\n",
        "parameters": [],
        "returns": "A AudioDeviceManager object.",
        "is_hide": false
    },
    {
        "id": "class_userinfo",
        "name": "UserInfo",
        "description": "The information of the user.\n",
        "parameters": [
            {
                "uid": "The user ID."
            },
            {
                "userAccount": "User account. The maximum data length is MaxUserAccountLengthType ."
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "callback_onconnectionlost",
        "name": "onConnectionLost",
        "description": "Occurs when the SDK cannot reconnect to Agora's edge server 10 seconds after its connection to the server is interrupted.\nThe SDK triggers this callback when it cannot connect to the server 10 seconds after calling the joinChannelWithOptions method, regardless of whether it is in the channel. If the SDK fails to rejoin the channel within 20 minutes after disconnecting, the SDK will stop trying to reconnect.",
        "parameters": [
            {
                "connection": "The connection information. See RtcConnection ."
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "api_enablefishcorrection",
        "name": "enableFishCorrection",
        "description": "",
        "parameters": [
            {
                "null": ""
            }
        ],
        "returns": "",
        "is_hide": true
    },
    {
        "id": "enum_audioprocessingchannels",
        "name": "audioprocessingchannels",
        "description": "The number of channels for audio preprocessing.\nIn scenarios that require enhanced realism, such as concerts, local users might need to capture stereo audio and send stereo signals to remote users. For example, the singer, guitarist, and drummer are standing in different positions on the stage. The audio capture device captures their stereo audio and sends stereo signals to remote users. Remote users can hear the song, guitar, and drum from different directions as if they were at the auditorium.\n You can set the dual-channel processing to implement stereo audio in this class. Agora recommends the following settings:\n Preprocessing: call setAdvancedAudioOptions and set audioProcessingChannels to AdvancedAudioOptions (2) in audioProcessingStereo.\n Post-processing: call setAudioProfile2 profile to audioProfileMusicStandardStereo (3) or audioProfileMusicHighQualityStereo (5). \n The stereo setting only takes effect when the SDK uses the media volume.",
        "parameters": [
            {
                "audioProcessingMono": "1: (Default) Mono."
            },
            {
                "audioProcessingStereo": "2: Stereo (two channels)."
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "enum_loglevel",
        "name": "LogLevel",
        "description": "The output log level of the SDK.\n",
        "parameters": [
            {
                "logLevelNone": "0: Do not output any log information."
            },
            {
                "logLevelInfo": "0x0001: (Default) Output FATAL, ERROR, WARN, and INFO level log information. We recommend setting your log filter to this level."
            },
            {
                "logLevelWarn": "0x0002: Output FATAL, ERROR, and WARN level log information."
            },
            {
                "logLevelError": "0x0004: Output FATAL and ERROR level log information."
            },
            {
                "logLevelFatal": "0x0008: Output FATAL level log information."
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "api_createdatastream2",
        "name": "createDataStream",
        "description": "Creates a data stream.\nCreates a data stream. Each user can create up to five data streams in a single channel.",
        "parameters": [
            {
                "config": "The configurations for the data stream. See DataStreamConfig ."
            }
        ],
        "returns": "ID of the created data stream, if the method call succeeds.\n < 0: Failure.",
        "is_hide": false
    },
    {
        "id": "api_iaudiodevicemanager_getplaybackdeviceinfo",
        "name": "getPlaybackDeviceInfo",
        "description": " Retrieves the audio playback device associated with the device ID. \n",
        "parameters": [],
        "returns": "A AudioDeviceInfo object, which contains the ID and device name of the audio devices.",
        "is_hide": false
    },
    {
        "id": "callback_ivideoframeobserver_getrotationapplied",
        "name": "getRotationApplied",
        "description": "Occurs each time the SDK receives a video frame and prompts you whether or not to mirror the captured video.\nThis function only supports video data in RGBA and YUV420 formats.",
        "parameters": [],
        "returns": "Sets whether or not to mirror the captured video:\n true: Rotate.\n false: (Default) Do not rotate the captured video.",
        "is_hide": true
    },
    {
        "id": "api_mediaplayercontroller_create",
        "name": "create",
        "description": "Create a MediaPlayerController.\nMake sure the RtcEngine is initialized before you call this method.\n Make sure to call this method before calling other APIs in MediaPlayer .",
        "parameters": [
            {
                "useAndroidSurfaceView": "Whether to use Android SurfaceView to render video:\n true: Use Android SurfaceView to render video.\n false: Do not use Android SurfaceView to render video. Android SurfaceView applies to Android platform only."
            },
            {
                "useFlutterTexture": "Whether to use FlutterTexture to render video:\n true: Use FlutterTexture to render video.\n false: Do not use FlutterTexture to render video. FlutterTexture applies to iOS, macOS and Windows platforms."
            },
            {
                "canvas": "Local video display properties. See VideoCanvas ."
            },
            {
                "rtcEngine": " RtcEngine ."
            }
        ],
        "returns": "MediaPlayerController object.",
        "is_hide": false
    },
    {
        "id": "api_adjustplaybacksignalvolume",
        "name": "adjustPlaybackSignalVolume",
        "description": "Adjusts the playback signal volume of all remote users.\nThis method adjusts the playback volume that is the mixed volume of all remote users.\n You can call this method either before or after joining a channel.",
        "parameters": [
            {
                "volume": "The volume of the user. The value range is [0,400].\n 0: Mute.\n 100: (Default) The original volume.\n 400: Four times the original volume (amplifying the audio signals by four times). "
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "callback_onaudiodevicevolumechanged",
        "name": "onAudioDeviceVolumeChanged",
        "description": "Occurs when the volume on the playback or audio capture device, or the volume in the application changes.\n",
        "parameters": [
            {
                "deviceType": "The evice type. See MediaDeviceType ."
            },
            {
                "volume": "The volume. The value range is [0,255]."
            },
            {
                "muted": "Whether the audio device is muted:\n true: The audio device is muted.\n false: The audio device is not muted. "
            }
        ],
        "returns": "",
        "is_hide": true
    },
    {
        "id": "class_videoencoderconfiguration",
        "name": "VideoEncoderConfiguration",
        "description": "Video encoder configurations.\n",
        "parameters": [
            {
                "dimensions": "The dimensions of the encoded video (px). See VideoDimensions . This parameter measures the video encoding quality in the format of length × width. The default value is 640 × 360. You can set a custom value.\n "
            },
            {
                "codecType": "The codec type of the local video stream. See VideoCodecType .\n "
            },
            {
                "frameRate": "The frame rate (fps) of the encoding video frame. The default value is 15. See FrameRate .\n "
            },
            {
                "bitrate": "The encoding bitrate (Kbps) of the video.  : (Recommended) Standard bitrate mode. In this mode, the video bitrate is twice the base bitrate.\n : Adaptive bitrate mode. In this mode, the video bitrate is the same as the base bitrate. If you choose this mode in the interactive streaming profile, the video frame rate may be lower than the set value.\n "
            },
            {
                "minBitrate": "The minimum encoding bitrate (Kbps) of the video.\n The SDK automatically adjusts the encoding bitrate to adapt to the network conditions. Using a value greater than the default value forces the video encoder to output high-quality images but may cause more packet loss and sacrifice the smoothness of the video transmission. Unless you have special requirements for image quality, Agora does not recommend changing this value.\n This parameter only applies to the interactive streaming profile. "
            },
            {
                "orientationMode": "The orientation mode of the encoded video. See OrientationMode ."
            },
            {
                "degradationPreference": "Video degradation preference under limited bandwidth. "
            },
            {
                "mirrorMode": "Whether to enable mirroring mode when sending encoded video, only affects the video images seen by remote users. See VideoMirrorModeType .\n By default, the video is not mirrored. "
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "api_imediaplayer_switchagoracdnlinebyindex",
        "name": "switchAgoraCDNLineByIndex",
        "description": "Changes the CDN route for playing the media resource.\nAfter calling openWithAgoraCDNSrc to open the media resource, you can call this method if you want to change the CDN routes for playing the media resource. Call this method after calling openWithAgoraCDNSrc .\n You can call this method either before or after play . If you call this method before play, the switch does not take effect immediately. The SDK waits for the playback to complete before switching the CDN line of the media resource.",
        "parameters": [
            {
                "index": "The index of the CDN routes."
            }
        ],
        "returns": "",
        "is_hide": true
    },
    {
        "id": "api_muterecordingsignal",
        "name": "muteRecordingSignal",
        "description": "Whether to mute the recording signal.\n",
        "parameters": [
            {
                "mute": "true: Mute the recording signal.\n false: (Default) Do not mute the recording signal.\n "
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "api_irtcengineex_clearvideowatermarkex",
        "name": "clearVideoWatermarkEx",
        "description": "Removes the watermark image from the video stream.\n",
        "parameters": [
            {
                "connection": "The connection information. See RtcConnection ."
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "api_enableremotesuperresolution",
        "name": "enableRemoteSuperResolution",
        "description": "Enables/Disables the super resolution algorithm for a remote user's video stream.\nThis feature effectively boosts the resolution of a remote user's video seen by the local user. If the original resolution of a remote user's video is a × b, the local user's device can render the remote video at a resolution of 2a × 2b.\n After calling this method, you can confirm whether super resolution is successfully enabled through the remote video stream statistics ( RemoteVideoStats ) in the onRemoteVideoStats callback:\n If the parameter superResolutionType >0: Super resolution is enabled.\n If parameter superResolutionType =0: Super resolution is not enabled. The super resolution feature requires extra system resources. To balance the visual experience and system consumption, this feature can only be enabled for a single remote user. If the local user uses super resolution on Android, the original resolution of the remote user's video cannot exceed 640 × 360 pixels; if the local user uses super resolution on iOS, the original resolution of the remote user's video cannot exceed 640 × 480 pixels. \n This method is for Android and iOS only.\n Before calling this method, ensure that you have integrated the following dynamic libraries:\n Android: libagora_super_resolution_extension.so\n iOS: AgoraSuperResolutionExtension.xcframework Because this method has certain system performance requirements, Agora recommends that you use the following devices or better:\n Android:\n VIVO: V1821A, NEX S, 1914A, 1916A, 1962A, 1824BA, X60, X60 Pro\n OPPO: PCCM00, Find X3\n OnePlus: A6000\n Xiaomi: Mi 8, Mi 9, Mi 10, Mi 11, MIX3, Redmi K20 Pro\n SAMSUNG: SM-G9600, SM-G9650, SM-N9600, SM-G9708, SM-G960U, SM-G9750, S20, S21\n HUAWEI: SEA-AL00, ELE-AL00, VOG-AL00, YAL-AL10, HMA-AL00, EVR-AN00, nova 4, nova 5 Pro, nova 6 5G, nova 7 5G, Mate 30, Mate 30 Pro, Mate 40, Mate 40 Pro, P40, P40 Pro, Huawei M6, MatePad 10.8 iOS:\n iPhone XR\n iPhone XS\n iPhone XS Max\n iPhone 11\n iPhone 11 Pro\n iPhone 11 Pro Max\n iPhone 12\n iPhone 12 mini\n iPhone 12 Pro\n iPhone 12 Pro Max\n iPhone 12 SE (2nd generation)\n iPad Pro 11-inch (3rd generation)\n iPad Pro 12.9-inch (3rd generation)\n iPad Air 3 (3rd generation)\n iPad Air 3 (4th generation)",
        "parameters": [
            {
                "userId": "The user ID of the remote user."
            },
            {
                "enable": "Whether to enable super resolution for the remote user’s video:\n true: Enable the image enhancement function.\n false: Disable virtual background.\n "
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "enum_screencapturesourcetype",
        "name": "ScreenCaptureSourceType",
        "description": "The type of the shared target. Set in ScreenCaptureSourceInfo .\n",
        "parameters": [
            {
                "screencapturesourcetypeUnknown": "-1: Unknown type."
            },
            {
                "screencapturesourcetypeWindow": "0: The shared target is a window."
            },
            {
                "screencapturesourcetypeScreen": "1: The share target is the screen of a particular monitor."
            },
            {
                "screencapturesourcetypeCustom": "6: Reserved parameter"
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "enum_externalvideosourcetype",
        "name": "ExternalVideoSourceType",
        "description": "The external video frame encoding type.\n",
        "parameters": [
            {
                "videoFrame": "0: The video frame is not encoded."
            },
            {
                "encodedVideoFrame": "1: The video frame is encoded."
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "class_advancedaudiooptions",
        "name": "AdvancedAudioOptions",
        "description": "The advanced options for audio.\n",
        "parameters": [
            {
                "audioProcessingChannels": "The number of channels for audio preprocessing. See audioprocessingchannels ."
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "enum_streamsubscribestate",
        "name": "StreamSubscribeState",
        "description": "The subscribing state.\n",
        "parameters": [
            {
                "subStateIdle": "0: The initial publishing state after joining the channel."
            },
            {
                "subStateNoSubscribed": "1: Fails to subscribe to the remote stream. Possible reasons:\n Remote users:\n Calls muteLocalAudioStream (true) or muteLocalVideoStream (true) to stop sending local media streams.\n Calls disableAudio or disableVideo to disable the local audio or video module.\n Calls enableLocalAudio (false) or enableLocalVideo (false) to disable local audio or video capture.\n The role of the local user is audience. The local user calls the following method to stop receiving the remote media stream:\n Call muteRemoteAudioStream (true), muteAllRemoteAudioStreams (true) or setDefaultMuteAllRemoteAudioStreams (true) to stop receiving the remote audio stream.\n Call muteRemoteVideoStream (true), muteAllRemoteVideoStreams (true) or setDefaultMuteAllRemoteVideoStreams (true) to stop receiving the remote video stream. "
            },
            {
                "subStateSubscribing": "2: Publishing."
            },
            {
                "subStateSubscribed": "3: The remote stream is received, and the subscription is successful."
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "api_uploadlogfile",
        "name": "uploadLogFile",
        "description": "Uploads all SDK log files.\nUploads all SDK log files from the client to the Agora server. After calling this method successfully, the SDK triggers the onUploadLogResult callback to report whether the log file is successfully uploaded to the Agora server.\n For easier debugging, Agora recommends that you bind the uploadLogFile method to the UI element of your app, to instruct the user to upload a log file when a quality issue occurs.",
        "parameters": [],
        "returns": "",
        "is_hide": true
    },
    {
        "id": "api_enableextension",
        "name": "enableExtension",
        "description": "Enables/Disables extensions.\nEnsure that you call this method before joining a channel. If you want to enable multiple extensions, you need to call this method multiple times.\n The data processing order of different extensions in the SDK is determined by the order in which the extensions are enabled. That is, the extension that is enabled first will process the data first.",
        "parameters": [
            {
                "extension": "The name of the extension."
            },
            {
                "provider": "The name of the extension provider."
            },
            {
                "enable": "Whether to enable the extension:\n true: Enable the extension.\n false: Disable the extension. "
            },
            {
                "type": "Type of media source. See MediaSourceType .In this method, this parameter supports only the following two settings:\n The default value is unknownMediaSource.\n If you want to use the second camera to capture video, set this parameter to secondaryCameraSource.\n "
            }
        ],
        "returns": "0: Success.\n < 0: Failure.",
        "is_hide": false
    },
    {
        "id": "api_setremotesubscribefallbackoption",
        "name": "setRemoteSubscribeFallbackOption",
        "description": "设置弱网条件下订阅的音视频流的回退选项。 \n网络不理想的环境下，直播音视频的质量都会下降。 如果你使用本方法并将 option 设置为 streamFallbackOptionVideoStreamLow(1) 或 streamFallbackOptionAudioOnly(2)，SDK 会在下行弱网且音视频质量严重受影响时，将视频流切换为小流，或关断视频流，从而保证或提高音频质量。 同时 SDK 会持续监控网络质量，并在网络质量改善时恢复音视频流。 当远端订阅流回退为音频流时，或由音频流恢复为音视频流时，SDK 会触发 onRemoteSubscribeFallbackToAudioOnly 回调。\n Ensure that you call this method before joining a channel.",
        "parameters": [
            {
                "option": "订阅音视频流的回退选项。 默认值为 streamFallbackOptionVideoStreamLow(1)。 See StreamFallbackOptions ."
            }
        ],
        "returns": "",
        "is_hide": true
    },
    {
        "id": "enum_videocontenthint",
        "name": "VideoContentHint",
        "description": "The content hint for screen sharing.\n",
        "parameters": [
            {
                "contentHintNone": "(Default) No content hint."
            },
            {
                "contentHintMotion": "Motion-intensive content. Choose this option if you prefer smoothness or when you are sharing a video clip, movie, or video game."
            },
            {
                "contentHintDetails": "Motionless content. Choose this option if you prefer sharpness or when you are sharing a picture, PowerPoint slides, or texts."
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "api_imediaplayer_setloopcount",
        "name": "setLoopCount",
        "description": "Sets the loop playback.\nIf you want to loop, call this method and set the number of the loops.\n When the loop finishes, the SDK triggers onPlayerSourceStateChanged and reports the playback state as playerStatePlaybackAllLoopsCompleted.",
        "parameters": [
            {
                "loopCount": "The number of times the audio effect loops:"
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "callback_iaudioframeobserver_onplaybackaudioframebeforemixing",
        "name": "onPlaybackAudioFrameBeforeMixing",
        "description": "Retrieves the audio frame of a specified user before mixing.\n",
        "parameters": [
            {}
        ],
        "returns": "Reserved for future use.",
        "is_hide": true
    },
    {
        "id": "api_setlogfile_ng",
        "name": "setLogFile",
        "description": "Set the log file\nDeprecated:\n Use the mLogConfig parameter in initialize method instead. Specifies an SDK output log file. The log file records all log data for the SDK’s operation. Ensure that the directory for the log file exists and is writable. Ensure that you call this method immediately after calling the RtcEngine initialize method, or the output log may not be complete.",
        "parameters": [
            {
                "filePath": "The complete path of the log files. These log files are encoded in UTF-8."
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "api_resumeeffect",
        "name": "resumeEffect",
        "description": "Resumes playing a specified audio effect.\n",
        "parameters": [
            {
                "soundId": "The audio effect ID. The ID of each audio effect file is unique."
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "api_startsecondarycameracapture",
        "name": "startSecondaryCameraCapture",
        "description": "Starts video capture with a secondary camera.\n",
        "parameters": [
            {
                "config": "The configuration of the video capture with a primary camera. See CameraCapturerConfiguration .\n "
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "class_imetadataobserver",
        "name": "MetadataObserver",
        "description": "The metadata observer.\n",
        "parameters": [],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "callback_onuseraccountupdated",
        "name": "onUserAccountUpdated",
        "description": "",
        "parameters": [
            {
                "null": ""
            }
        ],
        "returns": "",
        "is_hide": true
    },
    {
        "id": "api_startaudiorecording3_ng",
        "name": "startAudioRecording",
        "description": "Stops the audio recording on the client.\nThe Agora SDK allows recording during a call. After successfully calling this method, you can record the audio of all the users in the channel and get an audio recording file. Supported formats of the recording file are as follows:\n WAV: High-fidelity files with typically larger file sizes. For example, the size of a WAV file with a sample rate of 32,000 Hz and a recording duration of 10 minutes is around 73 MB.\n AAC: Low-fidelity files with typically smaller file sizes. For example, if the sample rate is 32,000 Hz and the recording quality is audioRecordingQualityMedium, the file size for 10-minute recording is approximately 2 MB. Once the user leaves the channel, the recording automatically stops.\n Call this method after joining a channel.",
        "parameters": [
            {
                "config": "Recording configuration. See AudioRecordingConfiguration ."
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "api_imediaengine_pullaudioframe",
        "name": "pullAudioFrame",
        "description": "Pulls the remote audio data.\nBefore calling this method, you need to set RtcEngineContext as false, and call setExternalAudioSink to notify the app to enable and set the external rendering.\n After a successful method call, the app pulls the decoded and mixed audio data for playback. This method only supports pulling data from custom audio source. If you need to pull the data captured by the SDK, do not call this method.\n Call this method after joining a channel.\n Once you enable the external audio sink, the app will not retrieve any audio data from the onPlaybackAudioFrame callback.\n The difference between this method and the onPlaybackAudioFrame callback is as follows:\n The SDK sends the audio data to the app through the onPlaybackAudioFrame callback. Any delay in processing the audio frames may result in audio jitter.\n After a successful method call, the app automatically pulls the audio data from the SDK. After setting the audio data parameters, the SDK adjusts the frame buffer and avoids problems caused by jitter in the external audio playback.",
        "parameters": [],
        "returns": "",
        "is_hide": true
    },
    {
        "id": "api_enablecontentinspect",
        "name": "enableContentInspect",
        "description": "Enable/disable the review of video streams both at the client side and the Agora cloud content review server.\nWhen video content inspection is enabled, the SDK takes ContentInspectConfig screenshots, reviews, and uploads videos sent by local users based on the type and frequency of the content inspection module you set in . After content inspection, the Agora content inspection server sends the results to your app server in HTTPS requests and sends all screenshots to the third-party cloud storage service.\n If you setContentInspectModule the type in the to identify yellow, after the review is completed, the SDK will trigger a callback to onContentInspectResult report the result of the yellow verification.",
        "parameters": [
            {
                "enabled": "Whether to enable content inspection.\n true: Enable custom echo cancellation.\n false: Disable custom echo cancellation.\n "
            },
            {
                "config": "Configuration of content review. See ContentInspectConfig ."
            }
        ],
        "returns": "0: Success.\n < 0: Failure.",
        "is_hide": true
    },
    {
        "id": "api_iscameratorchsupported",
        "name": "isCameraTorchSupported",
        "description": "Checks whether the device supports enabling the flash.\nThis method needs to be called after the camera is started (for example, by calling startPreview or joinChannelWithOptions ). This method is for Android and iOS only.\n The app enables the front camera by default. If your front camera does not support enabling the flash, this method returns false. If you want to check whether the rear camera supports enabling the flash, call switchCamera to switch the camera to the rear camera, and then call this method.\n On iPads with system version 15, even if isCameraTorchSupported returns true, you might fail to successfully enable the flash by calling setCameraTorchOn due to system issues.",
        "parameters": [],
        "returns": "true: The device supports enabling the flash.\n false: The device does not support enabling the flash.",
        "is_hide": false
    },
    {
        "id": "enum_audiosampleratetype",
        "name": "AudioSampleRateType",
        "description": "The audio sampling rate of the stream to be pushed to the CDN.\n",
        "parameters": [
            {
                "audioSampleRate32000": "32000: 32 kHz"
            },
            {
                "audioSampleRate44100": "44100: 44.1 kHz"
            },
            {
                "audioSampleRate48000": "48000: (Default) 48 kHz"
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "callback_oncamerafocusareachanged",
        "name": "onCameraFocusAreaChanged",
        "description": "Occurs when the camera focus area changes.\n",
        "parameters": [
            {
                "x": "The x-coordinate of the changed focus area."
            },
            {
                "y": "The y-coordinate of the changed focus area."
            },
            {
                "width": "The width of the focus area that changes."
            },
            {
                "height": "The height of the focus area that changes."
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "callback_icloudspatialaudioeventhandler_onconnectionstatechange",
        "name": "onConnectionStateChange",
        "description": "Occurs when the connection state between the SDK and the Agora Spatial Audio Server changes.\nWhen the connection state between the SDK and the Agora Spatial Audio Server changes, the SDK triggers this callback to inform the user of the current connection state and the reason for the change.",
        "parameters": [
            {
                "state": "The connection state between the SDK and the Agora Spatial Audio Server. "
            },
            {
                "reason": "The reason for the change in the connection state between the SDK and the Agora Spatial Audio Server. "
            }
        ],
        "returns": "",
        "is_hide": true
    },
    {
        "id": "callback_ontranscodingupdated",
        "name": "onTranscodingUpdated",
        "description": "Occurs when the publisher's transcoding is updated.\nsetLiveTranscoding When the class in the method LiveTranscoding updates, theonTranscodingUpdated SDK triggers the callback to report the update information.\n If you callsetLiveTranscoding the method to set the class for the first timeLiveTranscoding, the SDK does not trigger this callback.",
        "parameters": [],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "api_imediaengine_pushaudioframe0",
        "name": "pushAudioFrame",
        "description": "Pushes the external audio frame.\n",
        "parameters": [
            {
                "type": "The type of the audio recording device. See MediaSourceType ."
            },
            {
                "frame": "The external audio frame. See AudioFrame ."
            },
            {
                "wrap": "Whether to use the placeholder. Agora recommends using the default value.\n true: Use the placeholder.\n false: (Default) Do not use the placeholder.\n "
            }
        ],
        "returns": "0: Success.\n < 0: Failure.",
        "is_hide": true
    },
    {
        "id": "api_mutelocalaudiostream",
        "name": "muteLocalAudioStream",
        "description": "Stops or resumes publishing the local audio stream.\nThis method does not affect any ongoing audio recording, because it does not disable the audio capture device.",
        "parameters": [
            {
                "mute": "Whether to stop publishing the local audio stream.\n true: Stop publishing the local audio stream.\n false: (Default) Resumes publishing the local audio stream. "
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "api_stopaudiomixing",
        "name": "stopAudioMixing",
        "description": "Stops playing and mixing the music file.\nThis method stops the audio mixing. Call this method when you are in a channel.",
        "parameters": [],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "api_getaudiomixingplayoutvolume",
        "name": "getAudioMixingPlayoutVolume",
        "description": "Retrieves the audio mixing volume for local playback.\nThis method retrieves the audio mixing volume for local playback. You can use it to troubleshoot audio volume related issues. You need to call this method after calling startAudioMixing and receiving the onAudioMixingStateChanged(audioMixingStatePlaying) callback.",
        "parameters": [],
        "returns": "≥ 0: The audio mixing volume, if this method call succeeds. The value range is [0,100].\n < 0: Failure.",
        "is_hide": false
    },
    {
        "id": "api_startrtmpstreamwithouttranscoding",
        "name": "startRtmpStreamWithoutTranscoding",
        "description": "Starts pushing media streams to a CDN without transcoding.\nYou can call this method to push a live audio-and-video stream to the specified CDN address and set the transcoding configuration. This method can push media streams to only one CDN address at a time, so if you need to push streams to multiple addresses, call this method multiple times.\n After you call this method, the SDK triggers the onRtmpStreamingStateChanged callback on the local client to report the state of the streaming. Call this method after joining a channel.\n Only hosts in the LIVE_BROADCASTING profile can call this method.\n If you want to retry pushing streams after a failed push, make sure to call stopRtmpStream first, then call this method to retry pushing streams; otherwise, the SDK returns the same error code as the last failed push.",
        "parameters": [
            {
                "url": "The address of Media Push. The format is RTMP or RTMPS. The character length cannot exceed 1024 bytes. Special characters such as Chinese characters are not supported."
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "enum_interfaceidtype",
        "name": "InterfaceIdType",
        "description": "The interface class.\n",
        "parameters": [
            {
                "agoraIidAudioDeviceManager": "The AudioDeviceManager interface class."
            },
            {
                "agoraIidVideoDeviceManager": "The VideoDeviceManager interface class."
            },
            {
                "agoraIidParameterEngine": "This interface class is deprecated."
            },
            {
                "agoraIidSignalingEngine": "This interface class is deprecated."
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "api_setlocalaccesspoint",
        "name": "setLocalAccessPoint",
        "description": "",
        "parameters": [
            {
                "null": ""
            }
        ],
        "returns": "",
        "is_hide": true
    },
    {
        "id": "api_imediaplayer_unregisterplayersourceobserver",
        "name": "unregisterPlayerSourceObserver",
        "description": "Releases a media player observer.\n",
        "parameters": [
            {
                "observer": "The player observer, listening for events during the playback. See MediaPlayerSourceObserver ."
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "enum_experiencequalitytype",
        "name": "ExperienceQualityType",
        "description": "The Quality of Experience (QoE) of the local user when receiving a remote audio stream.\n",
        "parameters": [
            {
                "experienceQualityGood": "0: The QoE of the local user is good."
            },
            {
                "experienceQualityBad": "1: The QoE of the local user is poor"
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "callback_onrtcstats",
        "name": "onRtcStats",
        "description": "Reports the statistics of the current call. \nThe SDK triggers this callback once every two seconds after the user joins the channel.",
        "parameters": [
            {
                "connection": "The connection information. See RtcConnection ."
            },
            {
                "stats": "Statistics of the RTC engine. See RtcStats . "
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "api_rate",
        "name": "rate",
        "description": "Allows a user to rate a call after the call ends.\nEnsure that you call this method after leaving a channel.",
        "parameters": [
            {
                "callId": "The current call ID. You can get the call ID by calling getCallId ."
            },
            {
                "rating": "The rating of the call. The value is between 1 (lowest score) and 5 (highest score). If you set a value out of this range, the SDK returns the -2 (ERR_INVALID_ARGUMENT) error."
            },
            {
                "description": ""
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "api_setaudioprofile2",
        "name": "setAudioProfile2",
        "description": "Sets the audio parameters and application scenarios.\nIn scenarios requiring high-quality audio, such as online music tutoring, Agora recommends you set profile as audioProfileMusicHighQuality (4).\n If you want to set the audio scenario, call initialize and set RtcEngineContext struct.",
        "parameters": [
            {
                "profile": "The audio profile, including the sampling rate, bitrate, encoding mode, and the number of channels. See AudioProfileType .\n "
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "api_addpublishstreamurl",
        "name": "addPublishStreamUrl",
        "description": "Publishes the local stream to a specified CDN live streaming URL.\nDeprecated:\n This method is deprecated. Use startRtmpStreamWithoutTranscoding or startRtmpStreamWithTranscoding instead according to your needs. After calling this method, you can push media streams in RTMP or RTMPS protocol to the CDN. The SDK triggers the onRtmpStreamingStateChanged callback on the local client to report the state of adding a local stream to the CDN. Call this method after joining a channel.\n Ensure that the Media Push function is enabled. \n This method takes effect only when you are a host in live interactive streaming.\n This method adds only one streaming URL to the CDN each time it is called. To push multiple URLs, call this method multiple times.",
        "parameters": [
            {
                "url": "The Media Push URL in the RTMP or RTMPS format. The maximum length of this parameter is 1024 bytes. The URL address must not contain special characters, such as Chinese language characters."
            },
            {
                "transcodingEnabled": "Whether to enable transcoding. Transcoding in a CDN live streaming converts the audio and video streams before pushing them to the CDN server. It applies to scenarios where a channel has multiple broadcasters and composite layout is needed.\n true: Enable transcoding.\n false: Disable transcoding. If you set this parameter as true, ensurethat you call the setLiveTranscoding method before calling this method. "
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "callback_imetadataobserver_onmetadatareceived",
        "name": "onMetadataReceived",
        "description": "Occurs when the local user receives the metadata.\n",
        "parameters": [
            {
                "metadata": "The metadata received, see Metadata ."
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "api_imediaplayer_enableautoswitchagoracdn",
        "name": "enableAutoSwitchAgoraCDN",
        "description": "Enables/Disables the automatic switch of the CDN routes for playing the media resource.\nYou can call this method if you want the SDK to automatically switch the CDN routes according to your network conditions. Call this method before openWithAgoraCDNSrc .",
        "parameters": [
            {
                "enable": "Whether to enable the automatic switch of the CDN routes for playing the media resource:\n true: Enables the automatic switch of the CDN routes.\n false: (Default) Disables the automatic switch of the CDN routes.\n "
            }
        ],
        "returns": "",
        "is_hide": true
    },
    {
        "id": "api_geteffectsvolume",
        "name": "getEffectsVolume",
        "description": "Retrieves the volume of the audio effects.\nThe volume range is [0,100]. The default value is 100, the original volume.\n Call this method after the playEffect method.",
        "parameters": [],
        "returns": "Volume of the audio effects, if this method call succeeds.\n < 0: Failure.",
        "is_hide": false
    },
    {
        "id": "api_getconnectionstate",
        "name": "getConnectionState",
        "description": "Gets the current connection state of the SDK.\nYou can call this method either before or after joining a channel.",
        "parameters": [],
        "returns": "The current connection state.  ConnectionStateType",
        "is_hide": false
    },
    {
        "id": "api_setcameracapturerconfiguration",
        "name": "setCameraCapturerConfiguration",
        "description": "Sets the camera capture configuration.\nThis method is for Android and iOS only.\n Call this method before calling joinChannelWithOptions , enableVideo , or enableLocalVideo , depending on which method you use to turn on your local camera.",
        "parameters": [
            {
                "config": "The camera capturer configuration. See CameraCapturerConfiguration ."
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "callback_oncontentinspectresult",
        "name": "onContentInspectResult",
        "description": "鉴黄审核结果回调。\n设置 ContentInspectConfig 中的 type 设为 ContentInspectModule 并调用 enableContentInspect 开启鉴黄后，SDK 会触发 onContentInspectResult 回调，报告鉴黄结果。",
        "parameters": [
            {
                "result": "鉴黄结果。 See CONTENT_INSPECT_RESULT ."
            }
        ],
        "returns": "",
        "is_hide": true
    },
    {
        "id": "api_setexternalaudiosource2",
        "name": "setExternalAudioSource",
        "description": "Sets the external audio source parameters.\nCall this method before joining a channel.",
        "parameters": [
            {
                "enabled": "Whether to enable the external audio source:\n true: Enable the external audio source.\n false: (Default) Disable the external audio source. "
            },
            {
                "sampleRate": "The sample rate (Hz) of the external audio source, which can be set as 8000, 16000, 32000, 44100, or 48000."
            },
            {
                "channels": "The number of channels of the external audio source, which can be set as 1 (Mono) or 2 (Stereo)."
            },
            {
                "sourceNumber": "The number of external audio sources. The value of this parameter should be larger than 0. The SDK creates a corresponding number of custom audio tracks based on this parameter value and names the audio tracks starting from 0. In ChannelMediaOptions , you can set publishCustomAudioSourceId to the ID of the audio track you want to publish."
            },
            {
                "localPlayback": "Whether to play the external audio source:\n true: Play the external audio source.\n false: (Default) Do not play the external source. "
            },
            {
                "publish": "Whether to publish audio to the remote users:\n true: (Default) Publish audio to the remote users.\n false: Do not publish audio to the remote users "
            }
        ],
        "returns": "0: Success.\n < 0: Failure.",
        "is_hide": true
    },
    {
        "id": "api_icloudspatialaudioengine_removeeventhandler",
        "name": "removeEventHandler",
        "description": "Removes callbacks specified in ICloudSpatialAudioEventHandler .\nFor callbacks that you only want to listen for once, call this method to remove these callbacks after you have received them.",
        "parameters": [
            {
                "eventHandler": "The callback handler to be deleted. See ICloudSpatialAudioEventHandler for details."
            }
        ],
        "returns": "",
        "is_hide": true
    },
    {
        "id": "api_icloudspatialaudioengine_enterroom",
        "name": "enterRoom",
        "description": "Enters a room.\nThe spatial audio effect does not take effect until you enter a room. After you call this method, the SDK triggers the onConnectionStateChange callback.\n Call this method after joinChannelWithOptions .",
        "parameters": [
            {
                "uid": "The user ID. This parameter must be the same as the user ID passed in when the user joined the channel."
            },
            {
                "token": "The RTM token for authentication. You can generate the RTM token in the following ways:\n Use to generate a temporary token.\n Deploy your own server for generating tokens. \n The uid or userAccount for generating the RTM token is the combination of the roomName and uid set in enterRoom . For example, if roomName is test and uid is 123, the uid or userAccount filled in when generating the RTM token is test123."
            },
            {
                "roomName": "The room name. This parameter must be the same as the channel name filled in when you join the channel."
            }
        ],
        "returns": "",
        "is_hide": true
    },
    {
        "id": "api_iaudiodevicemanager_stopaudiodeviceloopbacktest",
        "name": "stopAudioDeviceLoopbackTest",
        "description": "Stops the audio device loopback test.\nEnsure that you call this method before joining a channel.\n Ensure that you call this method to stop the loopback test after calling the startAudioDeviceLoopbackTest method.",
        "parameters": [],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "enum_areacode",
        "name": "AreaCode",
        "description": "The region for connection, i.e., the region where the server the SDK connects to is located.\n",
        "parameters": [
            {
                "areaCodeCn": "Mainland China."
            },
            {
                "areaCodeNa": "North America."
            },
            {
                "areaCodeEu": "Europe."
            },
            {
                "areaCodeAs": "Asia, excluding Mainland China."
            },
            {
                "areaCodeJp": "Japan."
            },
            {
                "areaCodeIn": "India."
            },
            {
                "areaCodeGlob": "Global."
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "api_playalleffects",
        "name": "playAllEffects",
        "description": "Plays all audio effects.\nAfter calling preloadEffect multiple times to preload multiple audio effects into the memory, you can call this method to play all the specified audio effects for all users in the channel.",
        "parameters": [
            {
                "loopCount": "The number of times the audio effect loops:\n -1: Play the audio effect in an indefinite loop until you call stopEffect or stopAllEffects .\n 0: Play the audio effect once.\n 1: Play the audio effect twice.\n "
            },
            {
                "pitch": "The pitch of the audio effect. The value ranges between 0.5 and 2.0. The default value is 1.0 (original pitch). The lower the value, the lower the pitch.\n "
            },
            {
                "pan": "The spatial position of the audio effect. The value ranges between -1.0 and 1.0:\n -1.0: The audio effect shows on the left.\n 0: The audio effect shows ahead.\n 1.0: The audio effect shows on the right.\n "
            },
            {
                "gain": "The volume of the audio effect. The value range is [0, 100]. The default value is 100 (original volume). The smaller the value, the lower the volume.\n "
            },
            {
                "publish": "Whether to publish the audio effect to the remote users:\n true: Publish the audio effect to the remote users. Both the local user and remote users can hear the audio effect.\n false: Do not publish the audio effect to the remote users. Only the local user can hear the audio effect.\n "
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "callback_onjoinchannelsuccess",
        "name": "onJoinChannelSuccess",
        "description": "Occurs when a user joins a channel.\nThis callback notifies the application that a user joins a specified channel.",
        "parameters": [
            {
                "connection": "The connection information. See RtcConnection ."
            },
            {
                "elapsed": "The time elapsed (ms) from the local user calling joinChannelWithOptions until the SDK triggers this callback."
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "api_updatescreencaptureregion2",
        "name": "updateScreenCaptureRegion [2/2]",
        "description": "Updates the screen sharing region.\n",
        "parameters": [
            {}
        ],
        "returns": "",
        "is_hide": true
    },
    {
        "id": "enum_watermarkfitmode",
        "name": "WatermarkFitMode",
        "description": "The adaptation mode of the watermark.\n",
        "parameters": [
            {
                "fitModeCoverPosition": "Use the positionInLandscapeMode and positionInPortraitMode values you set in WatermarkOptions . The settings in WatermarkRatio are invalid."
            },
            {
                "fitModeUseImageRatio": "Use the value you set in WatermarkRatio . The settings in positionInLandscapeMode and positionInPortraitMode in WatermarkOptions are invalid."
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "api_ivideodevicemanager_enumeratevideodevices",
        "name": "enumerateVideoDevices",
        "description": "Enumerates the video devices.\n",
        "parameters": [],
        "returns": "Success: A VideoDeviceInfo array including all video devices in the system.\n Failure: An empty array.",
        "is_hide": false
    },
    {
        "id": "api_iaudiodevicemanager_startrecordingdevicetest_ng",
        "name": "startRecordingDeviceTest",
        "description": "Starts the audio recording device test.\nThis method tests whether the audio capture device works properly. After calling this method, the SDK triggers the onAudioVolumeIndication callback at the time interval set in this method, which reports uid = 0 and the volume information of the capturing device.\n Ensure that you call this method before joining a channel.",
        "parameters": [
            {
                "indicationInterval": "The time interval (ms) at which the SDK triggers the onAudioVolumeIndication callback. Agora recommends setting a value greater than 200 ms. This value must not be less than 10 ms; otherwise, you can not receive the onAudioVolumeIndication callback."
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "api_iscamerafocussupported",
        "name": "isCameraFocusSupported",
        "description": "Check whether the device supports the manual focus function.\nThis method is for Android and iOS only.\n Call this method before calling joinChannelWithOptions , enableVideo , or enableLocalVideo , depending on which method you use to turn on your local camera.",
        "parameters": [],
        "returns": "true: The device supports the manual focus function.\n false: The device does not support the manual focus function.",
        "is_hide": false
    },
    {
        "id": "api_ivideodevicemanager_setdevice",
        "name": "setDevice",
        "description": "Specifies the video capture device with the device ID.\nPlugging or unplugging a device does not change its device ID.",
        "parameters": [
            {
                "deviceIdUTF8": "The device ID. You can get the device ID by calling enumerateVideoDevices .\n The maximum length is MaxDeviceIdLengthType .\n "
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "api_updatechannelmediarelay",
        "name": "updateChannelMediaRelay",
        "description": "Updates the channels for media stream relay.\nAfter the media relay starts, if you want to relay the media stream to more channels, or leave the current relay channel, you can call this method.\n After a successful method call, the SDK triggers the onChannelMediaRelayEvent callback with the relayEventPacketUpdateDestChannel (7) state code.\n Call the method after successfully calling the startChannelMediaRelay method and receiving onChannelMediaRelayStateChanged (relayStateRunning, relayOk); otherwise, the method call fails.",
        "parameters": [
            {
                "configuration": "The configuration of the media stream relay. "
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "enum_channelprofiletype_ng",
        "name": "ChannelProfileType",
        "description": "The channel profile.\n",
        "parameters": [
            {
                "channelProfileCommunication": "0: Communication. Use this profile when there are only two users in the channel."
            },
            {
                "channelProfileLiveBroadcasting": "1: Live streaming. Live streaming. Use this profile when there are more than two users in the channel."
            },
            {
                "channelProfileGame": "2: Gaming. This profile is deprecated."
            },
            {
                "channelProfileCloudGaming": "Cloud gaming. The scenario is optimized for latency. Use this profile if the use case requires frequent interactions between users."
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "callback_onuplinknetworkinfoupdated",
        "name": "onUplinkNetworkInfoUpdated",
        "description": "Occurs when the uplink network information changes.\nThe SDK triggers this callback when the uplink network information changes.\n This callback only applies to scenarios where you push externally encoded video data in H.264 format to the SDK.",
        "parameters": [
            {
                "info": "The uplink network information. See UplinkNetworkInfo ."
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "api_setdirectcdnstreamingaudioconfiguration",
        "name": "setDirectCdnStreamingAudioConfiguration",
        "description": "Sets the audio profile of the media streams directly pushed to the CDN by the host.\nWhen you set the publishMicrophoneTrack or publishCustomAudioTrack in the DirectCdnStreamingMediaOptions as true to capture audios, you can call this method to set the audio profile.",
        "parameters": [
            {
                "profile": "The audio profile, including the sampling rate, bitrate, encoding mode, and the number of channels. See AudioProfileType .\n "
            }
        ],
        "returns": "",
        "is_hide": true
    },
    {
        "id": "class_deviceinfo",
        "name": "DeviceInfo",
        "description": "The audio device information.\nThis class is for Android only.",
        "parameters": [
            {
                "isLowLatencyAudioSupported": "Whether the audio device supports ultra-low-latency capture and playback:\n true: The device supports ultra-low-latency capture and playback.\n false: The device does not support ultra-low-latency capture and playback.\n "
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "api_adjustaudiomixingvolume",
        "name": "adjustAudioMixingVolume",
        "description": "Adjusts the volume during audio mixing.\nThis method adjusts the audio mixing volume on both the local client and remote clients. Call this method after the startAudioMixing method.",
        "parameters": [
            {
                "volume": "Audio mixing volume. The value ranges between 0 and 100. The default value is 100, the original volume."
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "class_audiorecordingconfiguration_ng",
        "name": "AudioRecordingConfiguration",
        "description": "Recording configuration.\n",
        "parameters": [
            {
                "filePath": "The absolute path (including the filename extensions) of the recording file. For example: C:\\music\\audio.mp4. Ensure that the path for the recording file exists and is writable.\n "
            },
            {
                "encode": "Whether to encode the audio data: true\n : Encode audio data in AAC.\n false\n : (Default) Do not encode audio data, but save the recorded audio data directly.\n "
            },
            {
                "sampleRate": "Recording sample rate (Hz). 16000\n (Default) 32000\n 44100\n 48000 If you set this parameter to 44100 or 48000, Agora recommends recording WAV files, or AAC files with quality to be AgoraAudioRecordingQualityMedium or AgoraAudioRecordingQualityHigh for better recording quality. "
            },
            {
                "fileRecordingType": "Recording content. See AudioFileRecordingType .\n "
            },
            {
                "quality": "Recording quality. See audiorecordingqualitytype . This parameter applies to AAC files only.\n "
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "api_stoppreview2",
        "name": "stopPreview",
        "description": "Stops the local video preview.\nAfter calling startPreview to start the preview, if you want to close the local video preview, please call this method.\n Please call this method before joining a channel or after leaving a channel.",
        "parameters": [
            {
                "sourceType": "The type of the video source, see VideoSourceType ."
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "enum_contentinspectdevicetype",
        "name": "CONTENT_INSPECT_DEVICE_TYPE",
        "description": "在设备端进行内容审核的类型。\n",
        "parameters": [
            {
                "null": "1：通过客户端 Agora SDK 进行鉴黄。"
            }
        ],
        "returns": "",
        "is_hide": true
    },
    {
        "id": "api_imediaplayer_setview",
        "name": "setView",
        "description": "Sets the view.\n",
        "parameters": [],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "api_setlocalvoicechanger",
        "name": "setLocalVoiceChanger",
        "description": "Sets the local voice changer option.\nDeprecated:\n This method is deprecated, please use the following method instead: \n setAudioEffectPreset : Audio effects. \n setVoiceBeautifierPreset : Voice beautifier effects. \n setVoiceConversionPreset : Voice conversion effects. \n This method can be used to set the local voice effect for users in a COMMUNICATION channel or hosts in a LIVE_BROADCASTING channel. After successfully calling this method, all users in the channel can hear the voice with reverberation.\n VOICE_CHANGER_XXX: Changes the local voice to an old man, a little boy, or the Hulk. Applies to the voice talk scenario.\n VOICE_BEAUTY_XXX: Beautifies the local voice by making it sound more vigorous, resounding, or adding spacial resonance. Applies to the voice talk and singing scenario.\n GENERAL_VOICE_BEAUTY_XXX: Adds gender-based beautification effect to the local voice. Applies to the voice talk scenario. For a male voice: Adds magnetism to the voice. For a male voice: Adds magnetism to the voice. For a female voice: Adds freshness or vitality to the voice. \n To achieve better voice effect quality, Agora recommends setting the profile parameter in setAudioProfile as audioProfileMusicHighQuality (4) oraudioProfileMusicHighQualityStereo (5).\n This method works best with the human voice, and Agora does not recommend using it for audio containing music and a human voice.\n You can call this method either before or after joining a channel.",
        "parameters": [
            {
                "voiceChanger": "The local voice changer option. The default value is VOICE_CHANGER_OFF , which means the original voice. For more details, see VOICE_CHANGER_PRESET . The gender-based beatification effect works best only when assigned a proper gender. Use GENERAL_BEAUTY_VOICE_MALE_MAGNETIC for male and use GENERAL_BEAUTY_VOICE_FEMALE_FRESH and GENERAL_BEAUTY_VOICE_FEMALE_VITALITY for female. Failure to do so can lead to voice distortion.\n "
            }
        ],
        "returns": "",
        "is_hide": true
    },
    {
        "id": "api_setadvancedaudiooptions",
        "name": "setAdvancedAudioOptions",
        "description": "Sets audio advanced options.\nIf you have advanced audio processing requirements, such as capturing and sending stereo audio, you can call this method to set advanced audio options. This method is for Android and iOS only.\n Call this method after calling joinChannelWithOptions , enableAudio and enableLocalAudio .",
        "parameters": [],
        "returns": "The advanced options for audio. See AdvancedAudioOptions .",
        "is_hide": false
    },
    {
        "id": "api_setlocalrendermode",
        "name": "setLocalRenderMode [1/2]",
        "description": "Sets the local video display mode.\nDeprecated:\n This method is deprecated. Use setLocalRenderMode [2/2] instead. Call this method to set the local video display mode. This method can be called multiple times during a call to change the display mode.",
        "parameters": [
            {
                "renderMode": "The local video display mode. See RenderModeType .\n "
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "callback_onaudiodevicestatechanged",
        "name": "onAudioDeviceStateChanged",
        "description": "Occurs when the audio device state changes.\nThis callback notifies the application that the system's audio device state is changed. For example, a headset is unplugged from the device.\n This method is for Windows and macOS only.",
        "parameters": [
            {
                "deviceId": "The device ID."
            },
            {
                "deviceType": "The evice type. See MediaDeviceType ."
            },
            {
                "deviceState": "The device state.\n On macOS:\n 0: The device is ready for use.\n 8: The device is not connected. On Windows: see MediaDeviceStateType .\n "
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "class_iaudioframeobserver",
        "name": "IAudioFrameObserver",
        "description": "The audio frame observer.\n",
        "parameters": [],
        "returns": "",
        "is_hide": true
    },
    {
        "id": "enum_connectionchangedreasontype",
        "name": "ConnectionChangedReasonType",
        "description": "Reasons causing the change of the connection state.\n",
        "parameters": [
            {
                "connectionChangedConnecting": "0: The SDK is connecting to the Agora edge server."
            },
            {
                "connectionChangedJoinSuccess": "1: The SDK has joined the channel successfully."
            },
            {
                "connectionChangedInterrupted": "2: The connection between the SDK and the Agora edge server is interrupted."
            },
            {
                "connectionChangedBannedByServer": "3: The connection between the SDK and the Agora edge server is banned by the Agora edge server. This error occurs when the user is kicked out of the channel by the server."
            },
            {
                "connectionChangedJoinFailed": "4: The SDK fails to join the channel. When the SDK fails to join the channel for more than 20 minutes, this error occurs and the SDK stops reconnecting to the channel."
            },
            {
                "connectionChangedLeaveChannel": "5: The SDK has left the channel."
            },
            {
                "connectionChangedInvalidAppId": "6: The connection failed because the App ID is not valid. Please rejoin the channel with a valid App ID."
            },
            {
                "connectionChangedInvalidChannelName": "7: The connection failed since channel name is not valid. Please rejoin the channel with a valid channel name."
            },
            {
                "connectionChangedInvalidToken": "8: The connection failed because the token is not valid. Typical reasons include:\n The App Certificate for the project is enabled in Agora Console, but you do not use a token when joining the channel. If you enable the App Certificate, you must use a token to join the channel.\n Theuid specified when calling joinChannelWithOptions to join the channel is inconsistent with the uid passed in when generating the token. "
            },
            {
                "connectionChangedTokenExpired": "9: The connection failed since token is expired."
            },
            {
                "connectionChangedRejectedByServer": "10: The connection is rejected by server. Typical reasons include:\n The user is already in the channel and still calls a method, for example,joinChannelWithOptions, to join the channel. Stop calling this method to clear this error.\n The user tries to join the channel when calling for a call test. The user needs to call the channel after the call test ends. "
            },
            {
                "connectionChangedSettingProxyServer": "11: The connection state changed to reconnecting because the SDK has set a proxy server."
            },
            {
                "connectionChangedRenewToken": "12: The connection state changed because the token is renewed."
            },
            {
                "connectionChangedClientIpAddressChanged": "13: The IP address of the client has changed, possibly because the network type, IP address, or port has been changed."
            },
            {
                "connectionChangedKeepAliveTimeout": "14: Timeout for the keep-alive of the connection between the SDK and the Agora edge server. The connection state changes to ."
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "api_setcameratorchon",
        "name": "setCameraTorchOn",
        "description": "Enables the camera flash.\nThis method is for Android and iOS only.\n Call this method before calling joinChannelWithOptions , enableVideo , or enableLocalVideo , depending on which method you use to turn on your local camera.",
        "parameters": [
            {
                "isOn": "Whether to turn on the camera flash:\n true: Turn on the flash.\n false: (Default) Turn off the flash. "
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "callback_onvirtualbackgroundsourceenabled",
        "name": "",
        "description": "报告虚拟背景是否成功开启。 （BETA 功能）\n调用 enableVirtualBackground 后，SDK 触发该回调报告虚拟背景是否成功开启。\n 如果虚拟背景中自定义的背景图为 PNG、JPG 格式的图片，该回调的触发时机会延迟到图片读取完毕。",
        "parameters": [
            {
                "enabled": "是否成功开启虚拟背景： true: 成功开启虚拟背景。\n false: 未成功开启虚拟背景。\n "
            },
            {
                "reason": "The reason why virtual background is not successfully enabled. "
            }
        ],
        "returns": "",
        "is_hide": true
    },
    {
        "id": "callback_onextensionevent",
        "name": "onExtensionEvent",
        "description": "The event callback of the extension.\nTo listen for events while the extension is running, you need to register this callback.",
        "parameters": [
            {
                "value": "The value of the extension key."
            },
            {
                "key": "The key of the extension."
            },
            {
                "provider": "The name of the extension provider."
            },
            {
                "extName": "The name of the extension."
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "callback_onconnectionbanned",
        "name": "onConnectionBanned",
        "description": "Occurs when the connection is banned by the Agora server.\nDeprecated:\n Please use onConnectionStateChanged instead.",
        "parameters": [
            {
                "connection": "The connection information. See RtcConnection ."
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "class_iaudioencodedframeobserver",
        "name": "IAudioEncodedFrameObserver",
        "description": "The encoded audio observer.\n",
        "parameters": [],
        "returns": "",
        "is_hide": true
    },
    {
        "id": "callback_iaudioframeobserver_onplaybackaudioframe",
        "name": "onPlaybackAudioFrame",
        "description": "Gets the audio frame for playback.\nIf you want to set the format of the audio frame for playback, Agora recommends that you call the registerAudioFrameObserver method to set the format of the audio frame after calling the setPlaybackAudioFrameParameters method to register an audio frame observer.",
        "parameters": [],
        "returns": "Reserved for future use.",
        "is_hide": true
    },
    {
        "id": "api_setclientrole",
        "name": "setClientRole [1/2]",
        "description": "Sets the client role.\nIf you call this method to set the user's role as the host before joining the channel and set the local video property through the setupLocalVideo method, the local video preview is automatically enabled when the user joins the channel.\n You can call this method either before or after joining the channel to set the user role as audience or host.\n If you call this method to switch the user role after joining the channel, the SDK triggers the following callbacks:\n The local client: onClientRoleChanged .\n The remote client: onUserJoined or onUserOffline (userOfflineBecomeAudience).",
        "parameters": [
            {
                "role": ""
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "callback_onrtmpstreamingstatechanged",
        "name": "onRtmpStreamingStateChanged",
        "description": "Occurs when the media push state changes.\nWhen the media push state changes, the SDK triggers this callback and reports the URL address and the current state of the media push. This callback indicates the state of the media push. When exceptions occur, you can troubleshoot issues by referring to the detailed error descriptions in the error code.",
        "parameters": [
            {
                "url": "The URL address where the state of the media push changes.\n "
            },
            {
                "state": "The current state of the media push. See RtmpStreamPublishState ."
            },
            {
                "errCode": "The detailed error information for the media push. See RtmpStreamPublishErrorType ."
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "callback_onstreammessageerror",
        "name": "onStreamMessageError",
        "description": "Occurs when the local user receives the data stream from the remote user.\nThe SDK triggers this callback when the local user fails to receive the stream sendStreamMessage message that the remote user sends by calling the method.",
        "parameters": [
            {
                "connection": "The connection information. See RtcConnection ."
            },
            {
                "remoteUid": "The ID of the remote user sending the message."
            },
            {
                "streamId": "The stream ID of the received message."
            },
            {
                "code": "The error code."
            },
            {
                "missed": "The number of lost messages."
            },
            {
                "cached": "Number of incoming cached messages when the data stream is interrupted."
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "api_imediaplayer_getstreaminfo",
        "name": "getStreamInfo",
        "description": "Gets the detailed information of the media stream.\nCall this method after calling getStreamCount .",
        "parameters": [
            {
                "index": "The index of the media stream."
            }
        ],
        "returns": "If the call succeeds, returns the detailed information of the media stream. See PlayerStreamInfo .\n If the call fails, returns NULL.",
        "is_hide": false
    },
    {
        "id": "api_geterrordescription",
        "name": "getErrorDescription",
        "description": "Gets the warning or error description.\n",
        "parameters": [
            {
                "code": "The error code or warning code reported by the SDK."
            }
        ],
        "returns": "The specific error or warning description.",
        "is_hide": false
    },
    {
        "id": "enum_encryptionerrortype",
        "name": "EncryptionErrorType",
        "description": "Encryption error type.\n",
        "parameters": [
            {
                "encryptionErrorInternalFailure": "0: Internal reason."
            },
            {
                "encryptionErrorDecryptionFailure": "1: Decryption errors.\nEnsure that the receiver and the sender use the same encryption mode and key."
            },
            {
                "encryptionErrorEncryptionFailure": "2: Encryption errors."
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "class_livetranscoding",
        "name": "LiveTranscoding",
        "description": "Transcoding configurations for Media Push.\n",
        "parameters": [
            {
                "width": "The width of the video in pixels. The default value is 360. When pushing video streams to the CDN, the value range of width is [64,1920]. If the value is less than 64, Agora server automatically adjusts it to 64; if the value is greater than 1920, Agora server automatically adjusts it to 1920.\n When pushing audio streams to the CDN, set width and height as 0.\n "
            },
            {
                "height": "The height of the video in pixels. The default value is 640. When pushing video streams to the CDN, the value range of height is [64,1080]. If the value is less than 64, Agora server automatically adjusts it to 64; if the value is greater than 1080, Agora server automatically adjusts it to 1080.\n When pushing audio streams to the CDN, set width and height as 0.\n "
            },
            {
                "videoBitrate": "Bitrate of the output video stream for Media Push in Kbps. The default value is 400 Kbps.\n "
            },
            {
                "videoFrameRate": "Frame rate (in fps) of the output video stream set for Media Push. The default value is 15 , and the value range is (0,30].\n The Agora server adjusts any value over 30 to 30."
            },
            {
                "lowLatency": " Deprecated\n This parameter is deprecated. Latency mode: true: Low latency with unassured quality.\n false: (Default) High latency with assured quality.\n "
            },
            {
                "videoGop": "GOP (Group of Pictures) in fps of the video frames for Media Push. The default value is 30."
            },
            {
                "videoCodecProfile": "Video codec profile type for Media Push. Set it as 66, 77, or 100 (default). See VIDEO_CODEC_PROFILE_TYPE for details.\n If you set this parameter to any other value, Agora adjusts it to the default value."
            },
            {
                "videoCodecType": "Video codec profile types for Media Push. See VideoCodecTypeForStream ."
            },
            {
                "transcodingUsers": "Manages the user layout configuration in the Media Push. Agora supports a maximum of 17 transcoding users in a Media Push channel. See TranscodingUser .\n "
            },
            {
                "transcodingExtraInfo": "Reserved property. Extra user-defined information to send SEI for the H.264/H.265 video stream to the CDN client. Maximum length: 4096 bytes. For more information on SEI, see SEI-related questions.\n "
            },
            {
                "backgroundColor": "The background color in RGB hex value. Value only. Do not include a preceeding #. For example, 0xFFB6C1 (light pink). The default value is 0x000000 (black).\n "
            },
            {
                "userCount": "The number of users in the video mixing. The value range is [0,17].\n "
            },
            {
                "metadata": " Deprecated\n This parameter is deprecated. The metadata sent to the CDN client.\n "
            },
            {
                "watermark": "The watermark on the live video. The image format needs to be PNG. See RtcImage .\n You can add one watermark, or add multiple watermarks using an array. \n "
            },
            {
                "backgroundImage": "The number of background images on the live video. The image format needs to be PNG. See RtcImage .\n You can add a background image or use an array to add multiple background images. This parameter is used with backgroundImageCount.\n "
            },
            {
                "audioSampleRate": "The audio sampling rate (Hz) of the output media stream. See AudioSampleRateType .\n "
            },
            {
                "audioBitrate": "Bitrate (Kbps) of the audio output stream for Media Push. The default value is 48, and the highest value is 128.\n "
            },
            {
                "audioChannels": "The number of audio channels for Media Push. Agora recommends choosing 1 (mono), or 2 (stereo) audio channels. Special players are required if you choose 3, 4, or 5. 1: (Default) Mono.\n 2: Stereo.\n 3: Three audio channels.\n 4: Four audio channels.\n 5: Five audio channels.\n "
            },
            {
                "audioCodecProfile": "Audio codec profile type for Media Push. See AudioCodecProfileType ."
            },
            {
                "watermarkCount": "The number of watermarks on the live video. The total number of watermarks and background images can range from 0 to 10. This parameter is used with watermark."
            },
            {
                "backgroundImageCount": "The number of background images on the live video. The total number of watermarks and background images can range from 0 to 10. This parameter is used with backgroundImage."
            },
            {
                "advancedFeatures": "Advanced features of the Media Push with transcoding. See LiveStreamAdvancedFeature ."
            },
            {
                "advancedFeatureCount": "The number of enabled advanced features. The default value is 0."
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "api_iscamerazoomsupported",
        "name": "isCameraZoomSupported",
        "description": "Checks whether the device supports camera zoom.\nThis method is for Android and iOS only.\n Call this method before calling joinChannelWithOptions , enableVideo , or enableLocalVideo , depending on which method you use to turn on your local camera.",
        "parameters": [],
        "returns": "true: The device supports camera zoom.\n false: The device does not support camera zoom.",
        "is_hide": false
    },
    {
        "id": "class_useraudiospectruminfo",
        "name": "UserAudioSpectrumInfo",
        "description": "Audio spectrum information of the remote user.\n",
        "parameters": [
            {
                "uid": "The user ID of the remote user."
            },
            {
                "spectrumData": "Audio spectrum information of the remote user.  AudioSpectrumData \n "
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "class_datastreamconfig",
        "name": "DataStreamConfig",
        "description": "The configurations for the data stream.\nThe following table shows the SDK behaviors under different parameter settings:",
        "parameters": [
            {
                "syncWithAudio": "Whether to synchronize the data packet with the published audio packet.\n true: Synchronize the data packet with the audio packet.\n false: Do not synchronize the data packet with the audio packet.\n When you set the data packet to synchronize with the audio, then if the data packet delay is within the audio delay, the SDK triggers the onStreamMessage callback when the synchronized audio packet is played out. Do not set this parameter as true if you need the receiver to receive the data packet immediately. Agora recommends that you set this parameter to `true` only when you need to implement specific functions, for example, lyric synchronization.\n "
            },
            {
                "ordered": "Whether the SDK guarantees that the receiver receives the data in the sent order.\n true: Guarantee that the receiver receives the data in the sent order.\n false: Do not guarantee that the receiver receives the data in the sent order.\n Do not set this parameter as true if you need the receiver to receive the data packet immediately.\n "
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "api_imediaplayer_selectaudiotrack",
        "name": "selectAudioTrack",
        "description": "Selects the audio track used during playback.\nAfter getting the track index of the audio file, you can call this method to specify any track to play. For example, if different tracks of a multi-track file store songs in different languages, you can call this method to set the playback language.\n You need to call this method after calling getStreamInfo to get the audio stream index value.",
        "parameters": [
            {
                "index": "The index of the audio track."
            }
        ],
        "returns": "",
        "is_hide": true
    },
    {
        "id": "enum_audioreverbtype",
        "name": "AudioReverbType",
        "description": "Audio reverberation types.\n",
        "parameters": [
            {
                "audioReverbDryLevel": "0: The level of the dry signal (dB). The value is between -20 and 10."
            },
            {
                "audioReverbWetLevel": "1: The level of the early reflection signal (wet signal) (dB). The value is between -20 and 10."
            },
            {
                "audioReverbRoomSize": "2: The room size of the reflection. The value is between 0 and 100."
            },
            {
                "audioReverbWetDelay": "3: The length of the initial delay of the wet signal (ms). The value is between 0 and 200."
            },
            {
                "audioReverbStrength": "4: The reverberation strength. The value is between 0 and 100."
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "api_resumealleffects",
        "name": "resumeAllEffects",
        "description": "Resumes playing all audio effects.\n",
        "parameters": [],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "api_imediaplayer_registeraudioframeobserver",
        "name": "registerAudioFrameObserver [1/2]",
        "description": "Registers an audio frame observer object.\n",
        "parameters": [
            {
                "observer": "The audio frame observer, reporting the reception of each audio frame. See IAudioFrameObserver ."
            }
        ],
        "returns": "",
        "is_hide": true
    },
    {
        "id": "callback_onfirstremoteaudiodecoded",
        "name": "onFirstRemoteAudioDecoded",
        "description": "Occurs when the first audio frame sent by a specified remote user is received.\nDeprecated:\n Use onRemoteAudioStateChanged instead. The SDK triggers this callback under one of the following circumstances:\n The remote user joins the channel and sends the audio stream.\n The remote user stops sending the audio stream and re-sends it after 15 seconds, and the possible reasons include:\n The remote user leaves the channel.\n The remote user is offline.\n The remote user calls muteLocalAudioStream to stop sending the video stream.\n The remote user calls disableAudio to disable video.",
        "parameters": [
            {
                "connection": "The connection information. See RtcConnection ."
            },
            {
                "uid": "The user ID of the remote user."
            },
            {
                "elapsed": "The time elapsed (ms) from the local user calling the joinChannelWithOptions method until the SDK triggers this callback."
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "enum_experiencepoorreason",
        "name": "EXPERIENCE_POOR_REASON",
        "description": "Reasons why the QoE of the local user when receiving a remote audio stream is poor.\n",
        "parameters": [
            {
                "EXPERIENCE_REASON_NONE": "0: No reason, indicating a good QoE of the local user."
            },
            {
                "REMOTE_NETWORK_QUALITY_POOR": "1: The remote user's network quality is poor."
            },
            {
                "LOCAL_NETWORK_QUALITY_POOR": "2: The local user's network quality is poor."
            },
            {
                "WIRELESS_SIGNAL_POOR": "4: The local user's Wi-Fi or mobile network signal is weak."
            },
            {
                "WIFI_BLUETOOTH_COEXIST": "8: The local user enables both Wi-Fi and bluetooth, and their signals interfere with each other. As a result, audio transmission quality is undermined."
            }
        ],
        "returns": "",
        "is_hide": true
    },
    {
        "id": "class_videoviewcontroller",
        "name": "VideoViewController",
        "description": "A AgoraVideoView controller for rendering local and remote video.\nOn different platforms, the default view corresponding to this class is different:\n Android: . If you want to use , set the useAndroidSurfaceView property to true.\n iOS: . If you want to use Flutter Texture, set the useFlutterTexture property to true.\n macOS and Windows: .",
        "parameters": [
            {
                "rtcEngine": " RtcEngine ."
            },
            {
                "canvas": "Local video display properties. See VideoCanvas ."
            },
            {
                "connection": "The connection information. See RtcConnection ."
            },
            {
                "useFlutterTexture": "Whether to use FlutterTexture to render video:\n true: Use FlutterTexture to render video.\n false: Do not use FlutterTexture to render video. FlutterTexture applies to iOS, macOS and Windows platforms."
            },
            {
                "useAndroidSurfaceView": "Whether to use Android SurfaceView to render video:\n true: Use Android SurfaceView to render video.\n false: Do not use Android SurfaceView to render video. Android SurfaceView applies to Android platform only."
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "enum_remotevideostate",
        "name": "RemoteVideoState",
        "description": "The state of the remote video.\n",
        "parameters": [
            {
                "remoteVideoStateStopped": "0: The remote audo is in the initial state. The SDK reports this state in the case of remoteVideoStateReasonLocalMuted, remoteVideoStateReasonRemoteMuted, or remoteVideoStateReasonRemoteOffline."
            },
            {
                "remoteVideoStateStarting": "1: The first remote audio packet is received."
            },
            {
                "remoteVideoStateDecoding": "2: The remote audio stream is decoded and plays normally. The SDK reports this state in the case of remoteVideoStateReasonNetworkRecovery, ,remoteVideoStateReasonLocalUnmuted,remoteVideoStateReasonRemoteUnmuted or remoteVideoStateReasonAudioFallbackRecovery."
            },
            {
                "remoteVideoStateFrozen": "3: The remote video is frozen. The SDK reports this state in the case of remoteVideoStateReasonNetworkCongestion or remoteVideoStateReasonAudioFallback."
            },
            {
                "remoteVideoStateFailed": "4: The remote video fails to start. The SDK reports this state in the case of remoteVideoStateReasonInternal."
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "callback_onuserjoined",
        "name": "onUserJoined",
        "description": " Occurs when a remote user (in the communication profile)/ host (in the live streaming profile) leaves the channel.\nIn a communication channel, this callback indicates that a remote user joins the channel. The SDK also triggers this callback to report the existing users in the channel when a user joins the channel.\n In a live-broadcast channel, this callback indicates that a host joins the channel. The SDK also triggers this callback to report the existing hosts in the channel when a host joins the channel. Agora recommends limiting the number of hosts to 17. The SDK triggers this callback under one of the following circumstances:\n A remote user/hostjoinChannelWithOptions joins the channel by calling the method.\n A remote user switches the user role to the host after joining the channel.\n A remote user/host rejoins the channel after a network interruption.",
        "parameters": [
            {
                "connection": "The connection information. See RtcConnection ."
            },
            {
                "remoteUid": "The ID of the user or host who joins the channel."
            },
            {
                "elapsed": "Time delay (ms) fromjoinChannelWithOptions the local user calling until this callback is triggered."
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "api_setremotevideostreamtype",
        "name": "setRemoteVideoStreamType",
        "description": "Sets the stream type of the remote video.\nUnder limited network conditions, if the publisher has not disabled the dual-stream mode using enableDualStreamMode (false), the receiver can choose to receive either the high-quality video stream (the high resolution, and high bitrate video stream) or the low-quality video stream (the low resolution, and low bitrate video stream). The high-quality video stream has a higher resolution and bitrate, and the low-quality video stream has a lower resolution and bitrate.\n By default, users receive the high-quality video stream. Call this method if you want to switch to the low-quality video stream. This method allows the app to adjust the corresponding video stream type based on the size of the video window to reduce the bandwidth and resources. The aspect ratio of the low-quality video stream is the same as the high-quality video stream. Once the resolution of the high-quality video stream is set, the system automatically sets the resolution, frame rate, and bitrate of the low-quality video stream.\n The method result returns in the onApiCallExecuted callback.\n You can call this method either before or after joining a channel. If you call both setRemoteVideoStreamType and setRemoteDefaultVideoStreamType , the setting of setRemoteVideoStreamType takes effect.",
        "parameters": [
            {
                "uid": "The user ID."
            },
            {
                "streamType": "The video stream type: VideoStreamType .\n "
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "api_startprimaryscreencapture",
        "name": "startPrimaryScreenCapture",
        "description": "Stop sharing the first screen.\n",
        "parameters": [
            {
                "config": "The configuration of the captured screen. See ScreenCaptureConfiguration .\n "
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "class_clientroleoptions",
        "name": "ClientRoleOptions",
        "description": "The detailed options of a user.\n",
        "parameters": [
            {
                "audienceLatencyLevel": "The latency level of an audience member in interactive live streaming. See AudienceLatencyLevelType ."
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "callback_imediaplayersourceobserver_onplayersrcinfochanged",
        "name": "onPlayerSrcInfoChanged",
        "description": "Occurs when the video bitrate of the media resource changes.\n",
        "parameters": [
            {
                "from": "Information about the video bitrate of the media resource being played. See SrcInfo ."
            },
            {
                "to": "Information about the changed video bitrate of media resource being played. See SrcInfo ."
            }
        ],
        "returns": "",
        "is_hide": true
    },
    {
        "id": "callback_ipacketobserver_onreceiveaudiopacket",
        "name": "onReceiveAudioPacket",
        "description": "Occurs when the local user receives an audio packet.\n",
        "parameters": [
            {
                "packet": "The received audio packet, see Packet ."
            }
        ],
        "returns": "true: The audio packet is received successfully.\n false: The audio packet is discarded.",
        "is_hide": true
    },
    {
        "id": "api_imediaengine_pushencodedvideoimage",
        "name": "pushEncodedVideoImage [1/2]",
        "description": "Pushes the external encoded video frame to the SDK.\nAfter calling setExternalVideoSource to enable external video source and set the sourceType parameter to encodedVideoFrame, you can call this method to push the encoded external video frame to the SDK.",
        "parameters": [
            {}
        ],
        "returns": "0: Pushes the external encoded video frame to the SDK successfully.\n < 0: Fails to push the external encoded video frame to the SDK.",
        "is_hide": true
    },
    {
        "id": "callback_ivideoframeobserver_getvideoframeprocessmode",
        "name": "getVideoFrameProcessMode",
        "description": "Occurs each time the SDK receives a video frame and prompts you to set the process mode of the video frame.\nAfter you successfully register the video frame observer, the SDK triggers this callback each time it receives a video frame. You need to set your preferred process mode in the return value of this callback.",
        "parameters": [],
        "returns": "",
        "is_hide": true
    },
    {
        "id": "api_setcameraautoexposurefacemodeenabled",
        "name": "setCameraAutoExposureFaceModeEnabled",
        "description": "Sets whether to enable auto exposure.\nThis method applies to iOS only.\n Call this method before calling joinChannelWithOptions , enableVideo , or enableLocalVideo , depending on which method you use to turn on your local camera.",
        "parameters": [
            {
                "enabled": "Whether to enable auto exposure: true: Enable auto exposure.\n false: Disable auto exposure. "
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "class_srcinfo",
        "name": "SrcInfo",
        "description": "Information about the video bitrate of the media resource being played.\n",
        "parameters": [
            {
                "bitrateInKbps": "The video bitrate (Kbps) of the media resource being played."
            },
            {
                "name": "The name of the media resource."
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "api_setaudioeffectparameters",
        "name": "setAudioEffectParameters",
        "description": "Sets parameters for SDK preset audio effects.\nCall this method to set the following parameters for the local user who sends an audio stream:\n 3D voice effect: Sets the cycle period of the 3D voice effect.\n Pitch correction effect: Sets the basic mode and tonic pitch of the pitch correction effect. Different songs have different modes and tonic pitches. Agora recommends bounding this method with interface elements to enable users to adjust the pitch correction interactively. After setting the audio parameters, all users in the channel can hear the effect. You can call this method either before or after joining a channel.\n To get better audio effect quality, Agora recommends calling and setting scenario in setAudioProfile as audioScenarioGameStreaming(3) before calling this method.\n Do not set the profile parameter in setAudioProfile to audioProfileSpeechStandard(1)audioProfileIot or (6), or the method does not take effect.\n This method works best with the human voice. Agora does not recommend using this method for audio containing music.\n After calling setAudioEffectParameters, Agora recommends not calling the following methods, or the settings in setAudioEffectParameters are overridden: \n setAudioEffectPreset \n setVoiceBeautifierPreset \n setLocalVoicePitch \n setLocalVoiceEqualization \n setLocalVoiceReverb \n setVoiceBeautifierParameters \n setVoiceConversionPreset",
        "parameters": [
            {
                "preset": "The options for SDK preset audio effects:\n roomAcoustics3dVoice, 3D voice effect:\n You need to set the profile parameter in setAudioProfile to audioProfileMusicStandardStereo(3) or audioProfileMusicHighQualityStereo(5) before setting this enumerator; otherwise, the enumerator setting does not take effect.\n If the 3D voice effect is enabled, users need to use stereo audio playback devices to hear the anticipated voice effect. pitchCorrection, Pitch correction effect: To achieve better audio effect quality, Agora recommends setting the profile parameter in setAudioProfile to audioProfileMusicHighQuality(4) or audioProfileMusicHighQualityStereo(5) before setting this enumerator.\n "
            },
            {
                "param1": "If you set preset to roomAcoustics3dVoice , param1 indicates the cycle period of the 3D voice effect. The value range is [1,60], in seconds. The default value is 10, indicating that the voice moves around you every 10 seconds.\n If you set preset to pitchCorrection , param1 indicates the basic mode of the pitch correction effect:\n 1: (Default) Natural major scale.\n 2: Natural minor scale.\n 3: Japanese pentatonic scale. "
            },
            {
                "param2": "If you set preset to roomAcoustics3dVoice, you need to set param2 to 0.\n If you set preset to pitchCorrection, param2 indicates the tonic pitch of the pitch correction effect:\n 1: A\n 2: A#\n 3: B\n 4: (Default) C\n 5: C#\n 6: D\n 7: D#\n 8: E\n 9: F\n 10: F#\n 11: G\n 12: G# "
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "api_getcallid",
        "name": "getCallId",
        "description": "Retrieves the call ID.\nWhen a user joins a channel on a client, a callId is generated to identify the call from the client. Some methods, for example, rate and complain ,\nneed to be called after a call ends. These methods require the callId parameter.\n Call this method after joining a channel.",
        "parameters": [],
        "returns": "The current call ID.",
        "is_hide": false
    },
    {
        "id": "enum_mediastreamtype",
        "name": "MediaStreamType",
        "description": "The type of the media stream.\n",
        "parameters": [
            {
                "streamTypeUnknown": "0: The type is unknown."
            },
            {
                "streamTypeVideo": "1: The video stream."
            },
            {
                "streamTypeAudio": "2: The audio stream."
            },
            {
                "streamTypeSubtitle": "3: The subtitle stream."
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "class_lastmileprobeconfig",
        "name": "LastmileProbeConfig",
        "description": "Configurations of the last-mile network test.\n",
        "parameters": [
            {
                "probeUplink": "Sets whether to test the uplink network. Some users, for example, the audience members in a LIVE_BROADCASTING channel, do not need such a test.\n true: Test.\n false: Not test. "
            },
            {
                "probeDownlink": "Sets whether to test the downlink network:\n true: Test.\n false: Not test. "
            },
            {
                "expectedUplinkBitrate": "The expected maximum uplink bitrate (bps) of the local user. The value range is [100000, 5000000]. Agora recommends setVideoEncoderConfiguration referring to to set the value."
            },
            {
                "expectedDownlinkBitrate": "The expected maximum downlink bitrate (bps) of the local user. The value range is [100000,5000000]."
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "enum_audioroute",
        "name": "AudioRoute",
        "description": "The type of the audio route.\n",
        "parameters": [
            {
                "routeDefault": "-1: Default audio route."
            },
            {
                "routeHeadset": "Audio output routing is a headset with microphone."
            },
            {
                "routeEarpiece": "1: The audio route is an earpiece."
            },
            {
                "routeHeadsetnomic": "2: The audio route is a headset without a microphone."
            },
            {
                "routeSpeakerphone": "3: The audio route is the speaker that comes with the device."
            },
            {
                "routeHeadsetbluetooth": "5: The audio route is a bluetooth headset."
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "api_setextensionproviderproperty",
        "name": "setExtensionProviderProperty",
        "description": "Sets the properties of the extension provider.\nYou can call this method to set the attributes of the extension provider and initialize the relevant parameters according to the type of the provider. Call this method after enableExtension , and before enabling the audio ( enableAudio / enableLocalAudio ) or the video ( enableVideo / enableLocalVideo ).",
        "parameters": [
            {
                "value": "The value of the extension key."
            },
            {
                "key": "The key of the extension."
            },
            {
                "provider": "The name of the extension provider."
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "api_setuplocalvideo",
        "name": "setupLocalVideo",
        "description": "Initializes the local video view.\nThis method initializes the video view of a local stream on the local device. It affects only the video view that the local user sees, not the published local video stream. Call this method to bind the local video stream to a video view and to set the rendering and mirror modes of the video view.\n After initialization, call this method to set the local video and then join the channel. The local video still binds to the view after you leave the channel. To unbind the local video from the view, set the view parameter as .NULL You can call this method either before or after joining a channel.\n To update the rendering or mirror mode of the local video view during a call, use the setLocalRenderMode [2/2] method.",
        "parameters": [
            {
                "canvas": "Local video display properties. See VideoCanvas ."
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "enum_audiencelatencyleveltype",
        "name": "AudienceLatencyLevelType",
        "description": "The latency level of an audience member in interactive live streaming. This enum takes effect only when the user role is set to clientRoleAudience .\n",
        "parameters": [
            {
                "audienceLatencyLevelLowLatency": "1: Low latency."
            },
            {
                "audienceLatencyLevelUltraLowLatency": "2: (Default) Ultra low latency."
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "enum_remotevideostatereason",
        "name": "RemoteVideoStateReason",
        "description": "The reason for the remote audio state change.\n",
        "parameters": [
            {
                "remoteVideoStateReasonInternal": "0: The SDK reports this reason when the audio state changes."
            },
            {
                "remoteVideoStateReasonNetworkCongestion": "1: Network congestion."
            },
            {
                "remoteVideoStateReasonNetworkRecovery": "2: Network recovery."
            },
            {
                "remoteVideoStateReasonLocalMuted": "3: The local user stops receiving the remote video stream or disables the video module."
            },
            {
                "remoteVideoStateReasonLocalUnmuted": "4: The local user resumes receiving the remote video stream or enables the video module."
            },
            {
                "remoteVideoStateReasonRemoteMuted": "5: The remote user stops sending the video stream or disables the video module."
            },
            {
                "remoteVideoStateReasonRemoteUnmuted": "6: The remote user resumes sending the video stream or enables the video module."
            },
            {
                "remoteVideoStateReasonRemoteOffline": "7: The remote user leaves the channel."
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "api_stopeffect",
        "name": "stopEffect",
        "description": "Stops playing a specified audio effect.\n",
        "parameters": [
            {
                "soundId": "The audio effect ID. The ID of each audio effect file is unique."
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "api_irtcengineex_pushvideoframeex",
        "name": "pushVideoFrame [2/2]",
        "description": "Pushes the external raw video frame to the SDK.\nIf you call setExternalVideoSource and set the enabled parameter as true and the encodedFrame parameter as false, you can call this method to push the external raw video frame to the SDK.",
        "parameters": [
            {
                "connection": "The connection information. See RtcConnection ."
            },
            {
                "frame": "The external raw video frame to be pushed. See ExternalVideoFrame .\n "
            }
        ],
        "returns": "",
        "is_hide": true
    },
    {
        "id": "api_setencryptionmode",
        "name": "setEncryptionMode",
        "description": "Sets the built-in encryption mode.\nDeprecated:\n Use enableEncryption instead. The Agora SDK supports built-in encryption, which is set to the AES-128-GCM mode by default. Call this method to use other encryption modes. All users in the same channel must use the same encryption mode and secret. Refer to the information related to the AES encryption algorithm on the differences between the encryption modes.\n Before calling this method, please call setEncryptionSecret to enable the built-in encryption function.",
        "parameters": [
            {
                "encryptionMode": "The following encryption modes:\n \"aes-128-xts\": 128-bit AES encryption, XTS mode.\n \"aes-128-ecb\": 128-bit AES encryption, ECB mode.\n \"aes-256-xts\": 256-bit AES encryption, XTS mode.\n \"sm4-128-ecb\": 128-bit SM4 encryption, ECB mode.\n \"aes-128-gcm\": 128-bit AES encryption, GCM mode.\n \"aes-256-gcm\": 256-bit AES encryption, GCM mode.\n \"\": When this parameter is set as null, the encryption mode is set as \"aes-128-gcm\" by default. "
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "api_irtcengineex_createdatastreamex",
        "name": "createDataStreamEx [1/2]",
        "description": "Creates a data stream.\nDeprecated:\n This method is deprecated. This method is deprecated. Use createDataStreamEx instead. You can call this method to create a data stream and improve the reliability and ordering of data transmission. Ensure that you set the same value for reliable and ordered.\n Each user can create up to five data streams during the lifecycle of RtcEngine .\n The data channel allows a data delay of up to 5 seconds. If the receiver does not receive the data stream within 5 seconds, the data channel reports an error.",
        "parameters": [
            {
                "connection": "The connection information. See RtcConnection ."
            },
            {
                "reliable": "Sets whether the recipients are guaranteed to receive the data stream from the sender within five seconds:\n true: The recipients receive the data stream from the sender within five seconds. If the recipient does not receive the data stream within five seconds, an error is reported to the application.\n false: There is no guarantee that the recipients receive the data stream from the sender within five seconds. The SDK does not report errors if reception is delayed or data is lost. "
            },
            {
                "ordered": "Sets whether the recipients receive the data stream in the sent order:\n true: The recipients receive the data stream in the sent order.\n false: There is no guarantee that the recipients receive the data stream in the sent order. "
            }
        ],
        "returns": "< 0: Fails to create the data stream.",
        "is_hide": true
    },
    {
        "id": "callback_onconnectionstatechanged",
        "name": "onConnectionStateChanged",
        "description": "Occurs when the network connection state changes.\nWhen the network connection state changes, the SDK triggers this callback and reports the current connection state and the reason for the change.",
        "parameters": [
            {
                "connection": "The connection information. See RtcConnection ."
            },
            {
                "state": "The current connection state.  ConnectionStateType \n "
            },
            {
                "reason": "The reason for a connection state change. See ConnectionChangedReasonType .\n "
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "api_imediaplayer_registereventhandler",
        "name": "registerEventHandler",
        "description": "Since\n v",
        "parameters": [
            {
                "null": ""
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "callback_ivideoencodedimagereceiver_onencodedvideoimagereceived",
        "name": "OnEncodedVideoImageReceived",
        "description": "Occurs each time the SDK receives an encoded video frame.\n",
        "parameters": [
            {},
            {
                "videoEncodedFrameInfo": "The information of the encoded video frame, see EncodedVideoFrameInfo ."
            }
        ],
        "returns": "Reserved for future use.",
        "is_hide": true
    },
    {
        "id": "class_localaudiostats",
        "name": "LocalAudioStats",
        "description": "Local audio statistics.\n",
        "parameters": [
            {
                "numChannels": "The number of audio channels."
            },
            {
                "sentSampleRate": "The sampling rate (Hz) of sending the local user's audio stream."
            },
            {
                "sentBitrate": "The average bitrate (Kbps) of sending the local user's audio stream."
            },
            {
                "txPacketLossRate": "The packet loss rate (%) from the local client to the Agora server before applying the anti-packet loss strategies."
            },
            {
                "internalCodec": "The internal payload codec."
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "class_audioencodedframeobserverconfig",
        "name": "AudioEncodedFrameObserverConfig",
        "description": "Observer settings for encoded audio.\n",
        "parameters": [
            {
                "postionType": "Audio profile. See AudioEncodedFrameObserverPosition .\n "
            },
            {
                "encodingType": "Audio encoding type. See AudioEncodingType .\n "
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "api_icloudspatialaudioengine_exitroom",
        "name": "exitRoom",
        "description": "Exits the room.\nAfter the user exits the room, the spatial audio effect disappears immediately.",
        "parameters": [],
        "returns": "",
        "is_hide": true
    },
    {
        "id": "enum_metadatatype",
        "name": "MetadataType",
        "description": "Metadata type of the observer. We only support video Metadata for now.\n",
        "parameters": [
            {
                "unknownMetadata": "The type of metadata is unknown."
            },
            {
                "videoMetadata": "The type of metadata is video."
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "api_irtcengineex_leavechannelex",
        "name": "leaveChannelEx",
        "description": "Leaves a channel.\n",
        "parameters": [
            {
                "connection": "The connection information. See RtcConnection ."
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "callback_onaudioquality",
        "name": "onAudioQuality",
        "description": "Reports the statistics of the audio stream from each remote user.\nDeprecated:\n Please use onRemoteAudioStats instead. \n The SDK triggers this callback once every two seconds to report the audio quality of each remote user/host sending an audio stream. If a channel has multiple users/hosts sending audio streams, the SDK triggers this callback as many times.",
        "parameters": [
            {
                "connection": "The connection information. See RtcConnection ."
            },
            {
                "remoteUid": "The user ID of the remote user sending the audio stream."
            },
            {
                "quality": "Audio quality of the user. See QualityType ."
            },
            {
                "delay": "The network delay (ms) from the sender to the receiver, including the delay caused by audio sampling pre-processing, network transmission, and network jitter buffering."
            },
            {
                "lost": "The packet loss rate (%) of the audio packet sent from the remote user."
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "api_icloudspatialaudioengine_initialize",
        "name": "initialize",
        "description": "Initializes ICloudSpatialAudioEngine .\nBefore calling other methods of the ICloudSpatialAudioEngine class, you need to call this method to initialize ICloudSpatialAudioEngine.\n The SDK supports creating only one ICloudSpatialAudioEngine instance for an app.",
        "parameters": [
            {
                "config": "The configuration of ICloudSpatialAudioEngine. See CloudSpatialAudioConfig for details."
            }
        ],
        "returns": "",
        "is_hide": true
    },
    {
        "id": "api_enablesoundpositionindication",
        "name": "enableSoundPositionIndication",
        "description": "Enables/Disables stereo panning for remote users.\nEnsure that you call this method before joining a channel to enable stereo panning for remote users so that the local user can track the position of a remote user by calling setRemoteVoicePosition.",
        "parameters": [
            {
                "enabled": "Whether to enable stereo panning for remote users:\n true: Enable stereo panning.\n false: Disable stereo panning.\n "
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "enum_audioscenariotype_ng",
        "name": "AudioScenarioType",
        "description": "The audio scenario.\n",
        "parameters": [
            {
                "audioScenarioDefault": "0: (Default) Automatic scenario, where the SDK chooses the appropriate audio quality according to the user role and audio route."
            },
            {
                "audioScenarioGameStreaming": "3: High-quality audio scenario, where users mainly play music."
            },
            {
                "audioScenarioHighDefinition": "6: High-quality audio scenario, where users mainly play music."
            },
            {
                "audioScenarioChorus": "7: Real-time chorus scenario, where users have good network conditions and require ultra-low latency.\n "
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "api_getaudiomixingduration",
        "name": "getAudioMixingDuration",
        "description": "Retrieves the duration (ms) of the music file.\nRetrieves the total duration (ms) of the audio. You need to call this method after calling startAudioMixing and receiving the onAudioMixingStateChanged (audioMixingStatePlaying) callback.",
        "parameters": [],
        "returns": "≥ 0: The audio mixing duration, if this method call succeeds.\n < 0: Failure.",
        "is_hide": false
    },
    {
        "id": "callback_ivideoframeobserver_oncapturevideoframe",
        "name": "onCaptureVideoFrame",
        "description": "Occurs each time the SDK receives a video frame captured by the local camera.\nAfter you successfully register the video frame observer, the SDK triggers this callback each time it receives a video frame. In this callback, you can get the video data captured by the local camera. You can then pre-process the data according to your scenarios.\n After pre-processing, you can send the processed video data back to the SDK by this callback. This callback does not support sending processed RGBA video data back to the SDK.\n The video data obtained through this callback has not undergone preprocessing, such as watermarking, cropping content, rotating, or image enhancement.",
        "parameters": [
            {
                "videoFrame": "The video frame. "
            }
        ],
        "returns": "true: Sets the SDK to receive the video frame.\n false: Sets the SDK to discard the video frame.",
        "is_hide": true
    },
    {
        "id": "enum_mediaplayerstate",
        "name": "MediaPlayerState",
        "description": "The playback state.\n",
        "parameters": [
            {
                "playerStateIdle": "0: The default state.\nThe media player returns this state code before you open the media resource or after you stop the playback."
            },
            {
                "playerStateOpening": "Opening the media resource."
            },
            {
                "playerStateOpenCompleted": "Opens the media resource successfully."
            },
            {
                "playerStatePlaying": "The media resource is playing."
            },
            {
                "playerStatePaused": "Pauses the playback."
            },
            {
                "playerStatePlaybackCompleted": "The playback finishes."
            },
            {
                "playerStatePlaybackAllLoopsCompleted": "The loop finishes."
            },
            {
                "playerStateStopped": "The playback stops."
            },
            {
                "playerStateFailed": "100: The media player fails to play the media resource."
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "api_setremoteuserspatialaudioparams",
        "name": "setRemoteUserSpatialAudioParams",
        "description": "Sets the spatial audio effect parameters of the remote user.\nCall this method after enableSpatialAudio . After successfully setting the spatial audio effect parameters of the remote user, the local user can hear the remote user with a sense of space.",
        "parameters": [
            {
                "uid": "The user ID. This parameter must be the same as the user ID passed in when the user joined the channel."
            },
            {
                "params": "The spatial audio parameters. See SpatialAudioParams for details."
            }
        ],
        "returns": "",
        "is_hide": true
    },
    {
        "id": "api_setplaybackaudioframebeforemixingparameters",
        "name": "setPlaybackAudioFrameBeforeMixingParameters",
        "description": "Sets the audio data format reported by onPlaybackAudioFrameBeforeMixing .\n",
        "parameters": [
            {
                "sampleRate": "The sample rate (Hz) of the audio data, which can be set as 8000, 16000, 32000, 44100, or 48000.\n "
            },
            {
                "channel": "The number of channels of the external audio source, which can be set as 1(Mono) or 2(Stereo).\n "
            }
        ],
        "returns": "0: Success.\n < 0: Failure.",
        "is_hide": true
    },
    {
        "id": "api_pauseallchannelmediarelay",
        "name": "pauseAllChannelMediaRelay",
        "description": "Pauses the media stream relay to all destination channels.\nAfter the cross-channel media stream relay starts, you can call this method to pause relaying media streams to all destination channels; after the pause, if you want to resume the relay, call resumeAllChannelMediaRelay .\n After a successful method call, the SDK triggers the onChannelMediaRelayEvent callback to report whether the media stream relay is successfully paused.\n Call this method after the startChannelMediaRelay method.",
        "parameters": [],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "api_imediaengine_pushvideoframe",
        "name": "pushVideoFrame [1/2]",
        "description": "Pushes the external raw video frame to the SDK.\nIf you call setExternalVideoSource and set the enabled parameter as true and the encodedFrame parameter as false, you can call this method to push the external raw video frame to the SDK.",
        "parameters": [
            {
                "frame": "The external raw video frame to be pushed. See ExternalVideoFrame .\n "
            }
        ],
        "returns": "",
        "is_hide": true
    },
    {
        "id": "api_setdefaultmuteallremoteaudiostreams",
        "name": "setDefaultMuteAllRemoteAudioStreams",
        "description": "Stops or resumes subscribing to the audio streams of all remote users by default.\nCall this method after joining a channel. After successfully calling this method, the local user stops or resumes subscribing to the audio streams of all subsequent users. If you need to resume subscribing to the audio streams of remote users in the channel after calling this method, do the following:\n To resume subscribing to the audio stream of a specified user, call muteRemoteAudioStream (false), and specify the user ID.\n To resume subscribing to the audio streams of multiple remote users, call muteRemoteAudioStream(false) multiple times.",
        "parameters": [
            {
                "mute": "Whether to stop subscribing to the audio streams of all remote users by default.\n true: Stop subscribing to the audio streams of all remote users by default.\n false: (Default) Subscribe to the audio streams of all remote users by default. "
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "api_pauseaudiomixing",
        "name": "pauseAudioMixing",
        "description": "Pauses playing the music file.\nCall this method after joining a channel.",
        "parameters": [],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "class_irtcengine",
        "name": "RtcEngine",
        "description": "The basic interface of the Agora SDK that implements the core functions of real-time communication.\nRtcEngine provides the main methods that your app can call.",
        "parameters": [],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "callback_ipacketobserver_onsendvideopacket",
        "name": "onSendVideoPacket",
        "description": "Occurs when the local user sends a video packet.\n",
        "parameters": [
            {
                "packet": "The sent video packet, see Packet ."
            }
        ],
        "returns": "true: The video packet is sent successfully.\n false: The video packet is discarded.",
        "is_hide": true
    },
    {
        "id": "api_stoppreview",
        "name": "stopPreview1",
        "description": "Stops the local video preview.\nAfter calling startPreview to start the preview, if you want to close the local video preview, please call this method.\n Please call this method before joining a channel or after leaving a channel.",
        "parameters": [],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "api_create2",
        "name": "initialize",
        "description": "Initializes RtcEngine.\nAll called methods provided by the RtcEngine class are executed asynchronously. Agora recommends calling these methods in the same thread. Before calling other APIs, you must call createAgoraRtcEngine and RtcEngine to create and initialize the initialize object.\n The SDK supports creating only one RtcEngine instance for an app.",
        "parameters": [
            {
                "context": "Configurations for the RtcEngine instance. See RtcEngineContext .\n "
            }
        ],
        "returns": "The RtcEngine instance, if the method call succeeds.\n An error code, if the call fails.",
        "is_hide": false
    },
    {
        "id": "callback_onextensionstopped",
        "name": "onExtensionStopped",
        "description": "Occurs when the extension is disabled.\nAfter a successful call of enableExtension (false), this callback is triggered.",
        "parameters": [
            {
                "extName": "The name of the extension."
            },
            {
                "provider": "The name of the extension provider."
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "api_iaudiodevicemanager_setplaybackdevice",
        "name": "setPlaybackDevice",
        "description": "Sets the audio playback device.\n",
        "parameters": [
            {
                "deviceId": "The ID of the specified audio playback device. You can get the device ID by calling enumeratePlaybackDevices . Plugging or unplugging the audio device does not change the value of deviceId.\n "
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "api_enabledualstreammode2",
        "name": "enableDualStreamMode [2/3]",
        "description": "Enables/Disables dual-stream mode.\nYou can call this method to enable or disable the dual-stream mode on the publisher side. Dual streams are a hybrid of a high-quality video stream and a low-quality video stream:\n High-quality video stream: High bitrate, high resolution.\n Low-quality video stream: Low bitrate, low resolution. After you enable the dual-stream mode, you can call setRemoteVideoStreamType to choose toreceive the high-quality video stream or low-quality video stream on the subscriber side. You can call this method either before or after joining a channel.",
        "parameters": [
            {
                "sourceType": "The capture type of the custom video source. See VideoSourceType .\n "
            },
            {
                "enabled": "Whether to enable dual-stream mode.\n true: Enable dual-stream mode.\n false: Disable dual-stream mode. "
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "callback_onaudiovolumeindication",
        "name": "onAudioVolumeIndication",
        "description": "Reports the volume information of users.\nBy default, this callback is disabled. You can enable it by calling enableAudioVolumeIndication . Once this callback is enabled and users send streams in the channel, the SDK triggers the onAudioVolumeIndication callback at the time interval set in enableAudioVolumeIndication. The SDK triggers two independent onAudioVolumeIndication callbacks simultaneously, which separately report the volume information of the local user who sends a stream and the remote users (up to three) whose instantaneous volume is the highest.\n After you enable this callback, calling muteLocalAudioStream affects the SDK's behavior as follows:\n If the local user stops publishing the audio stream, the SDK stops triggering the local user's callback.\n 20 seconds after a remote user whose volume is one of the three highest stops publishing the audio stream, the callback excludes this user's information; 20 seconds after all remote users stop publishing audio streams, the SDK stops triggering the callback for remote users.",
        "parameters": [
            {
                "connection": "The connection information. See RtcConnection ."
            },
            {
                "speakers": "The volume information of the users, see AudioVolumeInfo . Ifan empty speakers array in the callback indicates that no remote user is in the channel or sending a stream at the moment."
            },
            {
                "speakerNumber": "The total number of users.\n In the local user's callback, when the local user sends a stream, speakerNumber is 1.\n In the callback for remote users, the value range of speakerNumber is [0,3]. If the number of remote users who send streams is greater than or equal to three, the value of speakerNumber is 3. "
            },
            {
                "totalVolume": "The volume of the speaker. The value ranges between 0 (lowest volume) and 255 (highest volume).\n In the callback for the local user, totalVolume is the volume of the local user who sends a stream.\n In the callback for remote users, totalVolume is the sum of the volume of all remote users (up to three) whose instantaneous volume is the highest. "
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "class_injectstreamconfig",
        "name": "InjectStreamConfig",
        "description": "Configurations of injecting an external audio or video stream.\n",
        "parameters": [
            {
                "width": "The width of the external video stream after injecting. The default value is 0, which represents the same width as the original."
            },
            {
                "height": "The height of the external video stream after injecting. The default value is 0, which represents the same height as the original."
            },
            {
                "videoGop": "The GOP (in frames) of injecting the external video stream. The default value is 30."
            },
            {
                "videoFrameRate": "The frame rate (fps) of injecting the external video stream. The default rate is 15 fps."
            },
            {
                "videoBitrate": "The bitrate (Kbps) of injecting the external video stream. The default value is 400 Kbps.\n The bitrate setting is closely linked to the video resolution. If the bitrate you set is beyond a reasonable range, the SDK sets it within a reasonable range.\n "
            },
            {
                "audioSampleRate": "The sampling rate (Hz) of injecting the external audio stream. The default value is 48000 Hz. See AudioSampleRateType .\n Agora recommends using the default value. "
            },
            {
                "audioBitrate": "The bitrate (Kbps) of injecting the external audio stream. The default value is 48 Kbps.\n Agora recommends using the default value. "
            },
            {
                "audioChannels": "The number of channels of the external audio stream after injecting.\n 1: (Default) Mono.\n 2: Stereo. Agora recommends using the default value. "
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "class_irtcengineeventhandlerex",
        "name": "RtcEngineEventHandlerEx",
        "description": "inherited from.\n类，用于监听和报告指定频道的事件和数据。 RtcEngineEventHandler \n",
        "parameters": [],
        "returns": "",
        "is_hide": true
    },
    {
        "id": "enum_encryptionmode",
        "name": "EncryptionMode",
        "description": "The built-in encryption mode.\nAgora recommends using aes128Gcm2 or aes256Gcm2 encrypted mode. These two modes support the use of salt for higher security.",
        "parameters": [
            {
                "aes128Xts": "1: 128-bit AES encryption, XTS mode."
            },
            {
                "aes128Ecb": "2: 128-bit AES encryption, ECB mode."
            },
            {
                "aes256Xts": "3: 256-bit AES encryption, XTS mode."
            },
            {
                "sm4128Ecb": "4: 128-bit SM4 encryption, ECB mode."
            },
            {
                "aes128Gcm": "5: 128-bit AES encryption, GCM mode."
            },
            {
                "aes256Gcm": "6: 256-bit AES encryption, GCM mode."
            },
            {
                "aes128Gcm2": "7: (Default) 128-bit AES encryption, GCM mode. This encryption mode requires the setting of salt (encryptionKdfSalt)."
            },
            {
                "aes256Gcm2": "8: 256-bit AES encryption, GCM mode. This encryption mode requires the setting of salt (encryptionKdfSalt)."
            },
            {
                "modeEnd": "Enumeration boundary."
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "class_ipacketobserver",
        "name": "IPacketObserver",
        "description": "The definition of IPacketObserver.\n",
        "parameters": [],
        "returns": "",
        "is_hide": true
    },
    {
        "id": "callback_onuserinfoupdated",
        "name": "onUserInfoUpdated",
        "description": "Occurs when the SDK gets the user ID and user account of the remote user.\nAfter a remote user joins the channel, the SDK gets the UID and user account of the remote user, caches them in a mapping table object, and triggers this callback on the local client.",
        "parameters": [
            {
                "uid": "The user ID of the remote user."
            },
            {
                "info": "The UserInfo object that contains the user ID and user account of the remote user. See for details UserInfo ."
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "api_getloopbackrecordingvolume",
        "name": "getLoopbackRecordingVolume",
        "description": "",
        "parameters": [
            {
                "null": ""
            }
        ],
        "returns": "",
        "is_hide": true
    },
    {
        "id": "api_setlocalvoiceequalization",
        "name": "setLocalVoiceEqualization",
        "description": "Sets the local voice equalization effect.\nYou can call this method either before or after joining a channel.",
        "parameters": [
            {
                "bandFrequency": "The band frequency. The value ranges between 0 and 9; representing the respective 10-band center frequencies of the voice effects, including 31, 62, 125, 250, 500, 1k, 2k, 4k, 8k, and 16k Hz. "
            },
            {
                "bandGain": "The gain of each band in dB. The value ranges between -15 and 15. The default value is 0."
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "enum_voiceconversionpreset",
        "name": "VoiceConversionPreset",
        "description": "The options for SDK preset voice conversion effects.\n",
        "parameters": [
            {
                "voiceConversionOff": "Turn off voice conversion effects and use the original voice."
            },
            {
                "voiceChangerNeutral": "A gender-neutral voice. To avoid audio distortion, ensure that you use this enumerator to process a female-sounding voice."
            },
            {
                "voiceChangerSweet": "A sweet voice. To avoid audio distortion, ensure that you use this enumerator to process a female-sounding voice."
            },
            {
                "voiceChangerSolid": "A steady voice. To avoid audio distortion, ensure that you use this enumerator to process a male-sounding voice."
            },
            {
                "voiceChangerBass": "A deep voice. To avoid audio distortion, ensure that you use this enumerator to process a male-sounding voice."
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "api_getuserinfobyuid",
        "name": "getUserInfoByUid",
        "description": "Gets the user information by passing in the user ID.\nAfter a remote user joins the channel, the SDK gets the UID and user account of the remote user, caches them in a mapping table object, and triggers the onUserInfoUpdated callback on the local client. After receiving the callback, you can call this method to get the user account of the remote user from the UserInfo object by passing in the user ID.",
        "parameters": [
            {
                "uid": ""
            }
        ],
        "returns": "The UserInfo object that identifies the user information. Not null: Success.\n Null: Failure.",
        "is_hide": false
    },
    {
        "id": "api_adjustrecordingsignalvolume",
        "name": "adjustRecordingSignalVolume",
        "description": "Adjusts the capturing signal volume.\nYou can call this method either before or after joining a channel.",
        "parameters": [
            {
                "volume": "The volume of the user. The value range is [0,400].\n 0: Mute.\n 100: (Default) The original volume.\n 400: Four times the original volume (amplifying the audio signals by four times). "
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "api_imediaplayer_registeraudioframeobserver2",
        "name": "registerAudioFrameObserver [2/2]",
        "description": "Registers an audio frame observer object.\n",
        "parameters": [
            {},
            {
                "mode": ""
            }
        ],
        "returns": "",
        "is_hide": true
    },
    {
        "id": "enum_videostreamtype",
        "name": "VideoStreamType",
        "description": "The type of video streams.\n",
        "parameters": [
            {
                "videoStreamHigh": "0: High-quality video stream."
            },
            {
                "videoStreamLow": "1: Low-quality video stream."
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "api_switchcamera",
        "name": "switchCamera",
        "description": "Switches between front and rear cameras.\nThis method needs to be called after the camera is started (for example, by calling startPreview or joinChannelWithOptions ). This method is for Android and iOS only.",
        "parameters": [],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "class_channelmediarelayconfiguration",
        "name": "ChannelMediaRelayConfiguration",
        "description": "The definition of ChannelMediaRelayConfiguration.\n",
        "parameters": [
            {
                "srcInfo": "The information of the source channel ChannelMediaInfo . It contains the following members:\n channelName: The name of the source channel. The default value is NULL, which means the SDK applies the name of the current channel.\n uid: The unique ID to identify the relay stream in the source channel. The default value is 0, which means the SDK generates a random uid. You must set it as 0.\n token: The token for joining the source channel. It is generated with the channelName and uid you set in srcInfo.\n If you have not enabled the App Certificate, set this parameter as the default value NULL, which means the SDK applies the App ID.\n If you have enabled the App Certificate, you must use the token generated with the channelName and uid, and the uid must be set as 0. "
            },
            {
                "destInfos": "The information of the destination channel ChannelMediaInfo. It contains the following members:\n channelName : The name of the destination channel.\n uid: The unique ID to identify the relay stream in the destination channel. The value ranges from 0 to (232-1). To avoid UID conflicts, this UID must be different from any other UID in the destination channel. The default value is 0, which means the SDK generates a random UID. Do not set this parameter as the UID of the host in the destination channel, and ensure that this UID is different from any other UID in the channel.\n token: The token for joining the destination channel. It is generated with the channelName and uid you set in destInfos.\n If you have not enabled the App Certificate, set this parameter as the default value NULL, which means the SDK applies the App ID.\n If you have enabled the App Certificate, you must use the token generated with the channelName and uid. "
            },
            {
                "destCount": "The number of destination channels. The default value is 0, and the value range is from 0 to 4. Ensure that the value of this parameter corresponds to the number of ChannelMediaInfo structs you define in destInfo."
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "api_setexternalaudiosink_ng",
        "name": "setExternalAudioSink",
        "description": "Sets the external audio sink.\nThis method applies to scenarios where you want to use external audio data for playback. After setting false in initialize , you can call setExternalAudioSink to set the external audio sink. After the setting is successful, you can call pullAudioFrame to pull the remote audio data. The app can process the remote audio and play it with the audio effects that you want. Ensure that you call this method before joining a channel.\n Once you enable the external audio sink, the app does not retrieve any audio data from the onPlaybackAudioFrame callback.",
        "parameters": [
            {
                "sampleRate": "The sample rate (Hz) of the external audio sink, which can be set as 16000, 32000, 44100, or 48000.\n "
            },
            {
                "channels": "The number of audio channels of the external audio sink:\n 1: Mono.\n 2: Stereo.\n "
            }
        ],
        "returns": "0: Success.\n < 0: Failure.",
        "is_hide": true
    },
    {
        "id": "callback_onfirstlocalvideoframepublished",
        "name": "onFirstLocalVideoFramePublished",
        "description": "Occurs when the first video frame is published.\nThe SDK triggers this callback under one of the following circumstances:\n The local client enables the video module and calls joinChannelWithOptions successfully.\n The local client calls muteLocalVideoStream (true) and muteLocalVideoStream(false) in sequence.\n The local client calls disableVideo and enableVideo in sequence.",
        "parameters": [
            {
                "connection": "The connection information. See RtcConnection ."
            },
            {
                "elapsed": "Time elapsed (ms) from the local user calling the joinChannelWithOptions method until this callback is triggered."
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "api_startscreencapture_ng",
        "name": "startScreenCapture",
        "description": "Starts screen sharing.\nAfter successfully calling this method, you can share the entire screen through MediaProjection, an Android native class.\n There are two ways to start screen sharing, you can choose one according to the actual needs:\n Call this method before joining a channel, and then call joinChannelWithOptions to join a channel and set publishScreenTrack true to start screen sharing.\n Call this method after joining a channel, and then call updateChannelMediaOptions and set publishScreenTrack true to start screen sharing. \n Before calling this method, you need to implement onActivityResult, an Android native callback, and obtain the value of the data parameter in this callback.\n When sharing the screen on Android 10 or later, to avoid the Android system from triggering SecurityException, you need to call startForeground (the Android native method) before calling MediaProjection to notify the user that the current device starts screen sharing.",
        "parameters": [
            {
                "mediaProjectionPermissionResultData": ""
            }
        ],
        "returns": "0: Success.\n < 0: Failure.\n -2: mediaProjectionPermissionResultData is null.",
        "is_hide": false
    },
    {
        "id": "api_setlogfilter",
        "name": "setLogFilter",
        "description": "Sets the log output level of the SDK.\nDeprecated:\n Use logConfig in initialize instead. This method sets the output log level of the SDK. You can use one or a combination of the log filter levels. The log level follows the sequence of logFilterOff, logFilterCritical, logFilterError, logFilterWarn, logFilterInfo, and logFilterDebug. Choose a level to see the logs preceding that level.\n If, for example, you set the log level to logFilterWarn, you see the logs within levels logFilterCritical, logFilterError, and logFilterWarn.",
        "parameters": [
            {
                "filter": "The output log level of the SDK. See LogFilterType .\n "
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "api_destroymediaplayer",
        "name": "destroyMediaPlayer",
        "description": "Destroys the media player instance.\n",
        "parameters": [],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "api_imediaengine_registervideoframeobserver",
        "name": "registerVideoFrameObserver",
        "description": "Registers a video frame observer object.\nYou need to implement the IVideoFrameObserver class in this method and register callbacks according to your scenarios. After registering the video observer, the SDK\ntriggers the above-mentioned callbacks every time a video frame is captured. When handling the video data returned in the callbacks, pay attention to the changes in the width and height parameters, which may be adapted under the following circumstances:\n When the network condition deteriorates, the video resolution decreases incrementally.\n If the user adjusts the video profile, the resolution of the video returned in the callbacks also changes. Ensure that you call this method before joining a channel.",
        "parameters": [
            {}
        ],
        "returns": "",
        "is_hide": true
    },
    {
        "id": "api_imediaplayer_getmute",
        "name": "getMute",
        "description": "Reports whether the media resource is muted.\n",
        "parameters": [],
        "returns": "true: Reports whether the media resource is muted.\n false: Reports whether the media resource is muted.",
        "is_hide": false
    },
    {
        "id": "callback_onaudioroutingchanged",
        "name": "onAudioRoutingChanged",
        "description": "Occurs when the local audio route changes.\nThis method is for Android, iOS and macOS only.",
        "parameters": [
            {
                "routing": "The current audio routing. See AudioRoute .\n "
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "enum_framerate",
        "name": "FrameRate",
        "description": "Video frame rate.\n",
        "parameters": [
            {
                "frameRateFps1": "1:1 fps"
            },
            {
                "frameRateFps7": "7:7fps"
            },
            {
                "frameRateFps10": "10: 10fps"
            },
            {
                "frameRateFps15": "15: 15fps"
            },
            {
                "frameRateFps24": "24: 24fps"
            },
            {
                "frameRateFps30": "30: 30fps"
            },
            {
                "frameRateFps60": "60: 60fps\n For Windows and macOS only."
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "api_stopsecondaryscreencapture",
        "name": "stopSecondaryScreenCapture",
        "description": "Stop sharing the secondary screen.\nAfter calling startSecondaryScreenCapture , you can call this method to stop sharing the secondary screen.",
        "parameters": [],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "callback_onnetworktypechanged",
        "name": "onNetworkTypeChanged",
        "description": "Occurs when the local network type changes.\nThis callback occurs when the connection state of the local user changes. You can get the connection state and reason for the state change in this callback. When the network connection is interrupted, this callback indicates whether the interruption is caused by a network type change or poor network conditions.",
        "parameters": [
            {
                "connection": "The connection information. See RtcConnection ."
            },
            {
                "type": "Network types: See NetworkType .\n "
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "enum_orientationmode",
        "name": "OrientationMode",
        "description": "Video output orientation mode.\n",
        "parameters": [
            {
                "orientationModeAdaptive": "0: (Default) The output video always follows the orientation of the captured video.\nThe receiver takes the rotational information passed on from the video encoder. This mode applies to scenarios where video orientation can be adjusted on the receiver. If the captured video is in landscape mode, the output video is in landscape mode.\n If the captured video is in portrait mode, the output video is in portrait mode.\n "
            },
            {
                "orientationModeFixedLandscape": "1: In this mode, the SDK always outputs videos in landscape (horizontal) mode. If the captured video is in portrait mode, the video encoder crops it to fit the output. Applies to situations where the receiving end cannot process the rotational information. For example, CDN live streaming."
            },
            {
                "orientationModeFixedPortrait": "2: In this mode, the SDK always outputs video in portrait (portrait) mode. If the captured video is in landscape mode, the video encoder crops it to fit the output.\nApplies to situations where the receiving end cannot process the rotational information. For example, CDN live streaming."
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "enum_directcdnstreamingstate",
        "name": "DIRECT_CDN_STREAMING_STATE",
        "description": "The current CDN streaming state.\n",
        "parameters": [
            {
                "DIRECT_CDN_STREAMING_STATE_IDLE": "0: The initial state before the CDN streaming starts."
            },
            {
                "DIRECT_CDN_STREAMING_STATE_RUNNING": "1: Streams are being pushed to the CDN. \nAfter you call startDirectCdnStreaming to stop streaming, the SDK returns this value."
            },
            {
                "DIRECT_CDN_STREAMING_STATE_STOPPED": "100: The streaming has been stopped normally. 2: Stops pushing streams to the CDN. After you call stopDirectCdnStreaming to stop streaming, the SDK returns this value."
            },
            {
                "DIRECT_CDN_STREAMING_STATE_FAILED": "3: Fails to push streams to the CDN.\nYou can troubleshoot the issue with the information reported by the onDirectCdnStreamingStateChanged callback, and then push streams to the CDN again."
            },
            {
                "DIRECT_CDN_STREAMING_STATE_RECOVERING": "4: Tries to reconnect Agora server to the CDN. The SDK attempts to reconnect a maximum of 10 times, if the connection is not restored, the streaming state becomes DIRECT_CDN_STREAMING_STATE_FAILED.\n"
            }
        ],
        "returns": "",
        "is_hide": true
    },
    {
        "id": "enum_mediaplayermetadatatype",
        "name": "MediaPlayerMetadataType",
        "description": "The type of media metadata.\n",
        "parameters": [
            {
                "playerMetadataTypeUnknown": "0: The type is unknown."
            },
            {
                "playerMetadataTypeSei": "1: The type is SEI."
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "enum_degradationpreference",
        "name": "DegradationPreference",
        "description": "Video degradation preferences when the bandwidth is a constraint.\n",
        "parameters": [
            {
                "maintainQuality": "0: (Default) Prefers to reduce the video frame rate while maintaining video quality during video encoding under limited bandwidth. This degradation preference is suitable for scenarios where video quality is prioritized.\n In the COMMUNICATION channel profile, the resolution of the video sent may change, so remote users need to handle this issue. See onVideoSizeChanged ."
            },
            {
                "maintainFramerate": "1: Prefers to reduce the video quality while maintaining the video frame rate during video encoding under limited bandwidth. This degradation preference is suitable for scenarios where smoothness is prioritized and video quality is allowed to be reduced."
            },
            {
                "maintainBalanced": ""
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "class_iaudiodevicemanager",
        "name": "AudioDeviceManager",
        "description": "Audio device management methods.\n",
        "parameters": [],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "class_snapshotconfig",
        "name": "",
        "description": "视频截图设置。\n",
        "parameters": [
            {
                "channel": "The channel name."
            },
            {
                "uid": "The user ID. Set uid as 0 if you want to take a snapshot of the local user's video."
            },
            {
                "filePath": "截图的本地保存路径，需精确到文件名及格式， 例如： \n Windows: C:\\Users\\<user_name>\\AppData\\Local\\Agora\\<process_name>\\example.jpg\n iOS: /App Sandbox/Library/Caches/example.jpg\n macOS: ～/Library/Logs/example.jpg\n Android: /storage/emulated/0/Android/data/<package name>/files/example.jpg\n Ensure that the path you specify exists and is writable.\n "
            }
        ],
        "returns": "",
        "is_hide": true
    },
    {
        "id": "api_isspeakerphoneenabled",
        "name": "isSpeakerphoneEnabled",
        "description": "Checks whether the speakerphone is enabled.\nThis method is for Android and iOS only.\n You can call this method either before or after joining a channel.",
        "parameters": [],
        "returns": "true: The speakerphone is enabled, and the audio plays from the speakerphone.\n false: The speakerphone is not enabled, and the audio plays from devices other than the speakerphone. For example, the headset or earpiece.",
        "is_hide": false
    },
    {
        "id": "enum_maxuseraccountlengthtype",
        "name": "MaxUserAccountLengthType",
        "description": "The maximum length of the user account.\n",
        "parameters": [
            {
                "maxUserAccountLength": "The maximum length of the user account is 256 bytes."
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "api_ibasespatialaudioengine_mutelocalaudiostream",
        "name": "muteLocalAudioStream",
        "description": "Stops or resumes publishing the local audio stream.\nThis method does not affect any ongoing audio recording, because it does not disable the audio capture device.\n Call this method after joinChannelWithOptions .\n When using the spatial audio effect, if you need to set whether to publish the local audio stream, Agora recommends calling this method instead of the muteLocalAudioStream method under RtcEngine .",
        "parameters": [
            {
                "mute": "Whether to stop publishing the local audio stream.\n true: Stop publishing the local audio stream.\n false: Publish the local audio stream.\n "
            }
        ],
        "returns": "",
        "is_hide": true
    },
    {
        "id": "enum_audioframetype",
        "name": "AudioFrameType",
        "description": "Audio frame type.\n",
        "parameters": [
            {
                "frameTypePcm16": "0: PCM 16"
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "api_stopaudiorecording",
        "name": "stopAudioRecording",
        "description": "Stops the audio recording on the client.\n",
        "parameters": [],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "callback_imediaplayersourceobserver_onplaybufferupdated",
        "name": "onPlayBufferUpdated",
        "description": "Reports the playback duration that the buffered data can support.\nWhen playing online media resources, the SDK triggers this callback every two seconds to report the playback duration that the currently buffered data can support.\n When the playback duration supported by the buffered data is less than the threshold (0 by default), the SDK returns playerEventBufferLow.\n When the playback duration supported by the buffered data is greater than the threshold (0 by default), the SDK returns playerEventBufferRecover.",
        "parameters": [
            {
                "playCachedBuffer": "The playback duration (ms) that the buffered data can support."
            }
        ],
        "returns": "",
        "is_hide": true
    },
    {
        "id": "class_simulcaststreamconfig",
        "name": "SimulcastStreamConfig",
        "description": "The configuration of the low-quality video stream.\n",
        "parameters": [
            {
                "dimensions": "The video dimension.  The default value is 160 × 120."
            },
            {
                "bitrate": "Video receive bitrate (Kbps). The default value is 65."
            },
            {
                "framerate": "The capture frame rate (fps) of the local video. The default value is 5."
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "api_iaudiodevicemanager_getrecordingdeviceinfo",
        "name": "getRecordingDeviceInfo",
        "description": " Retrieves the volume of the audio recording device. \n",
        "parameters": [],
        "returns": "A AudioDeviceInfo object, which includes the device ID and device name.",
        "is_hide": false
    },
    {
        "id": "api_ibasespatialaudioengine_setmaxaudiorecvcount",
        "name": "setMaxAudioRecvCount",
        "description": "Sets the maximum number of streams that a user can receive in a specified audio reception range.\nIf the number of receivable streams exceeds the set value, the local user receives the maxCount streams that are closest to the local user. If there are users who belong to the same team as the local user in the room, the local user receives the audio of the teammates first. For example, when maxCount is set to 3, if there are five remote users in the room, two of whom belong to the same team as the local user, and three of whom belong to different teams but are within the audio reception range of the local user, the local user can hear the two teammates and the one user from a different team closest to the local user.\n You can call this method either before or after enterRoom , with the following differences:\n If you call this method before enterRoom, this method takes effect when entering the room.\n If you call this method after enterRoom, this method takes effect immediately and changes the current maximum number of received streams of the local user.",
        "parameters": [
            {
                "maxCount": "The maximum number of streams that a user can receive within a specified audio reception range."
            }
        ],
        "returns": "",
        "is_hide": true
    },
    {
        "id": "callback_onaudiomixingfinished",
        "name": "onAudioMixingFinished",
        "description": "Occurs when the playback of the local music file finishes.\nDeprecated:\n Please use onAudioMixingStateChanged instead. After you call startAudioMixing to play a local music file, this callback occurs when the playback finishes. If the call startAudioMixing fails, the error code WARN_AUDIO_MIXING_OPEN_ERROR is returned.",
        "parameters": [],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "enum_voicebeautifierpreset",
        "name": "VoiceBeautifierPreset",
        "description": "The options for SDK preset voice beautifier effects.\n",
        "parameters": [
            {
                "voiceBeautifierOff": "Turn off voice beautifier effects and use the original voice."
            },
            {
                "chatBeautifierMagnetic": "A more magnetic voice.\n Agora recommends using this enumerator to process a male-sounding voice; otherwise, you may experience vocal distortion."
            },
            {
                "chatBeautifierFresh": "A fresher voice.\n Agora recommends using this enumerator to process a female-sounding voice; otherwise, you may experience vocal distortion. "
            },
            {
                "chatBeautifierVitality": "A more vital voice.\n Agora recommends using this enumerator to process a female-sounding voice; otherwise, you may experience vocal distortion. "
            },
            {
                "singingBeautifier": "Singing beautifier effect. If you call setVoiceBeautifierPreset (singingBeautifier), you can beautify a male-sounding voice and add a reverberation effect that sounds like singing in a small room. Agora recommends using this enumerator to process a male-sounding voice; otherwise, you might experience vocal distortion.\n If you call setVoiceBeautifierParameters (singingBeautifier, param1, param2), you can beautify a male- or female-sounding voice and add a reverberation effect.\n "
            },
            {
                "timbreTransformationVigorous": "A more vigorous voice."
            },
            {
                "timbreTransformationDeep": "A deep voice."
            },
            {
                "timbreTransformationMellow": "A mellower voice."
            },
            {
                "timbreTransformationFalsetto": "Falsetto."
            },
            {
                "timbreTransformationFull": "A fuller voice."
            },
            {
                "timbreTransformationClear": "A clearer voice."
            },
            {
                "timbreTransformationResounding": "A more resounding voice."
            },
            {
                "timbreTransformationRinging": "A more ringing voice."
            },
            {
                "ultraHighQualityVoice": "A ultra-high quality voice, which makes the audio clearer and restores more details.\n To achieve better audio effect quality, Agora recommends that you set the profile of setAudioProfile2 audioProfileMusicHighQuality (4) or audioProfileMusicHighQualityStereo (5) and scenario to audioScenarioHighDefinition(6) before calling setVoiceBeautifierPreset .\n If you have an audio capturing device that can already restore audio details to a high degree, Agora recommends that you do not enable ultra-high quality; otherwise, the SDK may over-restore audio details, and you may not hear the anticipated voice effect.\n "
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "class_rtcconnection",
        "name": "RtcConnection",
        "description": "Contains connection information.\n",
        "parameters": [
            {
                "channelId": "The channel name."
            },
            {
                "localUid": "The ID of the local user."
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "enum_audiomixingerrortype",
        "name": "AudioMixingErrorType",
        "description": "Errors that may occur when playing a music file.\n",
        "parameters": [
            {
                "audioMixingErrorCanNotOpen": "The SDK cannot open the music file."
            },
            {
                "audioMixingErrorTooFrequentCall": "The SDK opens the music file too frequently."
            },
            {
                "audioMixingErrorInterruptedEof": "The playback of the music file is interrupted."
            },
            {
                "audioMixingErrorOk": "710: The music file is playing."
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "api_setcameraexposureposition",
        "name": "setCameraExposurePosition",
        "description": "Set the camera exposure position.\nThis method needs to be called after the camera is started (for example, by calling startPreview or joinChannelWithOptions ).\n After a successful method call, the SDK triggers the onCameraExposureAreaChanged callback.\n This method is for Android and iOS only.",
        "parameters": [
            {
                "positionXinView": "The horizontal coordinate of the touchpoint in the view."
            },
            {
                "positionYinView": "The vertical coordinate of the touchpoint in the view."
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "class_videodeviceinfo",
        "name": "VideoDeviceInfo",
        "description": "The VideoDeviceInfo class that contains the ID and device name of the video devices.\n",
        "parameters": [
            {
                "deviceId": "The device ID."
            },
            {
                "deviceName": "The device name."
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "enum_streamfallbackoptions",
        "name": "StreamFallbackOptions",
        "description": "Stream fallback options.\n",
        "parameters": [
            {
                "streamFallbackOptionDisabled": "0: No fallback behavior for the local/remote video stream when the uplink/downlink network conditions are poor. The quality of the stream is not guaranteed.\n"
            },
            {
                "streamFallbackOptionVideoStreamLow": "1: Under poor downlink network conditions, the remote video stream, to which you subscribe, falls back to the low-quality (low resolution and low bitrate) video stream.\nThis option is only valid for setRemoteSubscribeFallbackOption . "
            },
            {
                "streamFallbackOptionAudioOnly": "2: Under poor uplink network conditions, the published video stream falls back to audio-only. Under poor downlink network conditions, the remote video stream, to which you subscribe, first falls back to the low-quality (low resolution and low bitrate) video stream; and then to an audio-only stream if the network conditions worsen."
            }
        ],
        "returns": "",
        "is_hide": true
    },
    {
        "id": "api_setlocalvoicereverb",
        "name": "setLocalVoiceReverb",
        "description": "Sets the local voice reverberation.\nThe SDK provides an easier-to-use method, setAudioEffectPreset , to directly implement preset reverb effects for such as pop, R&B, and KTV.\n You can call this method either before or after joining a channel.",
        "parameters": [
            {
                "reverbKey": "The reverberation key. Agora provides five reverberation keys, see AudioReverbType ."
            },
            {
                "value": "The value of the reverberation key."
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "enum_audiorangemodetype",
        "name": "AUDIO_RANGE_MODE_TYPE",
        "description": "The audio range mode.\n",
        "parameters": [
            {
                "AUDIO_RANGE_MODE_WORLD": "0: Everyone mode. In this mode, whether a user can hear other users in the room depends on their settings for audio reception range, audio range mode, and team ID.\n If both users A and B set the AUDIO_RANGE_MODE_WORLD mode, users A and B can hear each other when they are in the audio reception range of each other or belong to the same team.\n If users A and B set the AUDIO_RANGE_MODE_WORLD and AUDIO_RANGE_MODE_TEAM mode respectively, they can only hear each other when they belong to the same team. "
            },
            {
                "AUDIO_RANGE_MODE_TEAM": "1: Team mode. In this mode, the user can only hear other users of the same team in the room."
            }
        ],
        "returns": "",
        "is_hide": true
    },
    {
        "id": "api_muteallremoteaudiostreams",
        "name": "muteAllRemoteAudioStreams",
        "description": "Stops or resumes subscribing to the audio streams of all remote users.\nAfter successfully calling this method, the local user stops or resumes subscribing to the audio streams of all remote users, including all subsequent users. Call this method after joining a channel.",
        "parameters": [
            {
                "mute": "Whether to stop subscribing to the audio streams of all remote users:\n true: Stop subscribing to the audio streams of all remote users.\n false: (Default) Subscribe to the audio streams of all remote users by default. "
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "api_setinearmonitoringvolume",
        "name": "setInEarMonitoringVolume",
        "description": "Sets the volume of the in-ear monitor.\nThis method is for Android and iOS only.\n Users must use wired earphones to hear their own voices.\n You can call this method either before or after joining a channel.",
        "parameters": [
            {
                "volume": "Sets the volume of the in-ear monitor. The value ranges between 0 and 100. The default value is 100."
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "class_rtcimage",
        "name": "RtcImage",
        "description": "Image properties.\nThis class sets the properties of the watermark and background images in the live video.",
        "parameters": [
            {
                "url": "The HTTP/HTTPS URL address of the image in the live video. The maximum length of this parameter is 1024 bytes."
            },
            {
                "x": "The x coordinate (pixel) of the image on the video frame (taking the upper left corner of the video frame as the origin)."
            },
            {
                "y": "The y coordinate (pixel) of the image on the video frame (taking the upper left corner of the video frame as the origin)."
            },
            {
                "width": "The width (pixel) of the image on the video frame."
            },
            {
                "height": "The height (pixel) of the image on the video frame."
            },
            {
                "zOrder": "The layer index of the watermark or background image. When you use the watermark array to add a watermark or multiple watermarks, you must pass a value to zOrder in the range [1,255]; otherwise, the SDK reports an error. In other cases, zOrder can optionally be passed in the range [0,255], with 0 being the default value. 0 means the bottom layer and 255 means the top layer."
            },
            {
                "alpha": "The transparency of the watermark or background image. The value ranges between 0.0 and 1.0:\n 0.0: Completely transparent.\n 1.0: (Default) Opaque.\n "
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "callback_icloudspatialaudioeventhandler_onteammatejoined",
        "name": "onTeammateJoined",
        "description": "Occurs when the user joins the current team.\nWhen a remote user with the same team ID calls enterRoom to enter the current room, the local user receives this callback.",
        "parameters": [
            {
                "uid": "The user ID of the remote user who joins the current team."
            }
        ],
        "returns": "",
        "is_hide": true
    },
    {
        "id": "callback_onrtmpstreamingevent",
        "name": "onRtmpStreamingEvent",
        "description": "Reports events during the media push.\n",
        "parameters": [
            {
                "url": "The URL for media push."
            },
            {
                "eventCode": "The event code of media push. See RtmpStreamingEvent ."
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "api_getaudiodeviceinfo",
        "name": "getAudioDeviceInfo",
        "description": "Gets the audio device information.\nAfter calling this method, you can get whether the audio device supports ultra-low-latency capture and playback. This method is for Android only.\n You can call this method either before or after joining a channel.",
        "parameters": [],
        "returns": "The DeviceInfo object that identifies the audio device information.\n Not null: Success.\n Null: Failure.",
        "is_hide": false
    },
    {
        "id": "api_addinjectstreamurl",
        "name": "addInjectStreamUrl",
        "description": "Injects an online media stream to a live streaming channel.\nThis method takes effect only when you are a host in a live streaming channel.\n Only one online media stream can be injected into the same channel at the same time.\n Call this method after joining a channel.",
        "parameters": [
            {
                "url": "The URL address to be added to the ongoing streaming. Valid protocols are RTMP, HLS, and HTTP-FLV. Supported audio codec type: AAC.\n Supported video codec type: H264 (AVC). "
            },
            {
                "config": "The configuration information for the added video stream. See InjectStreamConfig ."
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "enum_mediadevicestatetype",
        "name": "MediaDeviceStateType",
        "description": "Media device states.\n",
        "parameters": [
            {
                "mediaDeviceStateIdle": "0: The device is ready for use."
            },
            {
                "mediaDeviceStateActive": "1: The device is in use."
            },
            {
                "mediaDeviceStateDisabled": "2: The device is disabled."
            },
            {
                "mediaDeviceStateNotPresent": "4: The device is not found."
            },
            {
                "mediaDeviceStateUnplugged": "8: The device is not connected."
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "enum_qualitytype",
        "name": "QualityType",
        "description": "Network quality types.\n",
        "parameters": [
            {
                "qualityUnknown": "0: The network quality is unknown."
            },
            {
                "qualityExcellent": "1: The network quality is excellent."
            },
            {
                "qualityGood": "2: The network quality is quite good, but the bitrate may be slightly lower than excellent."
            },
            {
                "qualityPoor": "3: Users can feel the communication is slightly impaired."
            },
            {
                "qualityBad": "4: Users cannot communicate smoothly."
            },
            {
                "qualityVbad": "5: The quality is so bad that users can barely communicate."
            },
            {
                "qualityDown": "6: The network is down and users cannot communicate at all."
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "api_irtcengineex_pushencodedvideoimageex",
        "name": "pushEncodedVideoImage [2/2]",
        "description": "Pushes the external encoded video frame to the SDK.\nAfter calling setExternalVideoSource to enable external video source and set the sourceType parameter to encodedVideoFrame, you can call this method to push the encoded external video frame to the SDK.",
        "parameters": [
            {
                "connection": ""
            }
        ],
        "returns": "0: Pushes the external encoded video frame to the SDK successfully.\n < 0: Fails to push the external encoded video frame to the SDK.",
        "is_hide": true
    },
    {
        "id": "api_icloudspatialaudioengine_renewtoken",
        "name": "renewToken",
        "description": "Renews the RTM token.\nAn RTM token is valid for 24 hours. When the SDK triggers the onTokenWillExpire callback, the application should get a new RTM token and then call this method to pass in the new token; otherwise, the SDK cannot connect to the Agora Spatial Audio Server.",
        "parameters": [
            {
                "token": "The RTM token for authentication. You can generate the RTM token in the following ways:\n Use to generate a temporary token.\n Deploy your own server for generating tokens. \n The uid or userAccount for generating the RTM token is the combination of the roomName and uid set in enterRoom . For example, if roomName is test and uid is 123, the uid or userAccount filled in when generating the RTM token is test123."
            }
        ],
        "returns": "",
        "is_hide": true
    },
    {
        "id": "callback_onintrarequestreceived",
        "name": "onIntraRequestReceived",
        "description": "",
        "parameters": [
            {
                "null": ""
            }
        ],
        "returns": "",
        "is_hide": true
    },
    {
        "id": "enum_rtmpstreamlifecycletype",
        "name": "RtmpStreamLifeCycleType",
        "description": "Lifecycle of the CDN live video stream.\nDeprecated",
        "parameters": [
            {
                "rtmpStreamLifeCycleBind2channel": "Bind to the channel lifecycle. If all hosts leave the channel, the CDN live streaming stops after 30 seconds.\n"
            },
            {
                "rtmpStreamLifeCycleBind2owner": "Bind to the owner of the RTMP stream. If the owner leaves the channel, the CDN live streaming stops immediately."
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "enum_clientroletype",
        "name": "ClientRoleType",
        "description": "The user role in the interactive live streaming.\n",
        "parameters": [
            {
                "clientRoleBroadcaster": "1: Host. A host can both send and receive streams."
            },
            {
                "clientRoleAudience": "2: (Default) Audience. An audience member can only receive streams."
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "callback_onvideopublishstatechanged",
        "name": "onVideoPublishStateChanged",
        "description": "Occurs when the video publishing state changes.\n",
        "parameters": [
            {
                "channel": "The channel name."
            },
            {
                "oldState": "The previous subscribing status. See StreamPublishState ."
            },
            {
                "newState": "The current subscribing status. See StreamPublishState."
            },
            {
                "elapseSinceLastState": "The time elapsed (ms) from the previous state to the current state."
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "enum_backgroundsourcetype",
        "name": "backgroundSourceType",
        "description": "The type of the custom background image.\n",
        "parameters": [
            {
                "backgroundColor": "1: (Default) The background image is a solid color."
            },
            {
                "backgroundImg": "The background image is a file in PNG or JPG format."
            },
            {
                "backgroundBlur": "The background image is the blurred background."
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "class_icloudspatialaudioengine",
        "name": "ICloudSpatialAudioEngine",
        "description": "This class calculates user positions through the Agora Spatial Audio Server to implement the spatial audio effect.\nThis class inherits from IBaseSpatialAudioEngine . Before calling other APIs in this class, you need to call the initialize method to initialize this class.",
        "parameters": [],
        "returns": "",
        "is_hide": true
    },
    {
        "id": "callback_imediaplayersourceobserver_onplayerinfoupdated",
        "name": "onPlayerInfoUpdated",
        "description": "Occurs when information related to the media player changes.\nWhen the information about the media player changes, the SDK triggers this callback. You can use this callback for troubleshooting.",
        "parameters": [
            {
                "info": "Information related to the media player. See PlayerUpdatedInfo ."
            }
        ],
        "returns": "",
        "is_hide": true
    },
    {
        "id": "enum_cameradirection",
        "name": "CameraDirection",
        "description": "The camera direction.\n",
        "parameters": [
            {
                "cameraRear": "The rear camera."
            },
            {
                "cameraFront": "The front camera."
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "api_renewtoken",
        "name": "renewToken",
        "description": "Gets a new token when the current token expires after a period of time.\nYou can use this method to pass a new token to the SDK. A token expires after a certain period of time. In the following two cases, the app should call this method to pass in a new token. Failure to do so will result in the SDK disconnecting from the server.\n The SDK triggers the onTokenPrivilegeWillExpire callback.\n The onConnectionStateChanged callback reports connectionChangedTokenExpired(9).",
        "parameters": [
            {
                "token": "The new token."
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "api_enableaudio",
        "name": "enableAudio",
        "description": "Enables the audio module.\nThe audio mode is enabled by default. This method enables the internal engine and can be called anytime after initialization. It is still valid after one leaves channel.\n This method enables the audio module and takes some time to take effect. Agora recommends using the following API methods to control the audio module separately: \n enableLocalAudio : Whether to enable the microphone to create the local audio stream. \n muteLocalAudioStream : Whether to publish the local audio stream. \n muteRemoteAudioStream : Whether to subscribe and play the remote audio stream. \n muteAllRemoteAudioStreams : Whether to subscribe to and play all remote audio streams.",
        "parameters": [],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "api_addhandler",
        "name": "registerEventHandler",
        "description": "Adds event handlers.\nThe SDK uses the RtcEngineEventHandler class to send callbacks to the app. The app inherits the methods of this class to receive these callbacks. All methods in this interface class have default (empty) implementations. Therefore, the application can only inherit some required events. In the callbacks, avoid time-consuming tasks or calling APIs that can block the thread, such as the sendStreamMessage method.\nOtherwise, the SDK may not work properly.",
        "parameters": [
            {
                "eventHandler": "Callback events to be added. "
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "api_setcontentinspec",
        "name": "setContentInspec",
        "description": "",
        "parameters": [
            {
                "null": ""
            }
        ],
        "returns": "",
        "is_hide": true
    },
    {
        "id": "callback_ipacketobserver_onsendaudiopacket",
        "name": "onSendAudioPacket",
        "description": "Occurs when the local user sends an audio packet.\n",
        "parameters": [
            {
                "packet": "The sent audio packet, see Packet ."
            }
        ],
        "returns": "true: The audio packet is sent successfully.\n false: The audio packet is discarded.",
        "is_hide": true
    },
    {
        "id": "api_imediaengine_setexternalvideosource_ng",
        "name": "setExternalVideoSource",
        "description": "Configures the external video source.\nCall this method before joining a channel.",
        "parameters": [
            {},
            {
                "useTexture": "Whether to use the external video frame in the Texture format.\n true: Use the external video frame in the Texture format.\n false: (Default) Do not use the external video frame in the Texture format.\n "
            },
            {
                "sourceType": "Whether to encode the external video frame, see ExternalVideoSourceType ."
            }
        ],
        "returns": "",
        "is_hide": true
    },
    {
        "id": "enum_localaudiostreamerror",
        "name": "LocalAudioStreamError",
        "description": "Local audio state error codes.\n",
        "parameters": [
            {
                " ": "5: The local audio encoding failed."
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "callback_onlastmilequality",
        "name": "onLastmileQuality",
        "description": "Reports the last-mile network quality of the local user.\nThis callback reports the last-mile network conditions of the local user before the user joins the channel. Last mile refers to the connection between the local device and Agora's edge server.\n Before the user joins the channel, this callback is triggered by the SDK once startLastmileProbeTest is called and reports the last-mile network conditions of the local user.",
        "parameters": [
            {
                "quality": "The last-mile network quality. \n qualityUnknown(0): The quality is unknown.\n qualityExcellent(1): The quality is excellent.\n qualityGood(2): The network quality seems excellent, but the bitrate can be slightly lower than excellent.\n qualityPoor(3): Users can feel the communication is slightly impaired.\n qualityBad(4): Users cannot communicate smoothly.\n qualityVbad(5): The quality is so bad that users can barely communicate.\n qualityDown(6): The network is down, and users cannot communicate at all.\n See QualityType ."
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "api_pushcaptureaudioframe",
        "name": "pushCaptureAudioFrame1",
        "description": "Push external audio frames that are not processed by the SDK echo cancellation module.\nThe local user can call topushCaptureAudioFrame1 push the locally collected audio frame to the remote user, but the audio frame will not be processed by the SDK's echo cancellation module. To prevent the local user from hearing his own echo, the remote user needs to call and successively pullAudioFrame pushReverseAudioFrame1 to push the audio frame collected by himself and the received audio frame to the custom echo cancellation module for processing.\n Call enableEchoCancellationExternal this method after calling joinChannelWithOptions ().",
        "parameters": [],
        "returns": "0: Success.\n < 0: Failure.",
        "is_hide": true
    },
    {
        "id": "api_icloudspatialaudioengine_setteamid",
        "name": "setTeamId",
        "description": "Sets the team ID.\nIn the same room, no matter what the audio range mode and audio reception range are, users with the same team ID can hear each other. Whether users with different team IDs can hear each other is determined by the audio range mode and audio reception range.Call this method before enterRoom . A user can only have one team ID in a room, and the team ID cannot be changed after entering the room.",
        "parameters": [
            {
                "teamId": "The team ID. The value must be greater than 0. The default value is 0, which means that the user is not on a team with other users."
            }
        ],
        "returns": "",
        "is_hide": true
    },
    {
        "id": "enum_audiodualmonomode",
        "name": "AudioDualMonoMode",
        "description": "The channel mode.\n",
        "parameters": [
            {
                "audioDualMonoStereo": "0: Original mode."
            },
            {
                "audioDualMonoL": "1: Left channel mode. This mode replaces the audio of the right channel with the audio of the left channel, which means the user can only hear the audio of the left channel."
            },
            {
                "audioDualMonoR": "2: Right channel mode. This mode replaces the audio of the left channel with the audio of the right channel, which means the user can only hear the audio of the right channel."
            },
            {
                "audioDualMonoMix": "3: Mixed channel mode. This mode mixes the audio of the left channel and the right channel, which means the user can hear the audio of the left channel and the right channel at the same time."
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "class_metadata",
        "name": "Metadata",
        "description": "Media metadata.\n",
        "parameters": [
            {
                "uid": "The user ID.\n For the recipient:the ID of the remote user who sent the Metadata.\n Ignore it for sender. "
            },
            {
                "size": "Buffer size for received or sent Metadata."
            },
            {
                "buffer": "The buffer address of the received or sent Metadata."
            },
            {
                "timeStampMs": "The timestamp (ms) of Metadata."
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "api_setloglevel",
        "name": "setLogLevel",
        "description": "Sets the output log level of the SDK.\nDeprecated:\n This method is deprecated. Use RtcEngineContext instead to set the log output level. Choose a level to see the logs preceding that level.",
        "parameters": [
            {
                "level": "The log level: LogLevel ."
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "enum_localvideostreamstate",
        "name": "LocalVideoStreamState",
        "description": "Local video state types.\n",
        "parameters": [
            {
                "localVideoStreamStateStopped": "0: The local video is in the initial state."
            },
            {
                "localVideoStreamStateCapturing": "1: The local video capturing device starts successfully. "
            },
            {
                "localVideoStreamStateEncoding": "2: The first video frame is successfully encoded."
            },
            {
                "localVideoStreamStateFailed": "3: Fails to start the local video."
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "api_ilocalspatialaudioengine_initialize",
        "name": "initialize",
        "description": "Initializes ILocalSpatialAudioEngine .\nBefore calling other methods of the ILocalSpatialAudioEngine class, you need to call this method to initialize ILocalSpatialAudioEngine.\n The SDK supports creating only one ILocalSpatialAudioEngine instance for an app.",
        "parameters": [
            {
                "config": "The configuration of ILocalSpatialAudioEngine. See LocalSpatialAudioConfig for details."
            }
        ],
        "returns": "",
        "is_hide": true
    },
    {
        "id": "callback_iaudioframeobserver_onmixedaudioframe",
        "name": "onMixedAudioFrame",
        "description": "Retrieves the mixed captured and playback audio frame.\nThis callback only returns the single-channel data.\n If you want to set the format of the mixed captured and playback audio frame, Agora recommends you call the setMixedAudioFrameParameters method to set the format of the audio frames after calling the registerAudioFrameObserver method to register an audio frame observer.",
        "parameters": [],
        "returns": "Reserved for future use.",
        "is_hide": true
    },
    {
        "id": "callback_onrequesttoken",
        "name": "onRequestToken",
        "description": "Occurs when the token expires.\nWhen the token expires during a call, the SDK triggers this callback to remind the app to renew the token.\n Once you receive this callback, generate a new token on your app server, and call to joinChannelWithOptions rejoin the channel.",
        "parameters": [
            {
                "connection": "The connection information. See RtcConnection ."
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "callback_ivideoframeobserver_onsecondarypreencodescreenvideoframe",
        "name": "onSecondaryPreEncodeScreenVideoFrame",
        "description": "Gets the video data captured from the second screen before encoding.\nAfter you successfully register the video frame observer, the SDK triggers this callback each time it receives a video frame. In this callback, you can get the video data captured from the screen before encoding and then process the data according to your particular scenarios.\n After processing, you can send the processed video data back to the SDK in this callback. This method applies to Windows only.\n You need to set getObservedFramePosition before you can get the video data captured from the second screen before encoding.\n The video data that this callback gets has been preprocessed, with its content cropped and rotated, and the image enhanced.\n This callback does not support sending processed RGBA video data back to the SDK.",
        "parameters": [
            {
                "videoFrame": "The video frame. "
            }
        ],
        "returns": "true: Sets the SDK to receive the video frame.\n false: Sets the SDK to discard the video frame.",
        "is_hide": true
    },
    {
        "id": "api_setlocalrendermode2",
        "name": "setLocalRenderMode [2/2]",
        "description": "Updates the display mode of the local video view.\nAfter initializing the local video view, you can call this method to update its rendering and mirror modes. It affects only the video view that the local user sees, not the published local video stream. Ensure that you have called the setupLocalVideo method to initialize the local video view before calling this method.\n During a call, you can call this method as many times as necessary to update the display mode of the local video view.",
        "parameters": [
            {
                "renderMode": "The local video display mode. See RenderModeType .\n "
            },
            {
                "mirrorMode": "The rendering mode of the local video view. See VideoMirrorModeType .\n If you use a front camera, the SDK enables the mirror mode by default; if you use a rear camera, the SDK disables the mirror mode by default. "
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "class_rectangle",
        "name": "Rectangle",
        "description": "The location of the target area relative to the screen or window. If you do not set this parameter, the SDK selects the whole screen or window.\n",
        "parameters": [
            {
                "x": "x: The horizontal offset from the top-left corner."
            },
            {
                "y": "y: The vertical offset from the top-left corner."
            },
            {
                "width": "The width of the target area."
            },
            {
                "height": "The height of the target area."
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "callback_onuserenablevideo",
        "name": "onUserEnableVideo",
        "description": "Occurs when a remote user enables/disables the video module.\nOnce the video module is disabled, the user can only use a voice call. The user cannot send or receive any video.\n The SDK triggers this callback when a remote user enables or disables the video module by calling the enableVideo or disableVideo method.",
        "parameters": [
            {
                "connection": "The connection information. See RtcConnection ."
            },
            {
                "remoteUid": "The user ID of the remote user."
            },
            {
                "enabled": "true: Enable.\n false: Disable.\n "
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "api_iaudiodevicemanager_stopplaybackdevicetest",
        "name": "stopPlaybackDeviceTest",
        "description": "Stops the audio playback device test.\nThis method stops the audio playback device test. You must call this method to stop the test after calling the startPlaybackDeviceTest method.\n Ensure that you call this method before joining a channel.",
        "parameters": [],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "callback_onextensionstarted",
        "name": "onExtensionStarted",
        "description": "Occurs when the extension is enabled.\nAfter a successful call of enableExtension (true), this callback is triggered.",
        "parameters": [
            {
                "provider": "The name of the extension provider."
            },
            {
                "extName": "The name of the extension."
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "class_imediaengine",
        "name": "IMediaEngine",
        "description": "The IMediaEngine class.\n",
        "parameters": [],
        "returns": "",
        "is_hide": true
    },
    {
        "id": "callback_onmediaenginestartcallsuccess",
        "name": "onMediaEngineStartCallSuccess",
        "description": "媒体引擎成功启动的回调。\n",
        "parameters": [],
        "returns": "",
        "is_hide": true
    },
    {
        "id": "api_joinchannel",
        "name": "joinChannel",
        "description": "Joins a channel.\nWhen the connection between the client and Agora's server is interrupted due to poor network conditions, the SDK tries reconnecting to the server. When the local client successfully rejoins the channel, the SDK triggers the onRejoinChannelSuccess callback on the local client.\n A successful call of this method triggers the following callbacks: \n The local client: The onJoinChannelSuccess and onConnectionStateChanged callbacks.\n The remote client: onUserJoined , if the user joining the channel is in the Communication profile or is a host in the Live-broadcasting profile. This method enables users to join a channel. Users in the same channel can talk to each other, and multiple users in the same channel can start a group chat. Users with different App IDs cannot call each other.\n Once a user joins the channel, the user subscribes to the audio and video streams of all the other users in the channel by default, giving rise to usage and billing calculation. To stop subscribing to a specified stream or all remote streams, call the corresponding mute methods.",
        "parameters": [
            {
                "channelId": "The channel name. This parameter signifies the channel in which users engage in real-time audio and video interaction. Under the premise of the same App ID, users who fill in the same channel ID enter the same channel for audio and video interaction. The string length must be less than 64 bytes. Supported characters:\n All lowercase English letters: a to z.\n All uppercase English letters: A to Z.\n All numeric characters: 0 to 9.\n Space\n \"!\", \"#\", \"$\", \"%\", \"&amp;\", \"(\", \")\", \"+\", \"-\", \":\", \";\", \"&lt;\", \"= \", \".\", \"&gt;\", \"?\", \"@\", \"[\", \"]\", \"^\", \"_\", \"{\", \"}\", \"|\", \"~\", \",\" "
            },
            {
                "token": "The token generated on your server for authentication. See \n "
            },
            {
                "info": "(Optional) Reserved for future use."
            },
            {
                "uid": "The user ID. This parameter is used to identify the user in the channel for real-time audio and video interaction. You need to set and manage user IDs yourself, and ensure that each user ID in the same channel is unique. This parameter is a 32-bit unsigned integer. The value range is 1 to 232-1. If the user ID is not assigned (or set to 0), the SDK assigns a random user ID and returns it in the onJoinChannelSuccess callback. Your application must record and maintain the returned user ID, because the SDK does not do so."
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "api_setdefaultmuteallremotevideostreams",
        "name": "setDefaultMuteAllRemoteVideoStreams",
        "description": "Stops or resumes subscribing to the video streams of all remote users by default.\nCall this method after joining a channel. After successfully calling this method, the local user stops or resumes subscribing to the audio streams of all subsequent users. If you need to resume subscribing to the audio streams of remote users in the channel after calling this method, do the following:\n To resume subscribing to the audio stream of a specified user, call muteRemoteVideoStream (false), and specify the user ID.\n To resume subscribing to the audio streams of multiple remote users, call muteRemoteVideoStream(false) multiple times.",
        "parameters": [
            {
                "mute": "Whether to stop subscribing to the audio streams of all remote users by default.\n true: Stop subscribing to the audio streams of all remote users by default.\n false: (Default) Resume subscribing to the audio streams of all remote users by default. "
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "api_create",
        "name": "create [1/2]",
        "description": "Creates and initializes RtcEngine .\nAll called methods provided by the RtcEngine class are executed asynchronously. Agora recommends calling these methods in the same thread. Before calling other APIs, you must call this method to create the RtcEngine object.\n You can create the RtcEngine instance either by calling this method or by calling initialize . The difference between initialize and this method is that initialize supportsmore configurations when creating the RtcEngine instance, for example, specifying the region for connection and setting the log files.\n The SDK supports creating only one RtcEngine instance for an app.",
        "parameters": [
            {
                "appId": ""
            }
        ],
        "returns": "The RtcEngine instance, if the method call succeeds.\n An error code, if the call fails.",
        "is_hide": false
    },
    {
        "id": "api_adjustcustomaudioplayoutvolume_ng",
        "name": "adjustCustomAudioPlayoutVolume",
        "description": "",
        "parameters": [
            {
                "null": ""
            }
        ],
        "returns": "",
        "is_hide": true
    },
    {
        "id": "callback_onuserenablelocalvideo",
        "name": "onUserEnableLocalVideo",
        "description": "Occurs when a specific remote user enables/disables the local video capturing function.\nThe SDK triggers this callback when the remote user resumes or stops capturing the video stream by calling the enableLocalVideo method.",
        "parameters": [
            {
                "connection": "The connection information. See RtcConnection ."
            },
            {
                "remoteUid": "The user ID of the remote user."
            },
            {
                "enabled": "Whether the specified remote user enables/disables the local video capturing function: true: Enable. Other users in the channel can see the video of this remote user.\n false: Disable. Other users in the channel can no longer receive the video stream from this remote user, while this remote user can still receive the video streams from other users. "
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "api_ibasespatialaudioengine_updateselfposition",
        "name": "updateSelfPosition",
        "description": "Updates the spatial position of the local user.\nWhen this method is called under different classes, the effect is different:\n When this method is called under the ICloudSpatialAudioEngine class, the SDK updates the spatial position of the local user to the Agora Spatial Audio Server. The Agora Spatial Audio Server calculates the user's spatial audio effect parameters according to the world coordinates and audio reception range of the local and remote users.\n Under the ILocalSpatialAudioEngine class, this method needs to be used with updateRemotePosition . The SDK calculates the relative position between the local and remote users according to this method and the parameter settings in updateRemotePosition, and then calculates the user's spatial audio effect parameters. \n Call this method after enterRoom .\n If you call this method under the ICloudSpatialAudioEngine class, note the following:\n When you call this method multiple times, Agora recommends a call interval of [120,7000) milliseconds; otherwise, the SDK and the Agora Spatial Audio Server lose synchronization.\n If the distance between the current spatial position and the last position is less than 0.2 meters or the rotation angle in each direction is less than 15 degrees, the SDK does not update the current spatial position.",
        "parameters": [
            {
                "position": "The coordinates in the world coordinate system. This parameter is an array of length 3, and the three values represent the front, right, and top coordinates in turn."
            },
            {
                "axisForward": "The unit vector of the x axis in the coordinate system. This parameter is an array of length 3, and the three values represent the front, right, and top coordinates in turn."
            },
            {
                "axisRight": "The unit vector of the y axis in the coordinate system. This parameter is an array of length 3, and the three values represent the front, right, and top coordinates in turn."
            },
            {
                "axisUp": "The unit vector of the z axis in the coordinate system. This parameter is an array of length 3, and the three values represent the front, right, and top coordinates in turn."
            }
        ],
        "returns": "",
        "is_hide": true
    },
    {
        "id": "api_registeraudiospectrumobserver",
        "name": "registerAudioSpectrumObserver",
        "description": "Registers an audio spectrum observer.\nAfter successfully registering the Audio Spectrum Observer and calling the enableAudioSpectrumMonitor Audio Spectrum Monitor, the SDK\nCallbacks you implement in your class will be reported at intervals you IAudioSpectrumObserver set.\n You can call this method either before or after joining a channel.",
        "parameters": [
            {}
        ],
        "returns": "",
        "is_hide": true
    },
    {
        "id": "api_unregistermediametadataobserver",
        "name": "unregisterMediaMetadataObserver",
        "description": "Unregisters the specified metadata observer.\n",
        "parameters": [],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "enum_audiofilerecordingtype",
        "name": "audiorecordingqualitytype",
        "description": "Recording quality.\n",
        "parameters": [
            {
                " ": "2: High quality. The sample rate is 32 kHz, and the file size is around 3.75 MB after 10 minutes of recording."
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "callback_oncameraexposureareachanged",
        "name": "onCameraExposureAreaChanged",
        "description": "Occurs when the camera exposure area changes.\n",
        "parameters": [],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "api_setplaybackaudioframeparameters",
        "name": "setPlaybackAudioFrameParameters",
        "description": "Sets the audio data format for playback.\nSets the data format for the onPlaybackAudioFrame callback. Ensure that you call this method before joining a channel.\n The SDK calculates the sampling interval based on the samplesPerCall, sampleRate, and channel parameters set in this method. The calculation formula is as follows:Sample interval = samplePerCall/(sampleRate × channel). Ensure that the sample interval ≥ 0.01 (s). The SDK triggers the onPlaybackAudioFrame callback according to the sampling interval.",
        "parameters": [
            {
                "sampleRate": "onPlaybackAudioFrameThe sample rate returned in the callback, which can be set as 8000, 16000, 32000, 44100, or 48000 Hz."
            },
            {
                "channel": "The number of channels returned in the onPlaybackAudioFrame callback:\n 1: Mono.\n 2: Stereo. "
            },
            {
                "mode": "The use mode of the audio frame. See RawAudioFrameOpModeType .\n "
            },
            {
                "samplesPerCall": "The number of data samples returned in the onPlaybackAudioFrame callback, such as 1024 for the media push."
            }
        ],
        "returns": "",
        "is_hide": true
    },
    {
        "id": "enum_userofflinereasontype",
        "name": "UserOfflineReasonType",
        "description": "Reasons for a user being offline.\n",
        "parameters": [
            {
                "userOfflineQuit": "0: The user quits the call."
            },
            {
                "userOfflineDropped": "1: The SDK times out and the user drops offline because no data packet is received within a certain period of time. If the user quits the call and the message is not passed to the SDK (due to an unreliable channel), the SDK assumes the user dropped offline.\n"
            },
            {
                "userOfflineBecomeAudience": "2: The user switches the client role from the host to the audience."
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "callback_onmediaengineloadsuccess",
        "name": "onMediaEngineLoadSuccess",
        "description": "媒体引擎成功加载的回调。\n",
        "parameters": [],
        "returns": "",
        "is_hide": true
    },
    {
        "id": "callback_ipacketobserver_onreceivevideopacket",
        "name": "onReceiveVideoPacket",
        "description": "Occurs when the local user receives a video packet.\n",
        "parameters": [
            {
                "packet": "The received video packet, see Packet ."
            }
        ],
        "returns": "true: The video packet is received successfully.\n false: The video packet is discarded.",
        "is_hide": true
    },
    {
        "id": "class_remotevideostats",
        "name": "RemoteVideoStats",
        "description": "The statistics of the remote video stream.\n",
        "parameters": [
            {
                "uid": "The ID of the remote user sending the video stream."
            },
            {
                "delay": " Deprecated:\n In scenarios where audio and video are synchronized, you can get the video delay datafrom networkTransportDelay and jitterBufferDelay in RemoteAudioStats . Delay (ms).\n "
            },
            {
                "width": "Width (pixels) of the video stream."
            },
            {
                "height": "Height (pixels) of the video stream."
            },
            {
                "receivedBitrate": "Bitrate (Kbps) received since the last count."
            },
            {
                "decoderOutputFrameRate": "The decoder output frame rate (fps) of the remote video."
            },
            {
                "rendererOutputFrameRate": "The render output frame rate (fps) of the remote video."
            },
            {
                "frameLossRate": "Remote video packet loss rate (%)."
            },
            {
                "packetLossRate": "Packet loss rate (%) of the remote video stream after using the anti-packet-loss method."
            },
            {
                "rxStreamType": "The type of the remote video stream. See VideoStreamType ."
            },
            {
                "totalFrozenTime": "The total freeze time (ms) of the remote video stream after the remote user joins the channel. In a video session where the frame rate is set to no less than 5 fps, video freeze occurs when the time interval between two adjacent renderable video frames is more than 500 ms."
            },
            {
                "frozenRate": "The total video freeze time as a percentage (%) of the total time when the video is available. The video is considered available when the remote user neither stops sending the audio stream nor disables the audio module after joining the channel."
            },
            {
                "totalActiveTime": "The total freeze time (ms) of the remote video stream after the remote user joins the channel.\n The total effective duration of the video is the duration of the call after the remote user or host joins the channel and neither stops sending the video stream nor disables the video module.\n "
            },
            {
                "publishDuration": "The total duration (ms) of the remote video stream.\n "
            },
            {
                "superResolutionType": "The state of super resolution:\n >0: Super resolution is enabled.\n =0: Super resolution is not enabled.\n "
            },
            {
                "avSyncTimeMs": "The amount of time (ms) that the audio is ahead of the video.If this value is negative, the audio is lagging behind the video."
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "api_setclientrole2",
        "name": "setClientRole",
        "description": "Sets the user role and level in an interactive live streaming channel.\nIn the interactive live streaming profile, the SDK sets the user role as audience by default. You can call this method to set the user role as host.\n You can call this method either before or after joining a channel.\n If you call this method to set the user's role as the host before joining the channel and set the local video property through the setupLocalVideo method, the local video preview is automatically enabled when the user joins the channel.\n If you call this method to switch the user role after joining a channel, the SDK automatically does the following:\n Calls muteLocalAudioStream and muteLocalVideoStream to change the publishing state.\n Triggers onClientRoleChanged on the local client.\n Triggers onUserJoined or onUserOffline on the remote client. This method applies to the interactive live streaming profile (the profile parameter of setChannelProfile is channelProfileLiveBroadcasting) only.",
        "parameters": [
            {
                "role": "The user role in the interactive live streaming. See ClientRoleType ."
            },
            {
                "options": "The detailed options of a user, including the user level. See ClientRoleOptions ."
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "api_enablevirtualbackground",
        "name": "enableVirtualBackground",
        "description": "Enables/Disables the virtual background (beta feature).\nThe virtual background function allows you to replace the original background image of the local user or to blur the background. After successfully enabling the virtual background function, all users in the channel can see the customized background.\n Enabling the virtual background function involves a series of method calls. The calling sequence is as follows:\n Call loadExtensionProvider (libagora_segmentation_extension.dll) during RtcEngine initialization to specify the extension's library path.\n Call enableExtension (agora_segmentation, PortraitSegmentation, true) to enable the extension.\n Call enableVideo to enable the video module.\n Call this method to enable the virtual background function. \n This function requires a high-performance device. Agora recommends that you use this function on devices with the following chips:\n Snapdragon 700 series 750G and later\n Snapdragon 800 series 835 and later\n Dimensity 700 series 720 and later\n Kirin 800 series 810 and later\n Kirin 900 series 980 and later\n Devices with an A9 chip and better, as follows:\n iPhone 6S and later\n iPad Air 3rd generation and later\n iPad 5th generation and later\n iPad Pro 1st generation and later\n iPad mini 5th generation and later Agora recommends that you use this function in scenarios that meet the following conditions:\n A high-definition camera device is used, and the environment is uniformly lit.\n The captured video image is uncluttered, the user's portrait is half-length and largely unobstructed, and the background is a single color that differs from the color of the user's clothing.",
        "parameters": [
            {
                "enabled": "Whether to enable virtual background:\n true: Enable the image enhancement function.\n false: Disable virtual background.\n "
            },
            {
                "backgroundSource": "The custom background image. See VirtualBackgroundSource . To adapt the resolution of the custom background image to that of the video captured by the SDK, the SDK scales and crops the custom background image while ensuring that the content of the custom background image is not distorted."
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "api_stopprimaryscreencapture",
        "name": "stopPrimaryScreenCapture",
        "description": "Stop sharing the first screen.\nAfter calling startPrimaryScreenCapture , you can call this method to stop sharing the first screen.",
        "parameters": [],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "api_seteffectsvolume",
        "name": "setEffectsVolume",
        "description": "Sets the volume of the audio effects.\nCall this method after the playEffect method.",
        "parameters": [
            {
                "volume": "The playback volume. The value range is [0, 100]. The default value is 100, which represents the original volume."
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "api_getversion",
        "name": "getVersion",
        "description": "Gets the SDK version.\n",
        "parameters": [],
        "returns": "The SDK version number. The format is a string. See SDKBuildInfo .",
        "is_hide": false
    },
    {
        "id": "api_unregisteraudiospectrumobserver",
        "name": "unregisterAudioSpectrumObserver",
        "description": "Unregisters the audio spectrum observer.\nAfter calling registerAudioSpectrumObserver , if you want to disable audio spectrum monitoring, you can call this method.\n You can call this method either before or after joining a channel.",
        "parameters": [],
        "returns": "",
        "is_hide": true
    },
    {
        "id": "api_adjustuserplaybacksignalvolume",
        "name": "adjustUserPlaybackSignalVolume",
        "description": "Adjusts the playback signal volume of a specified remote user.\nYou can call this method to adjust the playback volume of a specified remote user. To adjust the playback volume of different remote users, call the method as many times, once for each remote user. Call this method after joining a channel.\n The playback volume here refers to the mixed volume of a specified remote user.",
        "parameters": [
            {
                "volume": "Audio mixing volume. The value ranges between 0 and 100. The default value is 100, the original volume."
            },
            {
                "uid": "The user ID of the remote user."
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "enum_videobuffertype",
        "name": "VideoBufferType",
        "description": "The video buffer type.\n",
        "parameters": [
            {
                "videoBufferRawData": "1: The video buffer in the format of raw data."
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "enum_proxytype",
        "name": "CloudProxyType",
        "description": "The cloud proxy type.\n",
        "parameters": [
            {
                "noneProxy": "0: The automatic mode. In this mode, the SDK attempts a direct connection to SD-RTN™ and automatically switches to TCP/TLS 443 if the attempt fails. "
            },
            {
                "udpProxy": "1: The cloud proxy for the UDP protocol, that is, the Force UDP cloud proxy mode. In this mode, the SDK always transmits data over UDP."
            },
            {
                "tcpProxy": "2: The cloud proxy for the TCP (encryption) protocol, that is, the Force TCP cloud proxy mode. In this mode, the SDK always transmits data over TCP/TLS 443."
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "api_setlivetranscoding",
        "name": "setLiveTranscoding",
        "description": "Sets the transcoding configurations for Media Push.\nDeprecated:\n This method is deprecated. Use startRtmpStreamWithTranscoding or updateRtmpTranscoding instead according to your needs. This method sets the video layout and audio settings for Media Push. The SDK triggers the onTranscodingUpdated callback when you call this method to update the transcoding settings. This method takes effect only when you are a host in live interactive streaming.\n Ensure that you enable the RTMP Converter service before using this function. See Prerequisites in the advanced guide Push Streams to CDN.\n If you call this method to set the transcoding configuration for the first time, the SDK does not trigger the onTranscodingUpdated callback.\n Call this method after joining a channel.",
        "parameters": [
            {
                "transcoding": "The transcoding configurations for Media Push. See LiveTranscoding.\n "
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "callback_onnetworkquality",
        "name": "onNetworkQuality",
        "description": "Reports the last mile network quality of each user in the channel.\nThis callback reports the last mile network conditions of each user in the channel. Last mile refers to the connection between the local device and Agora's edge server.\n The SDK triggers this callback once every two seconds. If a channel includes multiple users, the SDK triggers this callback as many times.\n txQuality is rxQuality is",
        "parameters": [
            {
                "connection": "The connection information. See RtcConnection ."
            },
            {
                "remoteUid": "The user ID. The network quality of the user with this user ID is reported.\n "
            },
            {
                "txQuality": "Uplink network quality rating of the user in terms of the transmission bit rate, packet loss rate, average RTT (Round-Trip Time) and jitter of the uplink network. This parameter is a quality rating helping you understand how well the current uplink network conditions can support the selected video encoder configuration. For example, a 1000 Kbps uplink network may be adequate for video frames with a resolution of 640 × 480 and a frame rate of 15 fps in the LIVE_BROADCASTING profile, but may be inadequate for resolutions higher than 1280 × 720. See QualityType ."
            },
            {
                "rxQuality": "Downlink network quality rating of the user in terms of packet loss rate, average RTT, and jitter of the downlink network. See QualityType ."
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "api_loadextensionprovider",
        "name": "loadExtensionProvider",
        "description": "Adds an extension to the SDK.\nThis method applies to Windows only.",
        "parameters": [
            {
                "path": "The extension library path and name. For example: /library/libagora_segmentation_extension.dll."
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "class_localvideostats",
        "name": "LocalVideoStats",
        "description": "The statistics of the local video stream.\n",
        "parameters": [
            {
                "uid": "The ID of the local user."
            },
            {
                "sentBitrate": "The actual bitrate (Kbps) while sending the local video stream.This value does not include the bitrate for resending the video after packet loss.\n "
            },
            {
                "sentFrameRate": "The actual frame rate (fps) while sending the local video stream.This value does not include the frame rate for resending the video after packet loss."
            },
            {
                "encoderOutputFrameRate": "The output frame rate (fps) of the local video encoder."
            },
            {
                "rendererOutputFrameRate": "The output frame rate (fps) of the local video renderer."
            },
            {
                "targetBitrate": "The target bitrate (Kbps) of the current encoder. This is an estimate made by the SDK based on the current network conditions."
            },
            {
                "targetFrameRate": "The target frame rate (fps) of the current encoder."
            },
            {
                "qualityAdaptIndication": "Quality adaption of the local video stream in the reported interval (based on the target frame rate and target bitrate). See QualityAdaptIndication ."
            },
            {
                "encodedBitrate": "The bitrate (Kbps) while encoding the local video stream.This value does not include the bitrate for resending the video after packet loss.\n "
            },
            {
                "encodedFrameWidth": "The width of the encoded video (px)."
            },
            {
                "encodedFrameHeight": "The height of the encoded video (px)."
            },
            {
                "encodedFrameCount": "The number of the sent video frames, represented by an aggregate value."
            },
            {
                "codecType": "The codec type of the local video. See VideoCodecType ."
            },
            {
                "txPacketLossRate": "The video packet loss rate (%) from the local client to the Agora server before applying the anti-packet loss strategies."
            },
            {
                "captureFrameRate": "The frame rate (fps) for capturing the local video stream."
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "api_pauseaudio",
        "name": "pauseAudio",
        "description": "Since\n v",
        "parameters": [
            {
                "null": ""
            }
        ],
        "returns": "",
        "is_hide": true
    },
    {
        "id": "api_enablevideo",
        "name": "enableVideo",
        "description": "Enables the video module.\nCall this method either before joining a channel or during a call. If this method is called before joining a channel, the call starts in the video mode. Call disableVideo to disable the video mode.\n A successful call of this method triggers the onRemoteVideoStateChanged callback on the remote client. This method enables the internal engine and is valid after leaving the channel.\n This method resets the internal engine and takes some time to take effect. Agora recommends using the following API methods to control the video engine modules separately: \n enableLocalVideo : Whether to enable the camera to create the local video stream. \n muteLocalVideoStream : Whether to publish the local video stream. \n muteRemoteVideoStream : Whether to subscribe to and play the remote video stream. \n muteAllRemoteVideoStreams : Whether to subscribe to and play all remote video streams.",
        "parameters": [],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "api_setdefaultaudioroutetospeakerphone_ng",
        "name": "setDefaultAudioRouteToSpeakerphone",
        "description": "Sets the default audio playback route\nThis method is for Android and iOS only. Most mobile phones have two audio routes: an earpiece at the top, and a speakerphone at the bottom. The earpiece plays at a lower volume, and the speakerphone at a higher volume. When setting the default audio route, you determine whether audio playback comes through the earpiece or speakerphone when no external audio device is connected.",
        "parameters": [
            {
                "defaultToSpeaker": "Whether to set the speakerphone as the default audio route:\n true: Set the speakerphone as the default audio route.\n false: Set the earpiece as the default audio route.\n "
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "enum_streampublishstate",
        "name": "StreamPublishState",
        "description": "The publishing state.\n",
        "parameters": [
            {
                "pubStateIdle": "0: The initial publishing state after joining the channel."
            },
            {
                "pubStateNoPublished": "1: Fails to publish the local stream. Possible reasons:\n Local user calls muteLocalAudioStream (true) or muteLocalVideoStream (true) to stop sending local media streams.\n The local user calls disableAudio or disableVideo to disable the local audio or video module.\n The local user calls enableLocalAudio (false) or enableLocalVideo (false) to disable the local audio or video capture.\n The role of the local user is audience. "
            },
            {
                "pubStatePublishing": "2: Publishing."
            },
            {
                "pubStatePublished": "3: Publishes successfully."
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "enum_captureroutputpreference",
        "name": "CAPTURER_OUTPUT_PREFERENCE",
        "description": "The camera capture preference.\n",
        "parameters": [
            {
                "CAPTURER_OUTPUT_PREFERENCE_AUTO": "0: (Default) Automatically adjust the camera capture preference. The SDK adjusts the camera output parameters according to the system performance and network conditions to balance CPU consumption and video preview quality."
            },
            {
                "CAPTURER_OUTPUT_PREFERENCE_PERFORMANCE": "1: Prioritizes the system performance. The SDK chooses the dimension and frame rate of the local camera capture closest to those set by . In this case, the local preview quality depends on the encoder. setVideoEncoderConfiguration "
            },
            {
                "CAPTURER_OUTPUT_PREFERENCE_PREVIEW": "2: Prioritizes the local preview quality. The SDK chooses higher camera output parameters to improve the local video preview quality. This option requires extra CPU and RAM usage for video pre-processing."
            },
            {
                "CAPTURER_OUTPUT_PREFERENCE_MANUAL": "3: Allows you to customize the width and height of the video image captured by the local camera.\n "
            }
        ],
        "returns": "",
        "is_hide": true
    },
    {
        "id": "api_imediaplayer_switchsrc",
        "name": "switchSrc",
        "description": "Switches the media resource being played.\nYou can call this method to switch the media resource to be played according to the current network status. For example:\n When the network is poor, the media resource to be played is switched to a media resource address with a lower bitrate.\n When the network is good, the media resource to be played is switched to a media resource address with a higher bitrate. After calling this method, if you receive the playerEventSwitchComplete event in the onPlayerEvent callback, the switch is successful; If you receive the playerEventSwitchError event in the onPlayerEvent callback, the switch fails.\n If you want to customize CDN routes for playing the media resource, call the switchAgoraCDNSrc method to switch media resources. Agora changes the CDN route through the self-developed scheduling center to improve the viewing experience. If you do not need to customize CDN routes for playing the media resource, call the switchSrc method to switch media resources. Ensure that you call this method after open .\n To ensure normal playback, pay attention to the following when calling this method:\n Do not call this method when playback is paused.\n Do not call the seek method during switching.\n Before switching the media resource, make sure that the playback position does not exceed the total duration of the media resource to be switched.",
        "parameters": [
            {
                "syncPts": "Whether to synchronize the playback position (ms) before and after the switch:\n true: Synchronize the playback position before and after the switch.\n false: (Default) Do not synchronize the playback position before and after the switch. falseMake sure to set this parameter as if you need to play live streams, or the switch fails. If you need to play on-demand streams, you can set the value of this parameter according to your scenarios.\n "
            },
            {
                "src": "The URL of the media resource."
            }
        ],
        "returns": "",
        "is_hide": true
    },
    {
        "id": "enum_contentinspecttype",
        "name": "CONTENT_INSPECT_TYPE",
        "description": "内容审核类型。\n",
        "parameters": [
            {
                "null": "2：截图。 SDK 会对视频流进行截图并上传。"
            }
        ],
        "returns": "",
        "is_hide": true
    },
    {
        "id": "api_getextensionproperty",
        "name": "getExtensionProperty",
        "description": "",
        "parameters": [
            {
                "null": ""
            }
        ],
        "returns": "",
        "is_hide": true
    },
    {
        "id": "callback_onfacepositionchanged",
        "name": "onFacePositionChanged",
        "description": "Reports the face detection result of the local user.\nOnce you enable face detection by calling enableFaceDetection (true), you can get the following information on the local user in real-time:\n The width and height of the local video.\n The position of the human face in the local view.\n The distance between the human face and the screen. This value is based on the fitting calculation of the local video size and the position of the human face. This callback is for Android and iOS only.\n When it is detected that the face in front of the camera disappears, the callback will be triggered immediately. When no human face is detected, the frequency of this callback to be rtriggered wil be decreased to reduce power consumption on the local device.\n The SDK stops triggering this callback when a human face is in close proximity to the screen.\n On Android, the value of distance reported in this callback may be slightly different from the actual distance. Therefore, Agora does not recommend using it for accurate calculation.",
        "parameters": [
            {
                "imageWidth": "The width (px) of the video image captured by the local camera."
            },
            {
                "imageHeight": "The height (px) of the video image captured by the local camera."
            },
            {
                "vecRectangle": ""
            },
            {
                "vecDistance": "The distance between the human face and the device screen (cm)."
            },
            {
                "numFaces": "The number of faces detected. If the value is 0, it means that no human face is detected."
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "api_startpreview",
        "name": "startPreview",
        "description": "Enables the local video preview.\nThis method starts the local video preview before joining the channel. Before calling this method, ensure that you do the following:\n Call setupLocalVideo to set the local preview window.\n Call enableVideo to enable the video. \n The local preview enables the mirror mode by default.\n After the local video preview is enabled, if you call leaveChannel [1/2] to exit the channel, the local preview remains until you call stopPreview1 to disable it.",
        "parameters": [],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "callback_onremotevideostats",
        "name": "onRemoteVideoStats",
        "description": "Reports the statistics of the video stream sent by each remote users.\nReports the statistics of the video stream from the remote users. The SDK triggers this callback once every two seconds for each remote user. If a channel has multiple users/hosts sending video streams, the SDK triggers this callback as many times.",
        "parameters": [
            {
                "stats": "Statistics of the remote video stream. "
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "api_setmaxmetadatasize",
        "name": "setMaxMetadataSize",
        "description": "Sets the maximum size of media metadata information.\nAfter calling registerMediaMetadataObserver , you can call this method to set the maximum size of media metadata information.",
        "parameters": [
            {
                "size": "Sets the maximum size of media metadata information."
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "api_resumeallchannelmediarelay",
        "name": "resumeAllChannelMediaRelay",
        "description": "Resumes the media stream relay to all destination channels.\nAfter calling the pauseAllChannelMediaRelay method, you can call this method to resume relaying media streams to all destination channels.\n After a successful method call, the SDK triggers the onChannelMediaRelayEvent callback to report whether the media stream relay is successfully resumed.\n Call this method after the pauseAllChannelMediaRelay method.",
        "parameters": [],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "class_cameracapturerconfiguration_ng",
        "name": "CameraCapturerConfiguration",
        "description": "The camera capturer preference.\n",
        "parameters": [
            {
                "cameraDirection": "This parameter applies to Android and iOS only.The camera direction.  CameraDirection "
            },
            {
                "format": "See VideoFormat ."
            },
            {
                "deviceId": "The device ID of the playback device. The maximum length is MaxDeviceIdLengthType ."
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "api_imediaplayer_unregistervideoframeobserver",
        "name": "unregisterVideoFrameObserver",
        "description": "Unregisters the video frame observer.\n",
        "parameters": [
            {
                "observer": "The video frame observer. See IVideoFrameObserver ."
            }
        ],
        "returns": "",
        "is_hide": true
    },
    {
        "id": "callback_imediaplayersourceobserver_onagoracdntokenwillexpire",
        "name": "onAgoraCDNTokenWillExpire",
        "description": "Occurs when the token is about to expire.\nIf the ts is about to expire when you call the switchAgoraCDNLineByIndex method to switch the CDN route for playing the media resource, the SDK triggers this callback to remind you to renew the authentication information. You need to call the renewAgoraCDNSrcToken method to pass in the updated authentication information to update the authentication information of the media resource URL. After updating the authentication information, you need to call switchAgoraCDNLineByIndex to complete the route switching.",
        "parameters": [],
        "returns": "",
        "is_hide": true
    },
    {
        "id": "api_setencryptionsecret",
        "name": "setEncryptionSecret",
        "description": "Enables built-in encryption with an encryption password before users join a channel.\nDeprecated:\n Use enableEncryption instead. Before joining the channel, you need to call this method to set the secret parameter to enable the built-in encryption. All users in the same channel should use the same secret. The secret is automatically cleared once a user leaves the channel. If you do not specify the secret or secret is set as null, the built-in encryption is disabled. Do not use this method for CDN live streaming.\n For optimal transmission, ensure that the encrypted data size does not exceed the original data size + 16 bytes. 16 bytes is the maximum padding size for AES encryption.",
        "parameters": [
            {
                "secret": "The encryption password."
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "class_leavechanneloptions",
        "name": "LeaveChannelOptions",
        "description": "The options for leaving a channel.\n",
        "parameters": [
            {
                "stopAudioMixing": "Whether to stop playing and mixing the music file when a user leaves the channel. true: (Default) Stop playing and mixing the music file.\n false: Do not stop playing and mixing the music file.\n "
            },
            {
                "stopAllEffect": "Whether to stop playing all audio effects when a user leaves the channel. true: (Default) Stop playing all audio effects.\n false: Do not stop playing any audio effect.\n "
            },
            {
                "stopMicrophoneRecording": "Whether to stop microphone recording when a user leaves the channel. true: (Default) Stop microphone recording.\n false: Do not stop microphone recording.\n "
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "api_startechotest",
        "name": "startEchoTest [1/2]",
        "description": "Starts an audio call test.\nDeprecated:\n This method is deprecated, please use startEchoTest instead. This method starts an audio call test to determine whether the audio devices (for example, headset and speaker) and the network connection are working properly. To conduct the test, the user speaks, and the recording is played back within 10 seconds. If the user can hear the recording within the interval, the audio devices and network connection are working properly. Call this method before joining a channel.\n After calling startEchoTest [1/2], you must call stopEchoTest to end the test. Otherwise, the app cannot perform the next echo test, and you cannot join the channel.\n In the live streaming channels, only a host can call this method.",
        "parameters": [],
        "returns": "0: Success.\n < 0: Failure.",
        "is_hide": true
    },
    {
        "id": "api_imediaplayer_openwithagoracdnsrc",
        "name": "openWithAgoraCDNSrc",
        "description": "Opens the media resource, and requests all the CDN routes of the media resources through the self-developed scheduling center.\nAfter you call this method, Agora opens the media resources and tries to obtain all the CDN routes for playing the media resource. By default, Agora uses the first CDN route for playing, and you can call the switchAgoraCDNLineByIndex method to switch routes.\n If you want to improve the security of the connection and the privacy of media files, contact to determine the sign and the ts fields for authentication. Once the fields are determined, use them as the query parameter of the URL to update the URL of the media resource. For example:\n The URL of the media file to be opened :rtmp://$domain/$appName/$streamName.\n The URL updated by the authentication of the media file to be opened: rtmp://$domain/$appName/$streamName?ts=$ts&sign=$sign Authentication information:\n sign: An encrypted string calculated according to the MD5 algorithm based on authKey, appName, streamName and ts. You need to for your authKey.\n ts: The timestamp when the authentication information expires. You can set the validity period of the authentication information according to your scenarios. For example, 24h or 1h30m20s.",
        "parameters": [
            {
                "src": "The URL of the media resource."
            },
            {
                "startPos": "The starting position (ms) for playback. The default value is 0. This value can be empty if the media resource to be played is live streams."
            }
        ],
        "returns": "",
        "is_hide": true
    },
    {
        "id": "callback_onencryptionerror",
        "name": "onEncryptionError",
        "description": "Reports the built-in encryption errors.\nWhen encryption is enabled by calling enableEncryption , the SDK triggers this callback if an error occurs in encryption or decryption on the sender or the receiver side.",
        "parameters": [
            {
                "connection": "The connection information. See RtcConnection ."
            },
            {
                "errorType": ""
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "class_mediaplayercontroller",
        "name": "MediaPlayerController",
        "description": "The AgoraVideoView controller used to render the video for the media player.\n",
        "parameters": [
            {
                "useAndroidSurfaceView": "Whether to use Android SurfaceView to render video:\n true: Use Android SurfaceView to render video.\n false: Do not use Android SurfaceView to render video. Android SurfaceView applies to Android platform only."
            },
            {
                "useFlutterTexture": "Whether to use FlutterTexture to render video:\n true: Use FlutterTexture to render video.\n false: Do not use FlutterTexture to render video. FlutterTexture applies to iOS, macOS and Windows platforms."
            },
            {
                "canvas": "Local video display properties. See VideoCanvas ."
            },
            {
                "rtcEngine": " RtcEngine ."
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "class_encodedvideoframeinfo",
        "name": "EncodedVideoFrameInfo",
        "description": "The information about the external encoded video frame.\n",
        "parameters": [
            {
                "codecType": "The codec type of the local video stream. See VideoCodecType . The default value is videoCodecH264(2)."
            },
            {
                "width": "The width (pixel) of the video frame."
            },
            {
                "height": "The height (pixel) of the video frame."
            },
            {
                "framesPerSecond": "The number of video frames per second.\n When this parameter is not 0, you can use it to calculate the Unix timestamp of the external encoded video frames.\n "
            },
            {
                "frameType": "The video frame type, see VideoFrameType ."
            },
            {
                "rotation": "The rotation information of the video frame, see VideoOrientation ."
            },
            {
                "trackId": "Reserved for future use."
            },
            {
                "renderTimeMs": "The Unix timestamp (ms) when the video frame is rendered. This timestamp can be used to guide the rendering of the video frame. It is required."
            },
            {
                "uid": "The user ID to push the the external encoded video frame."
            },
            {
                "streamType": "The type of video streams."
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "class_beautyoptions",
        "name": "BeautyOptions",
        "description": "Image enhancement options.\n",
        "parameters": [
            {
                "lighteningContrastLevel": "The contrast level, used with the lighteningLevel parameter. The larger the value, the greater the contrast between light and dark. See LighteningContrastLevel .\n "
            },
            {
                "lighteningLevel": "The brightness level. The value ranges from 0.0 (original) to 1.0. The default value is 0.0. The greater the value, the greater the degree of whitening.\n "
            },
            {
                "smoothnessLevel": "The value ranges from 0.0 (original) to 1.0. The default value is 0.0. The greater the value, the greater the degree of skin grinding.\n "
            },
            {
                "rednessLevel": "The redness level. The value ranges from 0.0 (original) to 1.0. The default value is 0.0. The larger the value, the greater the rosy degree.\n "
            },
            {
                "sharpnessLevel": "The sharpness level. The value ranges from 0.0 (original) to 1.0. The default value is 0.0. The larger the value, the greater the sharpening degree.\n "
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "api_muteremotevideostream",
        "name": "muteRemoteVideoStream",
        "description": "Cancels or resumes subscribing to the specified remote user's video stream.\nCall this method after joining a channel.",
        "parameters": [
            {
                "uid": "The user ID of the specified user."
            },
            {
                "mute": "Whether to subscribe to the specified remote user's video stream.\n true: Unsubscribe from the specified user's video stream.\n false: (Default) Subscribes to the specified user's video stream. "
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "api_iscamerafacedetectsupported",
        "name": "isCameraFaceDetectSupported",
        "description": "Checks whether the device camera supports face detection.\nThis method is for Android only.",
        "parameters": [],
        "returns": "true: The device camera supports face detection.\n false: The device camera does not support face detection.",
        "is_hide": false
    },
    {
        "id": "api_irtcengineex_muteremotevideostreamex",
        "name": "muteRemoteVideoStreamEx",
        "description": "Stops or resumes receiving the video stream of a specified user.\nThis method is used to stops or resumes receiving the video stream of a specified user. You can call this method before or after joining a channel. If a user leaves a channel, the settings in this method become invalid.",
        "parameters": [
            {
                "connection": "The connection information. See RtcConnection ."
            },
            {
                "uid": "The user ID of the remote user.\n "
            },
            {
                "mute": "Whether to stop receiving the video stream of the specified user:\n true: Stop receiving the video stream of the specified user.\n false: (Default) Resume receiving the video stream of the specified user. "
            }
        ],
        "returns": "0: Success.\n < 0: Failure.",
        "is_hide": false
    },
    {
        "id": "callback_onsnapshottaken",
        "name": "onSnapshotTaken",
        "description": "Reports the result of taking a video snapshot.\n成功调用 takeSnapshot 后，SDK 触发该回调报告截图是否成功和获取截图的详情。",
        "parameters": [
            {
                "connection": "The connection information. See RtcConnection ."
            },
            {
                "filePath": "截图的本地保存路径。"
            },
            {
                "width": "图片宽度（px）。"
            },
            {
                "height": "图片高度（px）。"
            },
            {
                "errCode": "截图成功的提示或失败的原因。 \n 0：截图成功。\n < 0: 截图失败。 \n -1：写入文件失败或 JPEG 编码失败。\n -2：takeSnapshot 方法调用成功后 1 秒内没有发现指定用户的视频流。 "
            }
        ],
        "returns": "",
        "is_hide": true
    },
    {
        "id": "api_ibasespatialaudioengine_setaudiorecvrange",
        "name": "setAudioRecvRange",
        "description": "Sets the audio reception range of the local user.\nAfter the setting is successful, the local user can only hear the remote users within the setting range or belonging to the same team. You can call this method at any time to update the audio reception range.\n Agora recommends calling this method before enterRoom .",
        "parameters": [
            {
                "range": "The maximum audio reception range. The unit is meters. The value must be greater than 0."
            }
        ],
        "returns": "",
        "is_hide": true
    },
    {
        "id": "callback_ivideoframeobserver_getmirrorapplied",
        "name": "getMirrorApplied",
        "description": "Occurs each time the SDK receives a video frame and prompts you whether or not to mirror the captured video.\nIf the video data you want to obtain is a mirror image of the original video, you need to register this callback when calling registerVideoFrameObserver . After you successfully register the video frame observer, the SDK triggers this callback each time it receives a video frame. You need to set whether or not to mirror the video frame in the return value of this callback.\n This function only supports video data in RGBA and YUV420 formats.",
        "parameters": [],
        "returns": "Sets whether or not to mirror the captured video:\n true: Mirror the captured video.\n false: (Default) Do not mirror the captured video.",
        "is_hide": true
    },
    {
        "id": "api_queryinterface",
        "name": "queryInterface",
        "description": "Gets the pointer to the specified interface.\n",
        "parameters": [
            {
                "iid": "The ID of the interface. See InterfaceIdType ."
            },
            {
                "inter": "Output parameter. The pointer to the specified interface."
            }
        ],
        "returns": "0: Success.\n < 0: Failure.",
        "is_hide": true
    },
    {
        "id": "class_ivideoencodedimagereceiver",
        "name": "IVideoEncodedImageReceiver",
        "description": "Receives encoded video images.\n",
        "parameters": [],
        "returns": "",
        "is_hide": true
    },
    {
        "id": "enum_audiosessionoperationrestriction",
        "name": "AudioSessionOperationRestriction",
        "description": "The operation permissions of the SDK on the audio session.\n",
        "parameters": [
            {
                "audioSessionOperationRestrictionNone": "No restriction, the SDK can change the audio session."
            },
            {
                "audioSessionOperationRestrictionSetCategory": "The SDK cannot change the audio session category."
            },
            {
                "audioSessionOperationRestrictionConfigureSession": "The SDK cannot change the audio session category, mode, or categoryOptions."
            },
            {
                "audioSessionOperationRestrictionDeactivateSession": "The SDK keeps the audio session active when the user leaves the channel, for example, to play an audio file in the background."
            },
            {
                "audioSessionOperationRestrictionAll": "Completely restricts the operation permissions of the SDK on the audio session; the SDK cannot change the audio session."
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "callback_onlocalvideostats",
        "name": "onLocalVideoStats",
        "description": "Reports the statistics of the local video stream.\nThe SDK triggers this callback once every two seconds to report the statistics of the local video stream.",
        "parameters": [
            {
                "connection": "The connection information. See RtcConnection ."
            },
            {
                "stats": "The statistics of the local video stream. See LocalVideoStats ."
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "class_rtcengineext",
        "name": "RtcEngineExt",
        "description": "The derived interface class from RtcEngine.\n",
        "parameters": [],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "enum_videocodectypeforstream",
        "name": "VideoCodecTypeForStream",
        "description": "The codec type of the output video.\n",
        "parameters": [
            {
                "videoCodecH264ForStream": "1: (Default) H.264."
            },
            {
                "videoCodecH265ForStream": "2: H.265."
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "api_enableaudiospectrummonitor",
        "name": "enableAudioSpectrumMonitor",
        "description": "Turn on audio spectrum monitoring.\nIf you want to obtain the audio spectrum data of local or remote users, please register the audio spectrum observer and enable audio spectrum monitoring. You can call this method either before or after joining a channel.",
        "parameters": [
            {
                "null": "The interval (in milliseconds)at which the SDK triggers the onLocalAudioSpectrum and onRemoteAudioSpectrum callbacks. The default value is 100. Do not set this parameter to less than 10 milliseconds, otherwise the callback will not be triggered.\n "
            }
        ],
        "returns": "0: Success.\n < 0: Failure.\n -2 (ERR_INVALID_ARGUMENT): The parameter is invalid.",
        "is_hide": true
    },
    {
        "id": "api_removeinjectstreamurl",
        "name": "removeInjectStreamUrl",
        "description": "Removes the external media stream URL address from the live streaming.\nAfter a successful method, the SDK triggers the onUserOffline callback with the uid of 666.",
        "parameters": [
            {
                "url": "The URL address of the injected stream to be removed."
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "api_enabledualstreammode",
        "name": "enableDualStreamMode [1/3]",
        "description": "Enables/Disables dual-stream mode.\nSets the stream mode to the single-stream (default) or dual-stream mode. (LIVE_BROADCASTING only.) You can call this method to enable or disable the dual-stream mode on the publisher side.\n Dual streams are a hybrid of a high-quality video stream and a low-quality video stream:\n High-quality video stream: High bitrate, high resolution.\n Low-quality video stream: Low bitrate, low resolution. After you enable the dual-stream mode, you can call setRemoteVideoStreamType to choose toreceive the high-quality video stream or low-quality video stream on the subscriber side.",
        "parameters": [
            {
                "enabled": "Whether to enable dual-stream mode.\n true: Enable dual-stream mode.\n false: Disable dual-stream mode. "
            }
        ],
        "returns": "",
        "is_hide": true
    },
    {
        "id": "api_enableloopbackrecording_ng",
        "name": "enableLoopbackRecording",
        "description": "Enables loopback audio capture.\nIf you enable loopback audio capture, the output of the sound card is mixed into the audio stream sent to the other end. Applies to the macOS and Windows platforms only.\n macOS does not support loopback audio capture of the default sound card. If you need to use this method, use a virtual sound card and pass its name to the deviceName parameter. Agora recommends that you use Soundflower for loopback audio capture.\n You can call this method either before or after joining a channel.",
        "parameters": [
            {
                "enabled": "Whether to enable loopback audio capture.\n true: Enable loopback audio capture.\n false: (Default) Disable loopback audio capture.\n "
            },
            {
                "deviceName": "macOS: The device name of the virtual sound card. The default is set to null, which means the SDK uses Soundflower for loopback audio capture.\n Windows: The device name of the sound card. The default is set to null, which means the SDK uses the sound card of your device for loopback audio capture.\n "
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "callback_imediaplayersourceobserver_onaudiovolumeindication",
        "name": "onAudioVolumeIndication",
        "description": "Reports the volume of the media player.\nThe SDK triggers this callback every 200 milliseconds to report the current volume of the media player.",
        "parameters": [
            {
                "volume": "The volume of the media player. The value ranges from 0 to 255."
            }
        ],
        "returns": "",
        "is_hide": true
    },
    {
        "id": "callback_onremotevideostatechanged",
        "name": "onRemoteVideoStateChanged",
        "description": "Occurs when the remote video stream state changes.\nThis callback can be inaccurate when the number of users (in the communication profile) or hosts (in the live broadcasting profile) in a channel exceeds 17.",
        "parameters": [
            {
                "uid": "The ID of the remote user whose video state changes."
            },
            {
                "state": "The state of the remote video, see RemoteVideoState .\n "
            },
            {
                "reason": "The reason for the remote video state change, see RemoteVideoStateReason .\n "
            },
            {
                "elapsed": "Time elapsed (ms) from the local user callingjoinChannelWithOptions the method until the SDK triggers this callback."
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "api_setmixedaudioframeparameters_ng",
        "name": "setMixedAudioFrameParameters",
        "description": "Sets the audio data format reported by onMixedAudioFrame .\n",
        "parameters": [
            {
                "sampleRate": "The sample rate (Hz) of the audio data, which can be set as 8000, 16000, 32000, 44100, or 48000.\n "
            },
            {
                "channel": "The number of channels of the audio data, which can be set as 1 (Mono) or 2 (Stereo).\n "
            },
            {
                "samplesPerCall": "Sets the number of samples. In media push scenarios, set it as 1024.\n "
            }
        ],
        "returns": "",
        "is_hide": true
    },
    {
        "id": "api_videoviewcontroller_remote",
        "name": "remote",
        "description": "Constructor for the VideoViewController class used to render remote video.\n",
        "parameters": [
            {
                "useAndroidSurfaceView": "Whether to use Android SurfaceView to render video:\n true: Use Android SurfaceView to render video.\n false: Do not use Android SurfaceView to render video. Android SurfaceView applies to Android platform only."
            },
            {
                "useFlutterTexture": "Whether to use FlutterTexture to render video:\n true: Use FlutterTexture to render video.\n false: Do not use FlutterTexture to render video. FlutterTexture applies to iOS, macOS and Windows platforms."
            },
            {
                "connection": "The connection information. See RtcConnection ."
            },
            {
                "canvas": "Local video display properties. See VideoCanvas ."
            },
            {
                "rtcEngine": " RtcEngine ."
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "api_videoviewcontroller",
        "name": "VideoViewController",
        "description": "Constructor for the VideoViewController class used to render local video.\n",
        "parameters": [
            {
                "useAndroidSurfaceView": "Whether to use Android SurfaceView to render video:\n true: Use Android SurfaceView to render video.\n false: Do not use Android SurfaceView to render video. Android SurfaceView applies to Android platform only."
            },
            {
                "useFlutterTexture": "Whether to use FlutterTexture to render video:\n true: Use FlutterTexture to render video.\n false: Do not use FlutterTexture to render video. FlutterTexture applies to iOS, macOS and Windows platforms."
            },
            {
                "canvas": "Local video display properties. See VideoCanvas ."
            },
            {
                "rtcEngine": " RtcEngine ."
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "api_setchannelprofile",
        "name": "setChannelProfile",
        "description": "Sets the channel profile.\nTo ensure the quality of real-time communication, Agora recommends that all users in a channel use the same channel profile.\n This method must be called and set before joinChannelWithOptions, and cannot be set again after joining the channel.",
        "parameters": [
            {
                "profile": "The channel profile. See ChannelProfileType .\n "
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "enum_videoprofiletype",
        "name": "VideoProfileType",
        "description": "The video profile.\n",
        "parameters": [
            {
                "videoProfileLandscape120p": "0: 160 × 120, frame rate 15 fps, bitrate 65 Kbps."
            },
            {
                "videoProfileLandscape120p3": "2: 120 × 120, frame rate 15 fps, bitrate 50 Kbps."
            },
            {
                "videoProfileLandscape180p": "10: 320 × 180, frame rate 15 fps, bitrate 140 Kbps."
            },
            {
                "videoProfileLandscape180p3": "12: 180 × 180, frame rate 15 fps, bitrate 100 Kbps."
            },
            {
                "videoProfileLandscape180p4": "13: 240 × 180, frame rate 15 fps, bitrate 120 Kbps."
            },
            {
                "videoProfileLandscape240p": "20: 320 × 240, frame rate 15 fps, bitrate 200 Kbps."
            },
            {
                "videoProfileLandscape240p3": "22: 240 × 240, frame rate 15 fps, bitrate 140 Kbps."
            },
            {
                "videoProfileLandscape240p4": "23: 424 × 240, frame rate 15 fps, bitrate 220 Kbps."
            },
            {
                "videoProfileLandscape360p": "30: 640 × 360, frame rate 15 fps, bitrate 400 Kbps."
            },
            {
                "videoProfileLandscape360p3": "32: 360 × 360, frame rate 15 fps, bitrate 260 Kbps."
            },
            {
                "videoProfileLandscape360p4": "33: 640 × 360, frame rate 30 fps, bitrate 600 Kbps."
            },
            {
                "videoProfileLandscape360p6": "35: 360 × 360, frame rate 30 fps, bitrate 400 Kbps."
            },
            {
                "videoProfileLandscape360p7": "36: 480 × 360, frame rate 15 fps, bitrate 320 Kbps."
            },
            {
                "videoProfileLandscape360p8": "37: 480 × 360, frame rate 30 fps, bitrate 490 Kbps."
            },
            {
                "videoProfileLandscape360p9": "38: 640 × 360, frame rate 15 fps, bitrate 800 Kbps.\n This profile applies only to the live streaming channel profile. "
            },
            {
                "videoProfileLandscape360p10": "39: 640 × 360, frame rate 24 fps, bitrate 800 Kbps.\n This profile applies only to the live streaming channel profile."
            },
            {
                "videoProfileLandscape360p11": "100: 640 × 360, frame rate 24 fps, bitrate 1000 Kbps.\n This profile applies only to the live streaming channel profile."
            },
            {
                "videoProfileLandscape480p": "40: 640 × 480, frame rate 15 fps, bitrate 500 Kbps."
            },
            {
                "videoProfileLandscape480p3": "42: 480 × 480, frame rate 15 fps, bitrate 400 Kbps."
            },
            {
                "videoProfileLandscape480p4": "43: 640 × 480, frame rate 30 fps, bitrate 750 Kbps."
            },
            {
                "videoProfileLandscape480p6": "45: 480 × 480, frame rate 30 fps, bitrate 600 Kbps."
            },
            {
                "videoProfileLandscape480p8": "47: 848 × 480, frame rate 15 fps, bitrate 610 Kbps."
            },
            {
                "videoProfileLandscape480p9": "48: 848 × 480, frame rate 30 fps, bitrate 930 Kbps."
            },
            {
                "videoProfileLandscape480p10": "49: 640 × 480, frame rate 10 fps, bitrate 400 Kbps."
            },
            {
                "videoProfileLandscape720p": "50: 1280 × 720, frame rate 15 fps, bitrate 1130 Kbps."
            },
            {
                "videoProfileLandscape720p3": "52: 1280 × 720, frame rate 30 fps, bitrate 1710 Kbps."
            },
            {
                "videoProfileLandscape720p5": "54: 960 × 720, frame rate 15 fps, bitrate 910 Kbps."
            },
            {
                "videoProfileLandscape720p6": "55: 960 × 720, frame rate 30 fps, bitrate 1380 Kbps."
            },
            {
                "videoProfileLandscape1080p": "60: 1920 × 1080, frame rate 15 fps, bitrate 2080 Kbps."
            },
            {
                "videoProfileLandscape1080p3": "60: 1920 × 1080, frame rate 30 fps, bitrate 3150 Kbps."
            },
            {
                "videoProfileLandscape1080p5": "64: 1920 × 1080, frame rate 60 fps, bitrate 4780 Kbps."
            },
            {
                "VIDEO_PROFILE_PORTRAIT_120P": "1000: 120 × 160, frame rate 15 fps, bitrate 65 Kbps."
            },
            {
                "VIDEO_PROFILE_PORTRAIT_120P3": "1002: 120 × 120, frame rate 15 fps, bitrate 50 Kbps."
            },
            {
                "videoProfilePortrait180p": "1010: 180 × 320, frame rate 15 fps, bitrate 140 Kbps."
            },
            {
                "videoProfilePortrait180p3": "1012: 180 × 180, frame rate 15 fps, bitrate 100 Kbps."
            },
            {
                "videoProfilePortrait180p4": "1013: 180 × 240, frame rate 15 fps, bitrate 120 Kbps."
            },
            {
                "videoProfilePortrait240p": "1020: 240 × 320, frame rate 15 fps, bitrate 200 Kbps."
            },
            {
                "videoProfilePortrait240p3": "1022: 240 × 240, frame rate 15 fps, bitrate 140 Kbps."
            },
            {
                "videoProfilePortrait240p4": "1023: 240 × 424, frame rate 15 fps, bitrate 220 Kbps."
            },
            {
                "videoProfilePortrait360p": "1030: 360 × 640, frame rate 15 fps, bitrate 400 Kbps."
            },
            {
                "videoProfilePortrait360p3": "1032: 360 × 360, frame rate 15 fps, bitrate 260 Kbps."
            },
            {
                "videoProfilePortrait360p4": "1033: 360 × 640, frame rate 15 fps, bitrate 600 Kbps."
            },
            {
                "videoProfilePortrait360p6": "1035: 360 × 360, frame rate 30 fps, bitrate 400 Kbps."
            },
            {
                "videoProfilePortrait360p7": "1036: 360 × 480, frame rate 15 fps, bitrate 320 Kbps."
            },
            {
                "videoProfilePortrait360p8": "1037: 360 × 480, frame rate 30 fps, bitrate 490 Kbps."
            },
            {
                "videoProfilePortrait360p9": "1038: 360 × 640, frame rate 15 fps, bitrate 800 Kbps.\n This profile applies only to the live streaming channel profile. "
            },
            {
                "videoProfilePortrait360p10": "1039: 360 × 640, frame rate 24 fps, bitrate 800 Kbps.\n This profile applies only to the live streaming channel profile. "
            },
            {
                "videoProfilePortrait360p11": "1100: 360 × 640, frame rate 24 fps, bitrate 1000 Kbps.\n This profile applies only to the live streaming channel profile. "
            },
            {
                "videoProfilePortrait480p": "1040: 480 × 640, frame rate 15 fps, bitrate 500 Kbps."
            },
            {
                "videoProfilePortrait480p3": "1042: 480 × 480, frame rate 15 fps, bitrate 400 Kbps."
            },
            {
                "videoProfilePortrait480p4": "1043: 480 × 640, frame rate 30 fps, bitrate 750 Kbps."
            },
            {
                "videoProfilePortrait480p6": "1045: 480 × 480, frame rate 30 fps, bitrate 600 Kbps."
            },
            {
                "videoProfilePortrait480p8": "1047: 480 × 848, frame rate 15 fps, bitrate 610 Kbps."
            },
            {
                "videoProfilePortrait480p9": "1048: 480 × 848, frame rate 30 fps, bitrate 930 Kbps."
            },
            {
                "videoProfilePortrait480p10": "1049: 480 × 640, frame rate 10 fps, bitrate 400 Kbps."
            },
            {
                "videoProfilePortrait720p": "1050: 720 × 1280, frame rate 15 fps, bitrate 1130 Kbps."
            },
            {
                "videoProfilePortrait720p3": "1052: 720 × 1280, frame rate 30 fps, bitrate 1710 Kbps."
            },
            {
                "videoProfilePortrait720p5": "1054: 720 × 960, frame rate 15 fps, bitrate 910 Kbps."
            },
            {
                "videoProfilePortrait720p6": "1055: 720 × 960, frame rate 30 fps, bitrate 1380 Kbps."
            },
            {
                "videoProfilePortrait1080p": "1060: 1080 × 1920, frame rate 15 fps, bitrate 2080 Kbps."
            },
            {
                "videoProfilePortrait1080p3": "1062: 1080 × 1920, frame rate 30 fps, bitrate 3150 Kbps."
            },
            {
                "videoProfilePortrait1080p5": "1064: 1080 × 1920, frame rate 60 fps, bitrate 4780 Kbps."
            },
            {
                "videoProfileDefault": "(Default) 640 × 360, frame rate 15 fps, bitrate 400 Kbps."
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "api_createdatastream",
        "name": "createDataStream [1/2]",
        "description": "Creates a data stream.\nEach user can create up to five data streams during the lifecycle of RtcEngine . Call this method after joining a channel.\n Agora does not support setting reliable as true and ordered as true.",
        "parameters": [
            {
                "reliable": "Whether or not the data stream is reliable:\n true: The recipients receive the data from the sender within five seconds. If the recipient does not receive the data within five seconds, the SDK triggers the onStreamMessageError callback and returns an error code.\n false: There is no guarantee that the recipients receive the data stream within five seconds and no error message is reported for any delay or missing data stream. "
            },
            {
                "ordered": "Whether or not the recipients receive the data stream in the sent order:\n true: The recipients receive the data in the sent order.\n false: The recipients do not receive the data in the sent order. "
            }
        ],
        "returns": "",
        "is_hide": true
    },
    {
        "id": "api_iaudiodevicemanager_getrecordingdevice",
        "name": "getRecordingDevice",
        "description": "Gets the current audio recording device.\n",
        "parameters": [],
        "returns": "The current audio recording device.",
        "is_hide": false
    },
    {
        "id": "api_setaudioprofile",
        "name": "setAudioProfile",
        "description": "Sets the audio profile and audio scenario.\nYou can call this method either before or after joining a channel.\n In scenarios requiring high-quality audio, such as online music tutoring, Agora recommends you set profile as audioProfileMusicHighQuality(4), and scenario as audioScenarioGameStreaming(3) or audioScenarioHighDefinition(6).",
        "parameters": [
            {
                "profile": "The audio profile, including the sampling rate, bitrate, encoding mode, and the number of channels. See AudioProfileType .\n "
            },
            {
                "scenario": "The audio scenario. Under different audio scenarios, the device uses different volume types."
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "class_audiodeviceinfo",
        "name": "AudioDeviceInfo",
        "description": "The AudioDeviceInfo class that contains the ID and device name of the audio devices.\n",
        "parameters": [
            {
                "deviceName": "The device name."
            },
            {
                "deviceId": "The device ID."
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "class_channelmediainfo",
        "name": "ChannelMediaInfo",
        "description": "The definition of ChannelMediaInfo.\n",
        "parameters": [
            {
                "channelName": "The channel name."
            },
            {
                "token": "The token that enables the user to join the channel."
            },
            {
                "uid": "The user ID."
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "api_irtcengineex_joinchannelex",
        "name": "joinChannelEx",
        "description": "Joins a channel with the connection ID.\nYou can call this method multiple times to join more than one channels. If you are already in a channel, you cannot rejoin it with the same user ID.\n If you want to join the same channel from different devices, ensure that the user IDs in all devices are different.\n Ensure that the app ID you use to generate the token is the same with the app ID used when creating the RtcEngine instance.",
        "parameters": [
            {
                "options": "The channel media options. See ChannelMediaOptions .\n "
            },
            {
                "token": "The token generated on your server for authentication. See \n "
            },
            {
                "connection": ""
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "api_registerlocaluseraccount",
        "name": "registerLocalUserAccount",
        "description": "Registers a user account.\nOnce registered, the user account can be used to identify the local user when the user joins the channel. After the registration is successful, the user account can identify the identity of the local user, and the user can use it to join the channel.\n After the user successfully registers a user account, the SDK triggers the onLocalUserRegistered callback on the local client, reporting the user ID and user account of the local user.\n This method is optional. To join a channel with a user account, you can choose either of the following ways:\n Call registerLocalUserAccount to create a user account, and then call joinChannelWithUserAccount to join the channel.\n Call the joinChannelWithUserAccount method to join the channel. The difference between the two ways is that the time elapsed between calling the registerLocalUserAccount method and joining the channel is shorter than directly calling joinChannelWithUserAccount. Ensure that you set the userAccount parameter; otherwise, this method does not take effect.\n Ensure that the userAccount is unique in the channel.\n To ensure smooth communication, use the same parameter type to identify the user. For example, if a user joins the channel with a user ID, then ensure all the other users use the user ID too. The same applies to the user account. If a user joins the channel with the Agora Web SDK, ensure that the ID of the user is set to the same parameter type.",
        "parameters": [
            {
                "appId": "The App ID of your project on Agora Console."
            },
            {
                "userAccount": "The user account. This parameter is used to identify the user in the channel for real-time audio and video engagement. You need to set and manage user accounts yourself and ensure that each user account in the same channel is unique. The maximum length of this parameter is 255 bytes. Ensure that you set this parameter and do not set it as NULL. Supported characters are (89 in total):\n The 26 lowercase English letters: a to z.\n The 26 uppercase English letters: A to Z.\n All numeric characters: 0 to 9.\n Space\n \"!\", \"#\", \"$\", \"%\", \"&amp;\", \"(\", \")\", \"+\", \"-\", \":\", \";\", \"&lt;\", \"= \", \".\", \"&gt;\", \"?\", \"@\", \"[\", \"]\", \"^\", \"_\", \"{\", \"}\", \"|\", \"~\", \",\" "
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "api_resumeaudio",
        "name": "resumeAudio",
        "description": "",
        "parameters": [
            {
                "null": ""
            }
        ],
        "returns": "",
        "is_hide": true
    },
    {
        "id": "api_imediaplayer_resume",
        "name": "resume",
        "description": "Resumes playing the media file.\n",
        "parameters": [],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "enum_capturebrightnessleveltype",
        "name": "CAPTURE_BRIGHTNESS_LEVEL_TYPE",
        "description": "The brightness level of the video image captured by the local camera.\n",
        "parameters": [
            {
                "CAPTURE_BRIGHTNESS_LEVEL_INVALID": "-1: The SDK does not detect the brightness level of the video image. Wait a few seconds to get the brightness level from captureBrightnessLevel in the next callback."
            },
            {
                "CAPTURE_BRIGHTNESS_LEVEL_NORMAL": "0: The brightness level of the video image is normal."
            },
            {
                "CAPTURE_BRIGHTNESS_LEVEL_BRIGHT": "1: The brightness level of the video image is too bright."
            },
            {
                "CAPTURE_BRIGHTNESS_LEVEL_DARK": "2: The brightness level of the video image is too dark."
            }
        ],
        "returns": "",
        "is_hide": true
    },
    {
        "id": "api_ilocalspatialaudioengine_removeremoteposition",
        "name": "removeRemotePosition",
        "description": "Removes the spatial position of the specified remote user.\nAfter successfully calling this method, the local user no longer hears the specified remote user.\n After leaving the channel, to avoid wasting resources, you can also call this method to delete the spatial position of the specified remote user.",
        "parameters": [
            {
                "uid": "The user ID. This parameter must be the same as the user ID passed in when the user joined the channel."
            }
        ],
        "returns": "",
        "is_hide": true
    },
    {
        "id": "api_ibasespatialaudioengine_muteallremoteaudiostreams",
        "name": "muteAllRemoteAudioStreams",
        "description": "Stops or resumes subscribing to the audio streams of all remote users.\nAfter successfully calling this method, the local user stops or resumes subscribing to the audio streams of all remote users, including all subsequent users. Call this method after joinChannelWithOptions .\n When using the spatial audio effect, if you need to set whether to stop subscribing to the audio streams of all remote users, Agora recommends calling this method instead of the muteAllRemoteAudioStreams method under RtcEngine .",
        "parameters": [
            {
                "mute": "Whether to stop subscribing to the audio streams of all remote users:\n true: Stop subscribing to the audio streams of all remote users.\n false: Subscribe to the audio streams of all remote users. "
            }
        ],
        "returns": "",
        "is_hide": true
    },
    {
        "id": "class_audioframe",
        "name": "AudioFrame",
        "description": " AudioFrame \n",
        "parameters": [
            {
                "samplesPerChannel": "The number of samples per channel in the audio frame."
            },
            {
                "bytesPerSample": "The number of bytes per audio sample, which is usually 16-bit (2 bytes)."
            },
            {
                "null": ""
            }
        ],
        "returns": "",
        "is_hide": true
    },
    {
        "id": "api_startaudioframedump",
        "name": "startAudioFrameDump",
        "description": "",
        "parameters": [
            {
                "null": ""
            }
        ],
        "returns": "",
        "is_hide": true
    },
    {
        "id": "api_imediaplayer_getmediaplayerid",
        "name": "getMediaPlayerId",
        "description": "Gets the ID of the media player.\n",
        "parameters": [],
        "returns": "≥ 0: Success. The ID of the media player.\n < 0: Failure.",
        "is_hide": false
    },
    {
        "id": "api_enablespatialaudio",
        "name": "enableSpatialAudio",
        "description": "Enables or disables the spatial audio effect.\nAfter enabling the spatial audio effect, you can call setRemoteUserSpatialAudioParams to set the spatial audio effect parameters of the remote user.You can call this method either before or after joining a channel.",
        "parameters": [
            {
                "enabled": "Whether to enable the spatial audio effect:\n true: Enable the spatial sound effect.\n false: Disable the spatial sound effect.\n "
            }
        ],
        "returns": "",
        "is_hide": true
    },
    {
        "id": "api_enableencryption",
        "name": "enableEncryption",
        "description": "Enables/Disables the built-in encryption.\nIn scenarios requiring high security, Agora recommends calling this method to enable the built-in encryption before joining a channel.\n All users in the same channel must use the same encryption mode and encryption key. After the user leaves the channel, the SDK automatically disables the built-in encryption. To enable the built-in encryption, call this method before the user joins the channel again.\n If you enable the built-in encryption, you cannot use the media push function.",
        "parameters": [
            {
                "enabled": "Whether to enable built-in encryption:\n true: Enable the built-in encryption.\n false: Disable the built-in encryption. "
            },
            {
                "config": "Built-in encryption configurations. See EncryptionConfig ."
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "api_imediaengine_registervideoencodedimagereceiver",
        "name": "registerVideoEncodedImageReceiver",
        "description": "Registers a receiver object for the encoded video image.\nCall this method after joining a channel.\n If you register an IVideoEncodedImageReceiver object, you cannot register an IVideoFrameObserver object.",
        "parameters": [
            {}
        ],
        "returns": "",
        "is_hide": true
    },
    {
        "id": "api_imediaplayer_renewagoracdnsrctoken",
        "name": "renewAgoraCDNSrcToken",
        "description": "Renew the authentication information for the URL of the media resource to be played.\nWhen the authentication information expires (exceeds the ts field), you can call the openWithAgoraCDNSrc method to reopen the media resource or the switchAgoraCDNSrc method to switch the media resource, and then pass in the authenticated URL (with the ts field updated) of the media resource.\n If your authentication information expires when you call the ( switchAgoraCDNLineByIndex ) to switch the CDN route for playing the media resource, you need to call this method to pass in the updated authentication information to update the authentication information of the media resource URL. After updating the authentication information, you need to call switchAgoraCDNLineByIndex to complete the route switching. To avoid frequent expiration of authentication information, ensure that you set the ts field appropriately or according to the scenario requirements.",
        "parameters": [
            {
                "token": "The authentication field. See the sign field of the authentication information."
            },
            {
                "ts": "The timestamp when the authentication information expires. See the ts field of the authentication information."
            }
        ],
        "returns": "",
        "is_hide": true
    },
    {
        "id": "api_imediaplayer_seek",
        "name": "seek",
        "description": "Seeks to a new playback position.\nAfter successfully calling this method, you will receive the onPlayerEvent callback, reporting the result of the seek operation to the new playback position.\n To play the media file from a specific position, do the following:\n Call this method to seek to the position you want to begin playback.\n Call the play method to play the media file.",
        "parameters": [
            {
                "newPos": "The new playback position (ms)."
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "class_playerstreaminfo",
        "name": "PlayerStreamInfo",
        "description": "The detailed information of the media stream.\n",
        "parameters": [
            {
                "streamIndex": "The index of the media stream."
            },
            {
                "streamType": "The type of the media stream. See MediaStreamType ."
            },
            {
                "codecName": "The codec of the media stream."
            },
            {
                "language": "The language of the media stream."
            },
            {
                "videoFrameRate": "This parameter only takes effect for video streams, and indicates the video frame rate (fps)."
            },
            {
                "videoBitrate": "This parameter only takes effect for video streams, and indicates the video bitrate (bps)."
            },
            {
                "videoWidth": "This parameter only takes effect for video streams, and indicates the video width (pixel)."
            },
            {
                "videoHeight": "This parameter only takes effect for video streams, and indicates the video height (pixel)."
            },
            {
                "videoRotation": "This parameter only takes effect for video streams, and indicates the video rotation angle."
            },
            {
                "audioSampleRate": "This parameter only takes effect for audio streams, and indicates the audio sample rate (Hz)."
            },
            {
                "audioChannels": "This parameter only takes effect for audio streams, and indicates the audio channel number."
            },
            {
                "audioBitsPerSample": "This parameter only takes effect for audio streams, and indicates the bit number of each audio sample."
            },
            {
                "duration": "The total duration (s) of the media stream."
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "enum_uploaderrorreason",
        "name": "UploadErrorReason",
        "description": "The reason for the upload failure.\n",
        "parameters": [
            {
                "uploadSuccess": "0: Successfully upload the log files."
            },
            {
                "uploadNetError": "1: Network error. Check the network connection and call uploadLogFile to retransmit."
            },
            {
                "uploadServerError": "2: An error occurs in the Agora server. Try uploading the log files later."
            }
        ],
        "returns": "",
        "is_hide": true
    },
    {
        "id": "api_stopechotest",
        "name": "stopEchoTest",
        "description": "Stops the audio call test.\n",
        "parameters": [],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "callback_onrejoinchannelsuccess",
        "name": "onRejoinChannelSuccess",
        "description": "Occurs when a user rejoins the channel.\nWhen a user loses connection with the server because of network problems, the SDK automatically tries to reconnect and triggers this callback upon reconnection.",
        "parameters": [
            {
                "elapsed": "The time elapsed (ms) from the local user trying to rejoin the channel until the SDK triggers this callback."
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "callback_icloudspatialaudioeventhandler_ontokenwillexpire",
        "name": "onTokenWillExpire",
        "description": "Occurs when the RTM token expires.\nOnce the RTM token expires, the SDK triggers this callback to notify the app to renew the RTM token.\n When you receive this callback, you need to generate a new token on your server and call renewToken to pass the new token to the SDK.",
        "parameters": [],
        "returns": "",
        "is_hide": true
    },
    {
        "id": "api_stopdirectcdnstreaming",
        "name": "stopDirectCdnStreaming",
        "description": "Stops pushing media streams to the CDN directly.\n",
        "parameters": [],
        "returns": "",
        "is_hide": true
    },
    {
        "id": "api_enableechocancellationexternal",
        "name": "enableEchoCancellationExternal",
        "description": "Turns custom echo cancellation on/off.\nAfter calling this method, you can push external audio frames to the custom audio module for echo cancellation.\n You need to call this method after calling setExternalAudioSink , but before joining the channel.",
        "parameters": [
            {
                "enabled": "Set whether to enable custom echo cancellation:\n true: Enable custom echo cancellation.\n false: Disable custom echo cancellation.\n "
            },
            {
                "audioSourceDelay": "Sets the time (in milliseconds) between pushing audio frames and publishing audio frames. The value range is [0, 100].\n To call pushCaptureAudioFrame1 , make sure this parameter is set to 0.\n To call pushReverseAudioFrame1 or process the audio frame captured by the sound card, please make sure that this parameter is an integer multiple of 10.\n "
            }
        ],
        "returns": "",
        "is_hide": true
    },
    {
        "id": "class_virtualbackgroundsource",
        "name": "VirtualBackgroundSource",
        "description": "The custom background image.\n",
        "parameters": [
            {
                "backgroundSourceType": "The type of the custom background image. See backgroundSourceType ."
            },
            {
                "color": "The color of the custom background image. The format is a hexadecimal integer defined by RGB, without the # sign, such as 0xFFB6C1 for light pink. The default value is 0xFFFFFF, which signifies white. The value range is [0x000000, 0xffffff]. If the value is invalid, the SDK replaces the original background image with a white background image.This parameter takes effect only when the type of the custom background image is backgroundColor."
            },
            {
                "source": ""
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "callback_imediaplayersourceobserver_onplayerevent",
        "name": "onPlayerEvent",
        "description": "Reports the playback event.\nAfter calling the seek method, the SDK triggers the callback to report the results of the seek operation.",
        "parameters": [
            {
                "eventCode": "The playback event. See MediaPlayerEvent ."
            },
            {
                "elapsedTime": "The time (ms) when the event occurs."
            },
            {
                "message": "Information about the event."
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "api_ibasespatialaudioengine_release",
        "name": "release",
        "description": "Destroys IBaseSpatialAudioEngine .\nThis method releases all resources under IBaseSpatialAudioEngine. When the user does not need to use the spatial audio effect, you can call this method to release resources for other operations.\n After calling this method, you can no longer use any of the APIs under IBaseSpatialAudioEngine.Call this method before the release method under RtcEngine .",
        "parameters": [],
        "returns": "",
        "is_hide": true
    },
    {
        "id": "api_iaudiodevicemanager_startplaybackdevicetest_ng",
        "name": "startPlaybackDeviceTest",
        "description": "Starts the audio playback device test.\nThis method tests whether the audio playback device works properly. Once a user starts the test, the SDK plays an audio file specified by the user. If the user can hear the audio, the playback device works properly.\n After calling this method, the SDK triggers the onAudioVolumeIndication callback every 100 ms, reporting uid = 1 and the volume information of the playback device.\n Ensure that you call this method before joining a channel.",
        "parameters": [
            {
                "testAudioFilePath": "The path of the audio file. The data format is string in UTF-8.\n Supported file formats: wav, mp3, m4a, and aac.\n Supported file sample rates: 8000, 16000, 32000, 44100, and 48000 Hz. "
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "enum_videoorientation",
        "name": "VideoOrientation",
        "description": "The clockwise rotation of the video.\n",
        "parameters": [
            {
                "videoOrientation0": "0: (Default) No rotation."
            },
            {
                "videoOrientation90": "90: 90 degrees."
            },
            {
                "videoOrientation180": "180: 180 degrees."
            },
            {
                "videoOrientation270": "270: 270 degrees."
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "enum_segmodeltype",
        "name": "SEG_MODEL_TYPE",
        "description": "The type of algorithms to user for background processing.\n",
        "parameters": [
            {
                "SEG_MODEL_AI": "1: (默认) 适用于所有场景下的背景处理算法。"
            },
            {
                "SEG_MODEL_GREEN": "2: 仅适用于绿幕背景下的背景处理算法。"
            }
        ],
        "returns": "",
        "is_hide": true
    },
    {
        "id": "enum_voicechangerpreset",
        "name": "VOICE_CHANGER_PRESET",
        "description": "Local voice changer options.\n",
        "parameters": [
            {
                "VOICE_CHANGER_OFF": "The original voice (no local voice change)."
            },
            {
                "VOICE_CHANGER_OLDMAN": "The voice of an old man."
            },
            {
                "VOICE_CHANGER_BABYBOY": "The voice of a little boy."
            },
            {
                "VOICE_CHANGER_BABYGIRL": "The voice of a little girl."
            },
            {
                "VOICE_CHANGER_ZHUBAJIE": "The voice of Zhu Bajie, a character in Journey to the West who has a voice like that of a growling bear."
            },
            {
                "VOICE_CHANGER_ETHEREAL": "The ethereal voice."
            },
            {
                "VOICE_CHANGER_HULK": "The voice of Hulk."
            },
            {
                "VOICE_BEAUTY_VIGOROUS": "A more vigorous voice."
            },
            {
                "VOICE_BEAUTY_DEEP": "A deeper voice."
            },
            {
                "VOICE_BEAUTY_MELLOW": "A mellower voice."
            },
            {
                "VOICE_BEAUTY_FALSETTO": "Falsetto."
            },
            {
                "VOICE_BEAUTY_FULL": "A fuller voice."
            },
            {
                "VOICE_BEAUTY_CLEAR": "A clearer voice."
            },
            {
                "VOICE_BEAUTY_RESOUNDING": "A more resounding voice."
            },
            {
                "VOICE_BEAUTY_RINGING": "A more ringing voice."
            },
            {
                "VOICE_BEAUTY_SPACIAL": "A more spatially resonant voice."
            },
            {
                "GENERAL_BEAUTY_VOICE_MALE_MAGNETIC": "(For male only) A more magnetic voice. Do not use it when the speaker is a male; otherwise, voice distortion occurs. Do not use it when the speaker is a female; otherwise, voice distortion occurs."
            },
            {
                "GENERAL_BEAUTY_VOICE_FEMALE_FRESH": "(For female only) A fresher voice. Do not use it when the speaker is a male; otherwise, voice distortion occurs. Do not use it when the speaker is a male; otherwise, voice distortion occurs."
            },
            {
                "GENERAL_BEAUTY_VOICE_FEMALE_VITALITY": "(For female only) A more vital voice. Do not use it when the speaker is a male; otherwise, voice distortion occurs. Do not use it when the speaker is a male; otherwise, voice distortion occurs."
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "callback_iaudioencodedframeobserver_onmixedaudioencodedframe",
        "name": "OnMixedAudioEncodedFrame",
        "description": "Gets the mixed and encoded audio data of the local and all remote users.\nAfteraudioEncodedFrameObserverPositionMixed calling registerAudioEncodedFrameObserver and setting the audio profile as , you can get the mixed and encoded audio data of the local and all remote users through this callback.",
        "parameters": [
            {
                "samplesPerSec": "Recording sample rate (Hz)."
            },
            {
                "channels": "The number of channels. 1: Mono.\n 2: Stereo. If the channel uses stereo, the data is interleaved. "
            },
            {
                "samplesPerChannel": ""
            }
        ],
        "returns": "",
        "is_hide": true
    },
    {
        "id": "api_updatertmptranscodingl",
        "name": "updateRtmpTranscoding",
        "description": "Updates the transcoding configuration.\nAfter you start pushing media streams to CDN with transcoding, you can dynamically update the transcoding configuration according to the scenario. The SDK triggers the onTranscodingUpdated callback after the transcoding configuration is updated.",
        "parameters": [
            {
                "transcoding": "The transcoding configuration for Media Push. See LiveTranscoding .\n "
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "class_playerupdatedinfo",
        "name": "PlayerUpdatedInfo",
        "description": "Information related to the media player.\n",
        "parameters": [
            {
                "playerId": "The ID of a media player."
            },
            {
                "deviceId": "The ID of a deivce."
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "enum_prioritytype",
        "name": "PRIORITY_TYPE",
        "description": "The priority of the remote user.\n",
        "parameters": [
            {
                "PRIORITY_HIGH": "The user's priority is high."
            },
            {
                "PRIORITY_NORMAL": "(Default) The user's priority is normal."
            }
        ],
        "returns": "",
        "is_hide": true
    },
    {
        "id": "api_setcamerafocuspositioninpreview",
        "name": "setCameraFocusPositionInPreview",
        "description": "Sets the camera manual focus position.\nThis method needs to be called after the camera is started (for example, by calling startPreview or joinChannelWithOptions ). After a successful method call, the SDK triggers the onCameraFocusAreaChanged callback.\n This method is for Android and iOS only.",
        "parameters": [
            {
                "positionX": "The horizontal coordinate of the touchpoint in the view."
            },
            {
                "positionY": "The vertical coordinate of the touchpoint in the view."
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "callback_onvideosizechanged",
        "name": "onVideoSizeChanged",
        "description": "Occurs when the video size or rotation of a specified user changes.\n",
        "parameters": [
            {
                "connection": "The connection information. See RtcConnection ."
            },
            {
                "uid": "The ID of the user whose video size or rotation changes. (The uid for the local user is 0. The video is the local user's video preview)."
            },
            {
                "width": "New width (pixels) of the video."
            },
            {
                "height": "New height (pixels) of the video."
            },
            {
                "rotation": "New rotation of the video [0 to 360)."
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "class_uplinknetworkinfo",
        "name": "UplinkNetworkInfo",
        "description": "The uplink network information.\n",
        "parameters": [
            {
                "video_encoder_target_bitrate_bps": "The target video encoder bitrate (bps)."
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "enum_injectstreamstatus",
        "name": "InjectStreamStatus",
        "description": "States of importing an external video stream in the interactive live streaming.\n",
        "parameters": [
            {
                "injectStreamStatusStartSuccess": "0: The external video stream is imported successfully."
            },
            {
                "injectStreamStatusStartAlreadyExists": "1: The external video stream already exists."
            },
            {
                "injectStreamStatusStartUnauthorized": "2: The external video stream to be imported is unauthorized."
            },
            {
                "injectStreamStatusStartTimedout": "3: A timeout occurs when importing the external video stream."
            },
            {
                "injectStreamStatusStartFailed": "4: The SDK fails to import the external video stream."
            },
            {
                "injectStreamStatusStopSuccess": "5: The SDK successfully stops importing the external video stream."
            },
            {
                "injectStreamStatusStopNotFound": "6: The external video stream to be stopped importing is not found."
            },
            {
                "injectStreamStatusStopUnauthorized": "7: The external video stream to be stopped importing is unauthorized."
            },
            {
                "injectStreamStatusStopTimedout": "8: A timeout occurs when stopping importing the external video stream."
            },
            {
                "injectStreamStatusStopFailed": "9: The SDK fails to stop importing the external video stream."
            },
            {
                "injectStreamStatusBroken": "10: The external video stream is corrupted."
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "callback_onclientrolechangefailed",
        "name": "onClientRoleChangeFailed",
        "description": "",
        "parameters": [
            {
                "null": ""
            }
        ],
        "returns": "",
        "is_hide": true
    },
    {
        "id": "callback_onstreammessage",
        "name": "onStreamMessage",
        "description": "Occurs when the local user receives the data stream from the remote user.\nThe SDK triggers this callback when the local user receives the stream message that the remote user sends by calling the sendStreamMessage method.",
        "parameters": [
            {
                "connection": "The connection information. See RtcConnection ."
            },
            {
                "uid": "The ID of the remote user sending the message."
            },
            {
                "streamId": "The stream ID of the received message."
            },
            {
                "data": "received data."
            },
            {
                "length": "The data length (byte)."
            },
            {
                "sentTs": "The time when the data stream is sent."
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "class_videoformat_ng",
        "name": "VideoFormat",
        "description": "The format of the video frame.\n",
        "parameters": [
            {
                "width": "The width (px) of the video frame."
            },
            {
                "height": "The height (px) of the video frame."
            },
            {
                "fps": "The video frame rate (fps)."
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "api_iscameraautofocusfacemodesupported",
        "name": "isCameraAutoFocusFaceModeSupported",
        "description": "Checks whether the device supports the face auto-focus function.\nThis method is for Android and iOS only.\n Call this method before calling joinChannelWithOptions , enableVideo , or enableLocalVideo , depending on which method you use to turn on your local camera.",
        "parameters": [],
        "returns": "true: The device supports the face auto-focus function.\n false: The device does not support the face auto-focus function.",
        "is_hide": false
    },
    {
        "id": "api_ivideodevicemanager_getdevice",
        "name": "getDevice",
        "description": "Retrieves the current video capture device.\n",
        "parameters": [],
        "returns": "The video capture device.",
        "is_hide": false
    },
    {
        "id": "callback_iaudiospectrumobserver_onremoteaudiospectrum",
        "name": "onRemoteAudioSpectrum",
        "description": "Gets the remote audio spectrum.\n成功调用 registerAudioSpectrumObserver 实现 IAudioSpectrumObserver 中的 onRemoteAudioSpectrum 回调并调用 enableAudioSpectrumMonitor 开启音频频谱监测后，SDK 会按照你设置的时间间隔触发该回调，报告接收到的远端音频数据的频谱。",
        "parameters": [
            {}
        ],
        "returns": "Whether you have received the spectrum data:\n true: Spectrum data is received.\n false: No spectrum data is received.",
        "is_hide": true
    },
    {
        "id": "api_addvideowatermark2",
        "name": "addVideoWatermark",
        "description": "Adds a watermark image to the local video.\nThis method adds a PNG watermark image to the local video in the live streaming. Once the watermark image is added, all the audience in the channel (CDN audience included), and the capturing device can see and capture it. Agora supports adding only one watermark image onto the local video, and the newly watermark image replaces the previous one.\n The watermark coordinatesare dependent on the settings in the setVideoEncoderConfiguration method:\n If the orientation mode of the encoding video ( OrientationMode ) is fixed landscape mode or the adaptive landscape mode, the watermark uses the landscape orientation.\n If the orientation mode of the encoding video (OrientationMode) is fixed portrait mode or the adaptive portrait mode, the watermark uses the portrait orientation.\n When setting the watermark position, the region must be less than thesetVideoEncoderConfiguration dimensions set in the method; otherwise, the watermark image will be cropped. \n Ensure that call this method after enableVideo .\n If you only want to add a watermark to the media push, you can call this method or the setLiveTranscoding method.\n This method supports adding a watermark image in the PNG file format only. Supported pixel formats of the PNG image are RGBA, RGB, Palette, Gray, and Alpha_gray.\n If the dimensions of the PNG image differ from your settings in this method, the image will be cropped or zoomed to conform to your settings.\n If you have enabledthe local video preview by calling the startPreview method, you can use the visibleInPreview member to set whether or not the watermark is visible in the preview.\n If you have enabled the mirror mode for the local video, the watermark on the local video is also mirrored. To avoid mirroring the watermark, Agora recommends that you do not use the mirror and watermark functions for the local video at the same time. You can implement the watermark function in your application layer.",
        "parameters": [
            {
                "watermarkUrl": "The local file path of the watermark image to be added. This method supports adding a watermark image from the local absolute or relative file path."
            },
            {
                "options": "The options of the watermark image to be added. "
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "api_registerpacketobserver",
        "name": "registerPacketObserver",
        "description": "Registers a packet observer.\nCall this method registers a packet observer. When the Agora SDK triggers IPacketObserver callbacks registered by for voice or video packet transmission, you can call this method to process the packets, such as encryption and decryption. The size of the packet sent to the network after processing should not exceed 1200 bytes, otherwise, the SDK may fail to send the packet.\n Ensure that both receivers and senders call this method; otherwise, you may meet undefined behaviors such as no voice and black screen.\n When you use media push or recording functions, Agora doesn't recommend calling this method.\n Call this method before joining a channel.",
        "parameters": [
            {
                "observer": " IPacketObserver ."
            }
        ],
        "returns": "0: Success.\n < 0: Failure.",
        "is_hide": true
    },
    {
        "id": "api_startprimarycameracapture",
        "name": "startPrimaryCameraCapture",
        "description": "Starts video capture with a primary camera.\n",
        "parameters": [
            {
                "config": "The configuration of the video capture with a primary camera. See CameraCapturerConfiguration .\n "
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "api_irtcengineex_setvideoencoderconfigurationex",
        "name": "setVideoEncoderConfigurationEx",
        "description": "Sets the encoder configuration for the local video.\nEach configuration profile corresponds to a set of video parameters, including the resolution, frame rate, and bitrate.\n The config specified in this method is the maximum values under ideal network conditions. If the network condition is not good, the video engine cannot use the\nconfig renders local video, which automatically reduces to an appropriate video parameter setting.",
        "parameters": [
            {
                "connection": "The connection information. See RtcConnection ."
            },
            {
                "config": "Video profile. See VideoEncoderConfiguration ."
            }
        ],
        "returns": "0: Success.\n < 0: Failure.",
        "is_hide": false
    },
    {
        "id": "class_logconfig_ng",
        "name": "LogConfig",
        "description": "Configuration of Agora SDK log files.\n",
        "parameters": [
            {
                "filePath": "The complete path of the log files. Ensure that the path for the log file exists and is writable. You can use this parameter to rename the log files.\n "
            },
            {
                "fileSizeInKB": "The size (KB) of an agorasdk.log file. The value range is [128,1024]. The default value is 1,024 KB. If you set fileSizeInKByte to a value lower than 128 KB, the SDK adjusts it to 128 KB. If you set fileSizeInKBytes to a value higher than 1,024 KB, the SDK adjusts it to 1,024 KB."
            },
            {
                "level": "The output level of the SDK log file. See LogLevel .\n For example, if you set the log level to WARN, the SDK outputs the logs within levels FATAL, ERROR, and WARN.\n "
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "api_startechotest2",
        "name": "startEchoTest",
        "description": "Starts an audio call test.\nThis method starts an audio call test to determine whether the audio devices (for example, headset and speaker) and the network connection are working properly. To conduct the test, let the user speak for a while, and the recording is played back within the set interval. If the user can hear the recording within the interval, the audio devices and network connection are working properly. Call this method before joining a channel.\n After calling startEchoTest, you must call stopEchoTest to end the test. Otherwise, the app cannot perform the next echo test, and you cannot join the channel.\n In the live streaming channels, only a host can call this method.",
        "parameters": [
            {
                "intervalInSeconds": "The time interval (s) between when you speak and when the recording plays back. The value range is [2, 10], and the default value is 10."
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "api_setvoiceconversionparameters",
        "name": "setVoiceConversionParameters",
        "description": "",
        "parameters": [
            {
                "null": ""
            }
        ],
        "returns": "",
        "is_hide": true
    },
    {
        "id": "api_startaudiomixing2",
        "name": "startAudioMixing",
        "description": "Starts playing the music file.\nThis method mixes the specified local or online audio file with the audio from the microphone, or replaces the microphone's audio with the specified local or remote audio file. A successful method call triggers the onAudioMixingStateChanged (audioMixingStatePlaying) callback. When the audio mixing file playback finishes, the SDK triggers the onAudioMixingStateChanged(audioMixingStateStopped) callback on the local client. For the audio file formats supported by this method, see What formats of audio files the Agora RTC SDK support.\n You can call this method either before or after joining a channel. If you need to call startAudioMixing multiple times, ensure that the call interval is longer than 500 ms.\n If the local music file does not exist, the SDK does not support the file format, or the the SDK cannot access the music file URL, the SDK reports (701).",
        "parameters": [
            {
                "filePath": "File path:\n Android: The file path, which needs to be accurate to the file name and suffix. Agora supports using a URI address, an absolute path, or a path that starts with /assets/. You might encounter permission issues if you use an absolute path to access a local file, so Agora recommends using a URI address instead. For example: content://com.android.providers.media.documents/document/audio%3A14441\n Windows: The absolute path or URL address (including the suffixes of the filename) of the audio effect file. For example: C:\\music\\audio.mp4.\n iOS or macOS: The absolute path or URL address (including the suffixes of the filename) of the audio effect file. For example: /var/mobile/Containers/Data/audio.mp4.\n "
            },
            {
                "loopback": "Whether to play music files only on the local client:\n true: Only play music files on the local client so that only the local user can hear the music.\n false: Publish music files to remote clients so that both the local user and remote users can hear the music. "
            },
            {
                "replace": "Whether to replace the audio captured by the microphone with a music file:\n true: Replace the audio captured by the microphone with a music file. Users can only hear the music.\n false: Do not replace the audio captured by the microphone with a music file. Users can hear both music and audio captured by the microphone. "
            },
            {
                "cycle": "The number of times the music file plays.\n ≥ 0: The number of playback times. For example, 0 means that the SDK does not play the music file while 1 means that the SDK plays once.\n -1: Play the music file in an infinite loop. "
            },
            {
                "startPos": "The playback position (ms) of the music file."
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "callback_ondownlinknetworkinfoupdated",
        "name": "onDownlinkNetworkInfoUpdated",
        "description": "Occurs when the downlink network information changes.\nThis callback is used for notifying the user to switch major/minor stream if needed.",
        "parameters": [
            {
                "info": "The downlink network information. "
            }
        ],
        "returns": "",
        "is_hide": true
    },
    {
        "id": "api_ivideodevicemanager_startdevicetest",
        "name": "startDeviceTest",
        "description": "Starts the video capture device test.\nThis method tests whether the video-capture device is working properly. Before calling this method, ensure that you have already called the enableVideo method, and the window handle (hwnd) parameter is valid.",
        "parameters": [],
        "returns": "",
        "is_hide": true
    },
    {
        "id": "callback_onuseroffline",
        "name": "onUserOffline",
        "description": "Occurs when a remote user (in the communication profile)/ host (in the live streaming profile) leaves the channel.\nThere are two reasons for users to become offline:\n Leave the channel: When a user/host leaves the channel, the user/host sends a goodbye message. When this message is received, the SDK determines that the user/host leaves the channel.\n Drop offline: When no data packet of the user or host is received for a certain period of time (20 seconds for the communication profile, and more for the live broadcast profile), the SDK assumes that the user/host drops offline. A poor network connection may lead to false detections. It's recommended to use the Agora RTM SDK for reliable offline detection.",
        "parameters": [
            {
                "connection": "The connection information. See RtcConnection ."
            },
            {
                "remoteUid": "The ID of the user who leaves the channel or goes offline."
            },
            {
                "reason": "Reasons why the user goes offline: UserOfflineReasonType .\n "
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "api_stopalleffects",
        "name": "stopAllEffects",
        "description": "Stops playing all audio effects.\n",
        "parameters": [],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "api_imediaplayer_unregisteraudioframeobserver",
        "name": "unregisterAudioFrameObserver",
        "description": "Unregisters an audio observer.\n",
        "parameters": [],
        "returns": "",
        "is_hide": true
    },
    {
        "id": "api_updatescreencaptureregion",
        "name": "updateScreenCaptureRegion",
        "description": "Updates the screen sharing region.\n",
        "parameters": [
            {
                "regionRect": "The relative location of the screen-share area to the screen or window. If you do not set this parameter, the SDK shares the whole screen or window. See Rectangle . If the specified region overruns the screen or window, the SDK shares only the region within it; if you set width or height as 0, the SDK shares the whole screen or window."
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "api_disablevideo",
        "name": "disableVideo",
        "description": "Disables the video module.\nThis method disables video. You can call this method either before or after joining a channel. If you call it before joining a channel, an audio call starts when you join the channel. If you call it after joining a channel, a video call switches to an audio call. Call enableVideo to enable video.\n A successful call of this method triggers the onUserEnableVideo (false)callback on the remote client. This method affects the internal engine and can be called after leaving the channel.\n This method resets the internal engine and takes some time to take effect. Agora recommends using the following API methods to control the video engine modules separately: \n enableLocalVideo : Whether to enable the camera to create the local video stream. \n muteLocalVideoStream : Whether to publish the local video stream. \n muteRemoteVideoStream : Whether to subscribe to and play the remote video stream. \n muteAllRemoteVideoStreams : Whether to subscribe to and play all remote video streams.",
        "parameters": [],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "callback_onclientrolechanged",
        "name": "onClientRoleChanged",
        "description": "Occurs when the user role switches in the interactive live streaming.\n",
        "parameters": [
            {
                "connection": "The connection information. See RtcConnection ."
            },
            {
                "oldRole": "Role that the user switches from: ClientRoleType ."
            },
            {
                "newRole": "Role that the user switches to: ClientRoleType ."
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "api_setremoterendermode2",
        "name": "setRemoteRenderMode",
        "description": "Updates the display mode of the video view of a remote user.\nAfter initializing the video view of a remote user, you can call this method to update its rendering and mirror modes. This method affects only the video view that the local user sees. Please call this method after initializing the remote view by calling the setupRemoteVideo method.\n During a call, you can call this method as many times as necessary to update the display mode of the video view of a remote user.",
        "parameters": [
            {
                "uid": "The user ID of the remote user."
            },
            {
                "renderMode": "The rendering mode of the remote user view. \n "
            },
            {
                "mirrorMode": "The mirror mode of the remote user view. \n "
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "class_livestreamadvancedfeature",
        "name": "LiveStreamAdvancedFeature",
        "description": "The configuration for advanced features of the RTMP or RTMPS streaming with transcoding.\nIf you want to enable the advanced features of streaming with transcoding, contact .",
        "parameters": [
            {
                "featureName": "The feature names, including LBHQ (high-quality video with a lower bitrate) and VEO (optimized video encoder)."
            },
            {
                "opened": "Whether to enable the advanced features of streaming with transcoding:\n true: Enable the advanced features.\n false: (Default) Do not enable the advanced features.\n "
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "api_adjustcustomaudiopublishvolume_ng",
        "name": "adjustCustomAudioPublishVolume",
        "description": "",
        "parameters": [
            {
                "null": ""
            }
        ],
        "returns": "",
        "is_hide": true
    },
    {
        "id": "api_mutelocalvideostream",
        "name": "muteLocalVideoStream",
        "description": "Stops or resumes publishing the local video stream.\nA successful call of this method triggers the onUserMuteVideo callback on the remote client. This method executes faster than the enableLocalVideo (false) method, which controls the sending of the local video stream.\n This method does not affect any ongoing video recording, because it does not disable the camera.",
        "parameters": [
            {
                "mute": "Whether to stop publishing the local video stream.\n true: Stop publishing the local video stream.\n false: (Default) Publish the local video stream. "
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "class_directcdnstreamingstats",
        "name": "DirectCdnStreamingStats",
        "description": "当前 CDN 推流的统计数据。\n",
        "parameters": [
            {
                "videoWidth": "视频的宽度（px）。"
            },
            {
                "videoHeight": "视频的高度（px）。"
            },
            {
                "fps": "当前视频帧率（fps）。"
            },
            {
                "videoBitrate": "当前视频码率（bps）。"
            },
            {
                "audioBitrate": "当前音频码率（bps）。"
            }
        ],
        "returns": "",
        "is_hide": true
    },
    {
        "id": "enum_lighteningcontrastlevel",
        "name": "LighteningContrastLevel",
        "description": "The contrast level.\n",
        "parameters": [
            {
                "lighteningContrastLow": "Low contrast level."
            },
            {
                "lighteningContrastNormal": "Normal contrast level."
            },
            {
                "lighteningContrastHigh": "High contrast level."
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "api_setaudiomixingposition",
        "name": "setAudioMixingPosition",
        "description": "Sets the audio mixing position.\nCall this method to set the playback position of the music file to a different starting position, rather than playing the file from the beginning.\n You need to call this method after calling startAudioMixing and receiving the onAudioMixingStateChanged(audioMixingStatePlaying) callback.",
        "parameters": [
            {
                "pos": "Integer. The playback position (ms)."
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "api_imediaplayer_getstate",
        "name": "getState",
        "description": "Gets current playback state.\n",
        "parameters": [],
        "returns": "The current playback state. See MediaPlayerState .",
        "is_hide": false
    },
    {
        "id": "api_takesnapshot",
        "name": "takeSnapshot",
        "description": "Takes a snapshot of a video stream.\nThis method takes a snapshot of a video stream from the specified user, generates a JPG image, and saves it to the specified path.\n The method is asynchronous, and the SDK has not taken the snapshot when the method call returns. After a successful method call, the SDK triggers the onSnapshotTaken callback to report whether the snapshot is successfully taken, as well as the details for that snapshot. Call this method after joining a channel.\n If the user's video has been preprocessed, for example, watermarked or beautified, the resulting snapshot includes the pre-processing effect.",
        "parameters": [
            {
                "config": "The configuration of the snaptshot, see ."
            }
        ],
        "returns": "",
        "is_hide": true
    },
    {
        "id": "api_imediaplayer_stop",
        "name": "stop",
        "description": "Stops playing the media track.\n",
        "parameters": [],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "api_stopprimarycameracapture",
        "name": "stopPrimaryCameraCapture",
        "description": "Stops capturing video through a primary camera.\nYou can call this method to stop capturing video through the primary camera after calling the startPrimaryCameraCapture .",
        "parameters": [],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "enum_playerpreloadevent",
        "name": "PlayerPreloadEvent",
        "description": "Events that occur when media resources are preloaded.\n",
        "parameters": [
            {
                "playerPreloadEventBegin": "0: Starts preloading media resources."
            },
            {
                "playerPreloadEventComplete": "1: Preloading media resources is complete."
            },
            {
                "playerPreloadEventError": "2: An error occurs when preloading media resources."
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "class_rtcengineconfig_ng",
        "name": "RtcEngineContext",
        "description": "Definition of RtcEngineContext.\n",
        "parameters": [
            {
                "appId": "The App ID issued by Agora for your project. Only users in apps with the same App ID can join the same channel and communicate with each other. An App ID can only be used to create one RtcEngine instance. To change your App ID, call release to destroy the current RtcEngine instance, and then create a new one."
            },
            {
                "channelProfile": "The channel profile. See ChannelProfileType .\n "
            },
            {
                "audioScenario": "The audio scenario. Under different audio scenarios, the device uses different volume types."
            },
            {
                "areaCode": "The region for connection. This is an advanced feature and applies to scenarios that have regional restrictions. For the regions that Agora supports, see AreaCode . The area codes support bitwise operation.After specifying the region, the app integrated with the Agora SDK connects to the Agora servers within that region.\n "
            },
            {
                "logConfig": "The SDK log files are: agorasdk.log, agorasdk.1.log, agorasdk.2.log, agorasdk.3.log, and agorasdk.4.log.\n The API call log files are: agoraapi.log, agoraapi.1.log, agoraapi.2.log, agoraapi.3.log, and agoraapi.4.log.\n The default size for each SDK log file is 1,024 KB; the default size for each API call log file is 2,048 KB. These log files are encoded in UTF-8.\n The SDK writes the latest logs in agorasdk.log or agoraapi.log.\n When agorasdk.log is full, the SDK processes the log files in the following order:\n Delete the agorasdk.4.log file (if any).\n Rename agorasdk.3.log to agorasdk.4.log.\n Rename agorasdk.2.log to agorasdk.3.log.\n Rename agorasdk.1.log to agorasdk.2.log.\n Create a new agorasdk.log file. The overwrite rules for the agoraapi.log file are the same as for agorasdk.log. The log files that the SDK outputs. See LogConfig .\n By default, the SDK generates five SDK log files and five API call log files with the following rules:\n "
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "callback_onaudiosubscribestatechanged",
        "name": "onAudioSubscribeStateChanged",
        "description": "Occurs when the audio subscribing state changes.\n",
        "parameters": [
            {
                "channel": "The channel name."
            },
            {
                "uid": "The user ID of the remote user."
            },
            {
                "oldState": "The previous subscribing status. See StreamSubscribeState ."
            },
            {
                "newState": "The current subscribing status. See StreamSubscribeState."
            },
            {
                "elapseSinceLastState": "The time elapsed (ms) from the previous state to the current state."
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "api_ivideodevicemanager_release",
        "name": "release",
        "description": "Releases all the resources occupied by the VideoDeviceManager object.\n",
        "parameters": [],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "api_imediaplayer_mute",
        "name": "mute",
        "description": "Sets whether to mute the media file.\n",
        "parameters": [
            {
                "mute": "Whether to mute the media file:\n true: Mute the media file.\n false: (Default) Unmute the media file.\n "
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "class_idirectcdnstreamingeventhandler",
        "name": "IDirectCdnStreamingEventHandler",
        "description": "The IDirectCdnStreamingEventHandler interface class is used by the SDK to send event notifications of CDN streaming to your app. Your app can get those notifications through methods that inherit this interface class.\n",
        "parameters": [],
        "returns": "",
        "is_hide": true
    },
    {
        "id": "api_createagorartcengine",
        "name": "createAgoraRtcEngine",
        "description": "Creates the RtcEngine object.\nCurrently, the Agora RTC SDK v4.0.0 supports creating only one RtcEngine object for an app.",
        "parameters": [],
        "returns": "RtcEngine object.",
        "is_hide": false
    },
    {
        "id": "class_imediaplayercustomdataprovider",
        "name": "IMediaPlayerCustomDataProvider",
        "description": "The callback for custom media resource files.\n",
        "parameters": [],
        "returns": "",
        "is_hide": true
    },
    {
        "id": "callback_onremoteaudiotransportstats",
        "name": "onRemoteAudioTransportStats",
        "description": "Reports the transport-layer statistics of each remote audio stream.\nDeprecated:\n Please use onRemoteAudioStats instead. \n This callback reports the transport-layer statistics, such as the packet loss rate and network time delay, once every two seconds after the local user receives an audio packet from a remote user. During a call, when the user receives the video packet sent by the remote user/host, the callback is triggered every 2 seconds.",
        "parameters": [
            {
                "connection": "The connection information. See RtcConnection ."
            },
            {
                "remoteUid": "The ID of the remote user sending the audio packets."
            },
            {
                "delay": "The network delay (ms) from the sender to the receiver."
            },
            {
                "lost": "The packet loss rate (%) of the audio packet sent from the remote user."
            },
            {
                "rxKBitRate": "The bitrate of the received audio (Kbps)."
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "class_agoravideoview",
        "name": "AgoraVideoView",
        "description": "The AgoraVideoView Class for rendering local and remote video.\n",
        "parameters": [
            {
                "key": "The identifier of the Widget. See the description of the key object in the official Flutter documentation."
            },
            {
                "controller": "Controls the type of video to render:\n If you want to render video of the RtcEngine, see VideoViewController .\n If you want to render video of the media player, see MediaPlayerController .\n "
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "api_ilocalspatialaudioengine_updateremoteposition",
        "name": "updateRemotePosition",
        "description": "Updates the spatial position of the specified remote user.\nAfter successfully calling this method, the SDK calculates the spatial audio parameters based on the relative position of the local and remote user.\n Call this method after joinChannelWithOptions .",
        "parameters": [
            {
                "uid": "The user ID. This parameter must be the same as the user ID passed in when the user joined the channel."
            },
            {
                "posInfo": "The spatial position of the remote user. See RemoteVoicePositionInfo for details."
            }
        ],
        "returns": "",
        "is_hide": true
    },
    {
        "id": "class_packet",
        "name": "Packet",
        "description": "Definition of Packet.\n",
        "parameters": [
            {
                "buffer": "The buffer address of the sent or received data.\n Agora 建议 buffer 值不要小于 2048 字节，否则有可能会出现未定义行为（例如崩溃）。\n "
            },
            {
                "size": "将要发送或接收的数据的缓存大小。"
            }
        ],
        "returns": "",
        "is_hide": true
    },
    {
        "id": "class_localtranscoderconfiguration",
        "name": "LocalTranscoderConfiguration",
        "description": "The configuration of the video mixing on the local client.\n",
        "parameters": [
            {
                "streamCount": "The number of the video streams for the video mixing on the local client.\n "
            },
            {
                "VideoInputStreams": "The video streams for the video mixing on the local client. See TranscodingVideoStream .\n "
            },
            {
                "videoOutputConfiguration": "The encoding configuration of the mixed video stream after the video mixing on the local client. See VideoEncoderConfiguration .\n "
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "class_rtcstats",
        "name": "RtcStats",
        "description": "Statistics of the channel.\n",
        "parameters": [
            {
                "duration": "Call duration of the local user in seconds, represented by an aggregate value."
            },
            {
                "txBytes": "Total number of bytes transmitted, represented by an aggregate value."
            },
            {
                "rxBytes": "Total number of bytes received, represented by an aggregate value."
            },
            {
                "txAudioBytes": "Total number of audio bytes sent, represented by an aggregate value."
            },
            {
                "txVideoBytes": "The total number of video bytes sent, represented by an aggregate value."
            },
            {
                "rxAudioBytes": "The total number of audio bytes received, represented by an aggregate value."
            },
            {
                "rxVideoBytes": "The total number of video bytes received, represented by an aggregate value."
            },
            {
                "txKBitRate": "Video transmission bitrate (Kbps), represented by an instantaneous value."
            },
            {
                "rxKBitRate": "The receiving bitrate (Kbps), represented by an instantaneous value."
            },
            {
                "rxAudioKBitRate": "Audio receive bitrate (Kbps), represented by an instantaneous value."
            },
            {
                "txAudioKBitRate": "The bitrate (Kbps) of sending the audio packet."
            },
            {
                "rxVideoKBitRate": "Video receive bitrate (Kbps), represented by an instantaneous value."
            },
            {
                "txVideoKBitRate": "The bitrate (Kbps) of sending the video."
            },
            {
                "lastmileDelay": "The client-to-server delay (ms)."
            },
            {
                "txPacketLossRate": "The packet loss rate (%) from the client to the Agora server before applying the anti-packet-loss algorithm."
            },
            {
                "rxPacketLossRate": "The packet loss rate (%) from the Agora server to the client before using the anti-packet-loss method."
            },
            {
                "userCount": "The number of users in the channel."
            },
            {
                "cpuAppUsage": "Application CPU usage (%). The value of cpuTotalUsage is always reported as 0 in the onLeaveChannel callback.\n As of Android 8.1, you cannot get the CPU usage from this attribute due to system limitations.\n "
            },
            {
                "cpuTotalUsage": "The system CPU usage (%).\n For Windows, in the multi-kernel environment, this member represents the average CPU usage. The value = (100 - System Idle Progress in Task Manager)/100. The value of cpuTotalUsage is always reported as 0 in the onLeaveChannel callback.\n "
            },
            {
                "connectTimeMs": "The duration (ms) between the SDK starts connecting and the connection is established. If the value reported is 0, it means invalid."
            },
            {
                "gatewayRtt": "The round-trip time delay (ms) from the client to the local router.On Android, to get gatewayRtt, ensure that you add the android.permission.ACCESS_WIFI_STATE permission after </application> in the AndroidManifest.xml file in your project."
            },
            {
                "memoryAppUsageRatio": "The memory ratio occupied by the app (%).\n This value is for reference only. Due to system limitations, you may not get this value. "
            },
            {
                "memoryTotalUsageRatio": "The memory occupied by the system (%).\n This value is for reference only. Due to system limitations, you may not get this value. "
            },
            {
                "memoryAppUsageInKbytes": "The memory size occupied by the app (KB).\n This value is for reference only. Due to system limitations, you may not get this value. "
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "callback_ivideoframeobserver_onrendervideoframe",
        "name": "onRenderVideoFrame",
        "description": "Occurs each time the SDK receives a video frame sent by the remote user.\nAfter you successfully register the video frame observer, the SDK triggers this callback each time it receives a video frame. In this callback, you can get the video data before encoding and then process the data according to your particular scenarios.\n After processing, you can send the processed video data back to the SDK in this callback.\n This callback does not support sending processed RGBA video data back to the SDK.",
        "parameters": [
            {
                "videoFrame": ""
            },
            {
                "channelId": "The ID of the channel."
            }
        ],
        "returns": "true: Sets the SDK to receive the video frame.\n false: Sets the SDK to discard the video frame.",
        "is_hide": true
    },
    {
        "id": "callback_onlastmileproberesult",
        "name": "onLastmileProbeResult",
        "description": "Reports the last mile network probe result.\nThe SDK triggers this callback within 30 seconds after the app calls startLastmileProbeTest .",
        "parameters": [
            {
                "result": "The uplink and downlink last-mile network probe test result. See LastmileProbeResult ."
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "api_pushdirectcdnstreamingcustomvideoframe",
        "name": "pushDirectCdnStreamingCustomVideoFrame",
        "description": "Pushes an external video stream to the SDK.\n",
        "parameters": [
            {
                "frame": "The external video data. For details, see "
            }
        ],
        "returns": "0: Success.\n < 0: Failure.",
        "is_hide": true
    },
    {
        "id": "api_imediaplayer_getplayoutvolume",
        "name": "getPlayoutVolume",
        "description": "Gets the local playback volume.\n",
        "parameters": [],
        "returns": "The local playback volume, which ranges from 0 to 100.\n 0: Mute.\n 100: (Default) The original volume.",
        "is_hide": false
    },
    {
        "id": "api_ivideodevicemanager_stopdevicetest",
        "name": "stopDeviceTest",
        "description": "停止视频采集设备测试。\n",
        "parameters": [],
        "returns": "",
        "is_hide": true
    },
    {
        "id": "api_sendcustomreportmessage",
        "name": "sendCustomReportMessage",
        "description": "Reports customized messages.\nAgora supports reporting and analyzing customized messages. This function is in the beta stage with a free trial. The ability provided in its beta test version is reporting a maximum of 10 message pieces within 6 seconds, with each message piece not exceeding 256 bytes and each string not exceeding 100 bytes. To try out this function, contact and discuss the format of customized messages with us.",
        "parameters": [],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "api_preloadeffect",
        "name": "preloadEffect",
        "description": "Preloads a specified audio effect file into the memory.\nTo ensure smooth communication, limit the size of the audio effect file. Wer ecommend using this method to preload the audio effect before calling joinChannelWithOptions. This method does not support online audio effect files.\n For the audio file formats supported by this method, see What formats of audio files the Agora RTC SDK support.",
        "parameters": [
            {
                "soundId": "The audio effect ID. The ID of each audio effect file is unique."
            },
            {
                "filePath": "File path:\n Android: The file path, which needs to be accurate to the file name and suffix. Agora supports using a URI address, an absolute path, or a path that starts with /assets/. You might encounter permission issues if you use an absolute path to access a local file, so Agora recommends using a URI address instead. For example: content://com.android.providers.media.documents/document/audio%3A14441\n Windows: The absolute path or URL address (including the suffixes of the filename) of the audio effect file. For example: C:\\music\\audio.mp4.\n iOS or macOS: The absolute path or URL address (including the suffixes of the filename) of the audio effect file. For example: /var/mobile/Containers/Data/audio.mp4.\n "
            },
            {
                "startPos": "The playback position (ms) of the audio effect file."
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "api_enableinearmonitoring2",
        "name": "enableInEarMonitoring",
        "description": "Enables in-ear monitoring.\nThis method enables or disables in-ear monitoring.",
        "parameters": [
            {
                "enabled": "Enables in-ear monitoring.\n true: Enables in-ear monitoring.\n false: (Default) Disables in-ear monitoring.\n "
            },
            {
                "includeAudioFilters": ""
            }
        ],
        "returns": "0: Success.\n < 0: Failure.",
        "is_hide": false
    },
    {
        "id": "api_adjustaudiomixingplayoutvolume",
        "name": "adjustAudioMixingPlayoutVolume",
        "description": "Adjusts the volume of audio mixing for local playback.\nCall this method after calling startAudioMixing and receiving the onAudioMixingStateChanged(audioMixingStatePlaying) callback.",
        "parameters": [
            {
                "volume": "The volume of audio mixing for local playback. The value ranges between 0 and 100 (default). 100 represents the original volume."
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "enum_localaudiostreamstate",
        "name": "localaudiostreamstate",
        "description": "The state of the local audio.\n",
        "parameters": [
            {
                "localAudioStreamStateStopped": "0: The local audo is in the initial state."
            },
            {
                "localAudioStreamStateRecording": "1: The local audo capturing device starts successfully."
            },
            {
                "localAudioStreamStateEncoding": "2: The first audo frame encodes successfully."
            },
            {
                "localAudioStreamStateFailed": "3: The local audio fails to start."
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "api_setlocalvideomirrormode",
        "name": "setLocalVideoMirrorMode",
        "description": "Sets the local video mirror mode.\nDeprecated:\n This method is deprecated.\n Use setupLocalVideo or setLocalRenderMode [2/2] instead.",
        "parameters": [
            {
                "mirrorMode": "The local video mirror mode. See VideoMirrorModeType .\n "
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "api_startaudiomixing",
        "name": "startAudioMixing [1/2]",
        "description": "Starts playing the music file.\nThis method mixes the specified local or online audio file with the audio from the microphone, or replaces the microphone's audio with the specified local or remote audio file. A successful method call triggers the onAudioMixingStateChanged (audioMixingStatePlaying) callback. When the audio mixing file playback finishes, the SDK triggers the onAudioMixingStateChanged (audioMixingStateStopped) callback on the local client. You can call this method either before or after joining a channel. If you need to call startAudioMixing [1/2] multiple times, ensure that the call interval is longer than 500 ms.\n If the local music file does not exist, the SDK does not support the file format, or the the SDK cannot access the music file URL, the SDK reports (701).",
        "parameters": [
            {
                "loopback": "Whether to play music files only on the local client:\n true: Only play music files on the local client so that only the local user can hear the music.\n false: Publish music files to remote clients so that both the local user and remote users can hear the music. "
            },
            {
                "replace": "Whether to replace the audio captured by the microphone with a music file:\n true: Replace the audio captured by the microphone with a music file. Users can only hear the music.\n false: Do not replace the audio captured by the microphone with a music file. Users can hear both music and audio captured by the microphone. "
            },
            {
                "cycle": "The number of times the music file plays.\n ≥ 0: The number of playback times. For example, 0 means that the SDK does not play the music file while 1 means that the SDK plays once.\n -1: Play the music file in an infinite loop. "
            }
        ],
        "returns": "0: Success.\n < 0: Failure.",
        "is_hide": true
    },
    {
        "id": "class_contentinspectmodule",
        "name": "ContentInspectModule",
        "description": " ContentInspectModule 结构体，用于配置内容审核模块的类型和频率。 ",
        "parameters": [
            {
                "type": " 内容审核模块的类型。 "
            },
            {
                "interval": "内容审核的间隔，单位为秒，取值必须大于 0。 默认值为 0，表示不进行内容审核。 推荐值为 10 秒，你也可以根据业务需求自行调整。"
            }
        ],
        "returns": "",
        "is_hide": true
    },
    {
        "id": "class_imediaplayersourceobserver",
        "name": "MediaPlayerSourceObserver",
        "description": "Provides callbacks for media players.\n",
        "parameters": [],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "class_iaudioframeobserverbase",
        "name": "IAudioFrameObserverBase",
        "description": "The audio frame observer.\n",
        "parameters": [],
        "returns": "",
        "is_hide": true
    },
    {
        "id": "callback_onusermutevideo",
        "name": "onUserMuteVideo",
        "description": "Occurs when a remote user stops or resumes publishing the video stream.\nWhen a remote user calls muteLocalVideoStream to stop or resume publishing the video stream, the SDK triggers this callback to report the state of the remote user's publishing stream to the local user.\n This callback can be inaccurate when the number of users (in the communication profile) or hosts (in the live broadcasting profile) in a channel exceeds 17.",
        "parameters": [
            {
                "connection": "The connection information. See RtcConnection ."
            },
            {
                "remoteUid": "The user ID of the remote user."
            },
            {
                "muted": "Whether the remote user stops publishing the video stream:\n true: Stop publishing the local video stream.\n false: Publish the video stream.\n "
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "enum_localvideostreamerror",
        "name": "LocalVideoStreamError",
        "description": "Local video state error code.\n",
        "parameters": [
            {
                "localVideoStreamErrorOk": "0: The local video is normal."
            },
            {
                "localVideoStreamErrorFailure": "1: No specified reason for the local video failure."
            },
            {
                "localVideoStreamErrorDeviceNoPermission": "2: No permission to use the local video capturing device."
            },
            {
                "localVideoStreamErrorDeviceBusy": "3: The local video capturing device is in use."
            },
            {
                "localVideoStreamErrorCaptureFailure": "4: The local video capture fails. Check whether the capturing device is working properly."
            },
            {
                "localVideoStreamErrorEncodeFailure": "5: The local video encoding fails."
            },
            {
                "localVideoStreamErrorDeviceNotFound": "8: Fails to find a local video capture device.\n "
            },
            {
                "localVideoStreamErrorScreenCaptureWindowMinimized": "11: When calling startScreenCaptureByWindowId to share the window, the shared window is in a minimized state."
            },
            {
                "localVideoStreamErrorScreenCaptureWindowClosed": "12: The error code indicates that a window shared by the window ID has been closed, or a full-screen window shared by the window ID has exited full-screen mode. After exiting full-screen mode, remote users cannot see the shared window. To prevent remote users from seeing a black screen, Agora recommends that you immediately stop screen sharing.\n Common scenarios for reporting this error code:\n When the local user closes the shared window, the SDK reports this error code.\n The local user shows some slides in full-screen mode first, and then shares the windows of the slides. After the user exits full-screen mode, the SDK reports this error code.\n The local user watches a web video or reads a web document in full-screen mode first, and then shares the window of the web video or document. After the user exits full-screen mode, the SDK reports this error code. "
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "callback_ontokenprivilegewillexpire",
        "name": "onTokenPrivilegeWillExpire",
        "description": "Occurs when the token expires in 30 seconds.\nWhen the token is about to expire in 30 seconds, the SDK triggers this callback to remind the app to renew the token.\n Upon receiving this callback, generate a new token on your server, and call renewToken to pass the new token to the SDK.",
        "parameters": [
            {
                "connection": "The connection information. See RtcConnection ."
            },
            {
                "token": "The token that expires in 30 seconds."
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "api_setcameraautofocusfacemodeenabled",
        "name": "setCameraAutoFocusFaceModeEnabled",
        "description": "Sets whether to enable face autofocus.\nBy default, the SDK disables face autofocus on Android and enables face autofocus on iOS. To set face autofocus, call this method. This method is for Android and iOS only.\n Call this method after the camera is started, such as after joinChannelWithOptions , enableVideo , or enableLocalVideo .",
        "parameters": [
            {
                "enabled": "Whether to enable face autofocus:\n true: Enable face autofocus.\n false: Disable face autofocus. "
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "api_iaudiodevicemanager_stoprecordingdevicetest",
        "name": "stopRecordingDeviceTest",
        "description": "Stops the audio capture device test.\nThis method stops the audio capture device test. You must call this method to stop the test after calling the startRecordingDeviceTest method.\n Ensure that you call this method before joining a channel.",
        "parameters": [],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "class_irtcengineex",
        "name": "RtcEngineEx",
        "description": "This interface class contains multi-channel methods.\nInherited from RtcEngine .",
        "parameters": [],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "api_getuserinfobyuseraccount",
        "name": "getUserInfoByUserAccount",
        "description": "Gets the user information by passing in the user account.\nAfter a remote user joins the channel, the SDK gets the UID and user account of the remote user, caches them in a mapping table object, and triggers the onUserInfoUpdated callback on the local client. After receiving the callback, you can call this method to get the user account of the remote user from the UserInfo object by passing in the user ID.",
        "parameters": [
            {},
            {
                "userAccount": "The user account."
            }
        ],
        "returns": "The UserInfo object that identifies the user information. Not null: Success.\n Null: Failure.",
        "is_hide": false
    },
    {
        "id": "api_getassetabsolutepath",
        "name": "getAssetAbsolutePath",
        "description": "Obtain the actual absolute path of the Asset through the relative path of the Asset.\n",
        "parameters": [
            {
                "assetPath": "The flutter -> assets field configured in the pubspec.yaml file."
            }
        ],
        "returns": "The actual path of the Asset.",
        "is_hide": false
    },
    {
        "id": "enum_directcdnstreamingerror",
        "name": "DIRECT_CDN_STREAMING_ERROR",
        "description": "The reason for the CDN streaming error.\n",
        "parameters": [
            {
                "DIRECT_CDN_STREAMING_ERROR_OK": "0: No error."
            },
            {
                "DIRECT_CDN_STREAMING_ERROR_FAILED": "1: General error, no clear reason. You can try to push the stream again."
            },
            {
                "DIRECT_CDN_STREAMING_ERROR_AUDIO_PUBLICATION": "2: An error occurs when pushing audio streams. For example, the local audio capture device is not working properly, is occupied by another process, or does not get the permission."
            },
            {
                "DIRECT_CDN_STREAMING_ERROR_VIDEO_PUBLICATION": "3: An error occurs when pushing video streams. For example, the local video capture device is not working properly, is occupied by another process, or does not get the permission."
            },
            {
                "DIRECT_CDN_STREAMING_ERROR_NET_CONNECT": "4: Fails to connect to the CDN."
            },
            {
                "DIRECT_CDN_STREAMING_ERROR_BAD_NAME": "5: The URL is already being used. Use a new media push URL."
            }
        ],
        "returns": "",
        "is_hide": true
    },
    {
        "id": "api_joinchannelwithuseraccount",
        "name": "joinChannelWithUserAccount [1/2]",
        "description": "Joins the channel with a user account.\nThis method allows a user to join the channel with the user account. After the user successfully joins the channel, the SDK triggers the following callbacks:\n The local client: onLocalUserRegistered , onJoinChannelSuccess and onConnectionStateChanged callbacks.\n The remote client: onUserJoined and onUserInfoUpdated , if the user joining the channel is in the communication profile or is a host in the live streaming profile. Once a user joins the channel, the user subscribes to the audio and video streams of all the other users in the channel by default, giving rise to usage and billing calculation. To stop subscribing to a specified stream or all remote streams, call the corresponding mute methods.\n To ensure smooth communication, use the same parameter type to identify the user. For example, if a user joins the channel with a user ID, then ensure all the other users use the user ID too. The same applies to the user account. If a user joins the channel with the Agora Web SDK, ensure that the ID of the user is set to the same parameter type.",
        "parameters": [
            {
                "userAccount": "The user account. This parameter is used to identify the user in the channel for real-time audio and video engagement. You need to set and manage user accounts yourself and ensure that each user account in the same channel is unique. The maximum length of this parameter is 255 bytes. Ensure that you set this parameter and do not set it as NULL. Supported characters are (89 in total):\n The 26 lowercase English letters: a to z.\n The 26 uppercase English letters: A to Z.\n All numeric characters: 0 to 9.\n Space\n \"!\", \"#\", \"$\", \"%\", \"&amp;\", \"(\", \")\", \"+\", \"-\", \":\", \";\", \"&lt;\", \"= \", \".\", \"&gt;\", \"?\", \"@\", \"[\", \"]\", \"^\", \"_\", \"{\", \"}\", \"|\", \"~\", \",\" "
            },
            {
                "token": ""
            }
        ],
        "returns": "0: Success.\n < 0: Failure.\n -2 (ERR_INVALID_ARGUMENT): The parameter is invalid.\n -3(ERR_NOT_READY): The SDK fails to be initialized. You can try re-initializing the SDK.\n -5(ERR_REFUSED): The request is rejected.\n -17(ERR_JOIN_CHANNEL_REJECTED): The request to join the channel is rejected. Since the SDK only supports users to join a channel at the same time RtcEngine , this error code RtcEngine will be returned when the user RtcEngine who has joined the channel calls the join channel method in the class again with a valid channel name.",
        "is_hide": false
    },
    {
        "id": "callback_imediaplayersourceobserver_onpositionchanged",
        "name": "onPositionChanged",
        "description": "Reports the current playback progress.\nWhen playing media files, the SDK triggers this callback every one second to report current playback progress.",
        "parameters": [
            {
                "position": "The playback position (ms) of media files."
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "callback_imediaplayercustomdataprovider_onseek",
        "name": "onSeek",
        "description": "Occurs when the SDK seeks the media resource data.\nWhen you call the method to open a custom media resource, the SDK triggers this callback to request to seek to a specified location in the media resource.",
        "parameters": [
            {
                "offset": "An input parameter. The offset of the target position relative to the starting point, in bytes. The value can be positive or negative."
            },
            {
                "whence": "An input parameter. The starting point. You can set it as one of the following values:\n 0: The starting point is the head of the data, and the actual data offset after seeking is offset.\n 1: The starting point is the current position, and the actual data offset after seeking is the current position plus offset.\n 2: The starting point is the end of the data, and the actual data offset after seeking is the whole data length plus offset.\n 65536: Do not perform position seeking and return the file size. Agora recommends that you use this parameter value when playing pure audio files such as MP3 and WAV.\n "
            }
        ],
        "returns": "When whence is 65536, the media file size is returned.\n When whence is 0, 1, or 2, the actual data offset after the seeking is returned.\n -1: Seeking failed.",
        "is_hide": true
    },
    {
        "id": "class_screencaptureconfiguration",
        "name": "ScreenCaptureConfiguration",
        "description": "The configuration of the captured screen.\n",
        "parameters": [
            {
                "isCaptureWindow": "Whether to capture the window on the screen:\n true: Capture the window.\n false: (Default) Capture the screen, not the window.\n "
            },
            {
                "displayId": "(macOS only) The display ID of the screen.This parameter takes effect only when you want to capture the screen on macOS."
            },
            {
                "screenRect": "(Windows only) The relative position of the shared screen to the virtual screen.This parameter takes effect only when you want to capture the screen on Windows."
            },
            {
                "windowId": "(For Windows and macOS only) Window ID.This parameter takes effect only when you want to capture the window."
            },
            {
                "params": "(For Windows and macOS only) The screen capture configuration. See ScreenCaptureParameters ."
            },
            {
                "regionRect": "(For Windows and macOS only) The relative position of the shared region to the whole screen. See Rectangle .\n If you do not set this parameter, the SDK shares the whole screen. If the region you set exceeds the boundary of the screen, only the region within in the screen is shared. If you setwidth or height in Rectangle as 0, the whole screen is shared.\n "
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "callback_onremotesubscribefallbacktoaudioonly",
        "name": "onRemoteSubscribeFallbackToAudioOnly",
        "description": "远端订阅流已回退为音频流回调。 \n如果你调用了 \n setRemoteSubscribeFallbackOption \n 并将\noption\n设置为\nstreamFallbackOptionAudioOnly\n ，当下行网络环境不理想、仅接收远端音频流时，或当下行网络改善、恢复订阅音视频流时，会触发该回调。 远端订阅流因弱网环境不能同时满足音视频而回退为小流时，你可以使用 \n onRemoteVideoStats \n 来监控远端视频大小流的切换。",
        "parameters": [
            {
                "uid": "The user ID of the remote user."
            },
            {
                "isFallbackOrRecover": "true: 由于网络环境不理想，远端订阅流已回退为音频流；\n false: 由于网络环境改善，订阅的音频流已恢复为音视频流。\n "
            }
        ],
        "returns": "",
        "is_hide": true
    },
    {
        "id": "api_setlocalvoicepitch",
        "name": "setLocalVoicePitch",
        "description": "Changes the voice pitch of the local speaker.\nYou can call this method either before or after joining a channel.",
        "parameters": [
            {
                "pitch": "The local voice pitch. The value range is [0.5,2.0]. The lower the value, the lower the pitch. The default value is 1 (no change to the pitch)."
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "api_setremotedefaultvideostreamtype",
        "name": "setRemoteDefaultVideoStreamType",
        "description": "Sets the default video-stream type of the remotely subscribed video stream when the remote user sends dual streams.\nUnder limited network conditions, if the publisher has not disabled the dual-stream mode using enableDualStreamMode (false), the receiver can choose to receive either the high-quality video stream or the low-quality video stream. The high-quality video stream has a higher resolution and bitrate, and the low-quality video stream has a lower resolution and bitrate.\n By default, users receive the high-quality video stream. Call this method if you want to switch to the low-quality video stream. This method allows the app to adjust the corresponding video stream type based on the size of the video window to reduce the bandwidth and resources. The aspect ratio of the low-quality video stream is the same as the high-quality video stream. Once the resolution of the high-quality video stream is set, the system automatically sets the resolution, frame rate, and bitrate of the low-quality video stream.\n The result of this method is returned in the onApiCallExecuted callback. Call this method before joining a channel. Agora does not support you to change the default subscribed video stream type after joining a channel.\n If you call both this method and setRemoteVideoStreamType , the SDK applies the settings in the setRemoteVideoStreamType method.",
        "parameters": [
            {
                "streamType": "The default video-stream type, see VideoStreamType .\n "
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "callback_ivideoframeobserver_onpreencodevideoframe",
        "name": "onPreEncodeVideoFrame",
        "description": "Occurs each time the SDK receives a video frame before encoding.\nAfter you successfully register the video frame observer, the SDK triggers this callback each time it receives a video frame. In this callback, you can get the video data before encoding and then process the data according to your particular scenarios.\n After processing, you can send the processed video data back to the SDK in this callback. To get the video data captured from the second screen before encoding, you need to set getObservedFramePosition .\n The video data that this callback gets has been preprocessed, with its content cropped and rotated, and the image enhanced.\n This callback does not support sending processed RGBA video data back to the SDK.",
        "parameters": [
            {
                "videoFrame": "The video frame. "
            }
        ],
        "returns": "true: Sets the SDK to receive the video frame.\n false: Sets the SDK to discard the video frame.",
        "is_hide": true
    },
    {
        "id": "callback_ivideoframeobserver_onmediaplayervideoframe",
        "name": "onMediaPlayerVideoFrame",
        "description": "Gets the video data of the media player.\nAfter you successfully register the video frame observer and calling the createMediaPlayer method, the SDK triggers this callback each time when it receives a video frame. In this callback, you can get the video data of the media player. You can then process the data according to your particular scenarios.\n After pre-processing, you can send the processed video data back to the SDK by this callback.",
        "parameters": [
            {
                "videoFrame": "The video frame. "
            },
            {
                "mediaPlayerId": "The ID of the media player."
            }
        ],
        "returns": "true: Sets the SDK to receive the video frame.\n false: Sets the SDK to discard the video frame.",
        "is_hide": true
    },
    {
        "id": "enum_qualityadaptindication",
        "name": "QualityAdaptIndication",
        "description": "Quality change of the local video in terms of target frame rate and target bit rate since last count.\n",
        "parameters": [
            {
                "adaptNone": "0: The local video quality stays the same."
            },
            {
                "adaptUpBandwidth": "1: The local video quality improves because the network bandwidth increases."
            },
            {
                "adaptDownBandwidth": "2: The local video quality deteriorates because the network bandwidth decreases."
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "api_iscameraautoexposurefacemodesupported",
        "name": "isCameraAutoExposureFaceModeSupported",
        "description": "Checks whether the device supports auto exposure.\nThis method applies to iOS only.\n Call this method before calling joinChannelWithOptions , enableVideo , or enableLocalVideo , depending on which method you use to turn on your local camera.",
        "parameters": [],
        "returns": "true: The device supports auto exposure.\n false: The device does not support auto exposure.",
        "is_hide": false
    },
    {
        "id": "enum_saeconnectionstatetype",
        "name": "SAE_CONNECTION_STATE_TYPE",
        "description": "SDK 与 Agora 空间音效服务器的连接状态。\n",
        "parameters": [
            {
                "SAE_CONNECTION_STATE_CONNECTING": "0: 建立连接中。"
            },
            {
                "SAE_CONNECTION_STATE_CONNECTED": "1: 已连接。 该状态下， updateSelfPosition 等空间音效设置才会生效。"
            },
            {
                "SAE_CONNECTION_STATE_DISCONNECTED": "2: 连接断开。"
            },
            {
                "SAE_CONNECTION_STATE_RECONNECTING": "3: 重新建立连接中。"
            },
            {
                "SAE_CONNECTION_STATE_RECONNECTED": "4: 已重新建立连接。"
            }
        ],
        "returns": "",
        "is_hide": true
    },
    {
        "id": "api_enablelocalaudio",
        "name": "enableLocalAudio",
        "description": "Enables/Disables the local audio capture.\nThe audio function is enabled by default. This method disables or re-enables the local audio function to stop or restart local audio capturing.\n This method does not affect receiving or playing the remote audio streams, and enableLocalAudio (false) is applicable to scenarios where the user wants to receive remote audio streams without sending any audio stream to other users in the channel.\n Once the local audio function is disabled or re-enabled, the SDK triggers the onLocalAudioStateChanged callback, which reports localAudioStreamStateStopped (0) or localAudioStreamStateRecording (1). This method is different from the muteLocalAudioStream method:\n enableLocalAudio: Disables/Re-enables the local audio capturing and processing. If you disable or re-enable local audio capturing using the enableLocalAudio method, the local user might hear a pause in the remote audio playback.\n muteLocalAudioStream: Sends/Stops sending the local audio streams. You can call this method either before or after joining a channel. Calling it before joining a channel only sets the device state, and it takes effect immediately after you join the channel.",
        "parameters": [
            {
                "enabled": " true: (Default) Re-enable the local audio function, that is, to start the local audio capturing device (for example, the microphone).\n false: Disable the local audio function, that is, to stop local audio capturing. "
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "api_startscreencapturebydisplayid",
        "name": "startScreenCaptureByDisplayId",
        "description": "Shares the screen by specifying the display ID.\nThere are two ways to start screen sharing, you can choose one according to the actual needs:\n Call this method before joining a channel, and then call joinChannelWithOptions to join a channel and set publishScreenTrack true to start screen sharing.\n Call this method after joining a channel, and then call updateChannelMediaOptions and set publishScreenTrack true to start screen sharing. This method shares a screen or part of the screen. You need to specify the ID of the screen to be shared in this method. This method applies to macOS only.",
        "parameters": [
            {
                "displayId": "The display ID of the screen to be shared. This parameter specifies which screen you want to share."
            },
            {
                "regionRect": "(Optional) Sets the relative location of the region to the screen. If you do not set this parameter, the SDK shares the whole screen. See Rectangle . If the specified region overruns the screen, the SDK shares only the region within it; if you set width or height as 0, the SDK shares the whole screen."
            },
            {
                "captureParams": "Screen sharing configurations. The default video dimension is 1920 x 1080, that is, 2,073,600 pixels. Agora uses the value of this parameter to calculate the charges. See ScreenCaptureParameters ."
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "api_icloudspatialaudioengine_setaudiorangemode",
        "name": "setAudioRangeMode",
        "description": "Sets the audio range mode.\nThe SDK supports two audio range modes: everyone mode and team mode. The SDK uses everyone mode by default. If you want to change to team mode, call this method.\n A user can only use one mode at a time in a room.\n You can call this method either before or after enterRoom , with the following differences:\n If you call this method before enterRoom, this method takes effect when entering the room.\n If you call this method after enterRoom, this method takes effect immediately and changes the current audio range mode.",
        "parameters": [
            {
                "rangeMode": "The audio range mode. "
            }
        ],
        "returns": "",
        "is_hide": true
    },
    {
        "id": "api_iscameraexposurepositionsupported",
        "name": "isCameraExposurePositionSupported",
        "description": "Checks whether the device supports manual exposure.\nThis method is for Android and iOS only.\n Call this method before calling joinChannelWithOptions , enableVideo , or enableLocalVideo , depending on which method you use to turn on your local camera.",
        "parameters": [],
        "returns": "true: The device supports manual exposure.\n false: The device does not support manual exposure.",
        "is_hide": false
    },
    {
        "id": "api_irtcengineex_sendcustomreportmessageex",
        "name": "sendCustomReportMessageEx",
        "description": "Agora supports reporting and analyzing customized messages.\nAgora supports reporting and analyzing customized messages. This function is in the beta stage with a free trial. The ability provided in its beta test version is reporting a maximum of 10 message pieces within 6 seconds, with each message piece not exceeding 256 bytes and each string not exceeding 100 bytes. To try out this function, contact and discuss the format of customized messages with us.",
        "parameters": [],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "api_startsecondaryscreencapture",
        "name": "startSecondaryScreenCapture",
        "description": "Starts sharing a secondary screen.\n",
        "parameters": [
            {
                "config": "The configuration of the captured screen. See ScreenCaptureConfiguration .\n "
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "callback_icloudspatialaudioeventhandler_onteammateleft",
        "name": "onTeammateLeft",
        "description": "Occurs when the user leaves the current team.\nWhen a remote user in the current team calls exitRoom to leave the current room, the local user receives this callback.",
        "parameters": [
            {
                "uid": "The user ID of the remote user who leaves the current team."
            }
        ],
        "returns": "",
        "is_hide": true
    },
    {
        "id": "api_setvideoencoderconfiguration",
        "name": "setVideoEncoderConfiguration",
        "description": "Sets the video encoder configuration.\nSets the encoder configuration for the local video.\n You can call this method either before or after joining a channel. If you don't need to set the video encoder configuration after joining a channel,\nAgora recommends you calling this method before the enableVideo method to reduce the rendering time of the first video frame.",
        "parameters": [
            {
                "config": "Video profile. See VideoEncoderConfiguration ."
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "api_iaudiodevicemanager_startaudiodeviceloopbacktest_ng",
        "name": "startAudioDeviceLoopbackTest",
        "description": "Starts an audio device loopback test.\nThis method tests whether the local audio capture device and playback device are working properly. Once the test starts, the audio recording device records the local audio, and the audio playback device plays the captured audio. The SDK triggers two independent onAudioVolumeIndication callbacks at the time interval set in this method, which reports the volume information of the capture device (uid = 0) and the volume information of the playback device (uid = 1) respectively. Ensure that you call this method before joining a channel.\n This method tests local audio devices and does not report the network conditions.",
        "parameters": [
            {
                "indicationInterval": "The time interval (ms) at which the SDK triggers the onAudioVolumeIndication callback. Agora recommends setting a value greater than 200 ms. This value must not be less than 10 ms; otherwise, you can not receive the onAudioVolumeIndication callback."
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "api_clearvideowatermarks",
        "name": "clearVideoWatermarks",
        "description": "Removes the watermark image from the video stream.\n",
        "parameters": [],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "callback_idirectcdnstreamingeventhandler_ondirectcdnstreamingstats",
        "name": "onDirectCdnStreamingStats",
        "description": "Reports the CDN streaming statistics.\nWhen the host directly pushes streams to the CDN, the SDK triggers this callback every one second.",
        "parameters": [
            {
                "stats": "The statistics of the current CDN streaming. See DirectCdnStreamingStats ."
            }
        ],
        "returns": "",
        "is_hide": true
    },
    {
        "id": "enum_saeconnectionchangedreasontype",
        "name": "SAE_CONNECTION_CHANGED_REASON_TYPE",
        "description": "SDK 与 Agora 空间音效服务器连接状态发生改变的原因。\n",
        "parameters": [
            {
                "SAE_CONNECTION_CHANGED_DEFAULT": "0: 正常。"
            },
            {
                "SAE_CONNECTION_CHANGED_CONNECTING": "1: SDK 建立连接中。"
            },
            {
                "SAE_CONNECTION_CHANGED_CREATE_ROOM_FAIL": "2: SDK 创建房间失败。"
            },
            {
                "SAE_CONNECTION_CHANGED_RTM_DISCONNECT": "3: SDK 与 RTM 系统连接中断。"
            },
            {
                "SAE_CONNECTION_CHANGED_RTM_ABORTED": "4: 用户被 RTM 系统踢出。"
            },
            {
                "SAE_CONNECTION_CHANGED_LOST_SYNC": "5: SDK 超过 15 秒未收到 Agora 空间音效服务器的消息。"
            }
        ],
        "returns": "",
        "is_hide": true
    },
    {
        "id": "class_remoteaudiostats",
        "name": "RemoteAudioStats",
        "description": "Audio statistics of the remote user.\n",
        "parameters": [
            {
                "uid": "The user ID of the remote user."
            },
            {
                "quality": "The quality of the audio stream sent by the user.  QualityType \n "
            },
            {
                "networkTransportDelay": "The network delay (ms) from the sender to the receiver."
            },
            {
                "jitterBufferDelay": "The network delay (ms) from the audio receiver to the jitter buffer. When the receiving end is an audience member andaudienceLatencyLevel of ClientRoleOptions is 1, this parameter does not take effect.\n "
            },
            {
                "audioLossRate": "The frame loss rate (%) of the remote audio stream in the reported interval."
            },
            {
                "numChannels": "The number of audio channels."
            },
            {
                "receivedSampleRate": "The sampling rate of the received audio stream in the reported interval."
            },
            {
                "receivedBitrate": "The average bitrate (Kbps) of the received audio stream in the reported interval."
            },
            {
                "totalFrozenTime": "The total freeze time (ms) of the remote audio stream after the remote user joins the channel. In a session, audio freeze occurs when the audio frame loss rate reaches 4%."
            },
            {
                "frozenRate": "The total audio freeze time as a percentage (%) of the total time when the audio is available. The audio is considered available when the remote user neither stops sending the audio stream nor disables the audio module after joining the channel."
            },
            {
                "totalActiveTime": "The total active time (ms) between the start of the audio call and the callback of the remote user.\n The active time refers to the total duration of the remote user without the mute state.\n "
            },
            {
                "publishDuration": "The total duration (ms) of the remote audio stream.\n "
            },
            {
                "qoeQuality": "The Quality of Experience (QoE) of the local user when receiving a remote audio stream. \n "
            },
            {
                "mosValue": "The quality of the remote audio stream in the reported interval. The quality is determined by the Agora real-time audio MOS (Mean Opinion Score) measurement method. The return value range is [0, 500]. Dividing the return value by 100 gets the MOS score, which ranges from 0 to 5. The higher the score, the better the audio quality.\n The subjective perception of audio quality corresponding to the Agora real-time audio MOS scores is as follows: MOS score\n Perception of audio quality Greater than 4\n Excellent. The audio sounds clear and smooth. From 3.5 to 4\n Good. The audio has some perceptible impairment but still sounds clear. From 3 to 3.5\n Fair. The audio freezes occasionally and requires attentive listening. From 2.5 to 3\n Poor. The audio sounds choppy and requires considerable effort to understand. From 2 to 2.5\n Bad. The audio has occasional noise. Consecutive audio dropouts occur, resulting in some information loss. The users can communicate only with difficulty. Less than 2\n Very bad. The audio has persistent noise. Consecutive audio dropouts are frequent, resulting in severe information loss. Communication is nearly impossible. "
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "class_rect",
        "name": "Rect",
        "description": "Updates the screen sharing region.\nDeprecated:\n 该类已废弃，请使用 updateScreenCaptureRegion 方法更新屏幕共享区域。",
        "parameters": [
            {
                "top": "已废弃。 共享区域顶部在纵轴上的坐标。"
            },
            {
                "left": "已废弃。 共享区域左侧在横轴上的坐标。"
            },
            {
                "bottom": "已废弃。 共享区域底部在纵轴上的坐标。"
            },
            {
                "right": "已废弃。 共享区域右侧在横轴上的坐标。"
            },
            {
                "x": "x: The horizontal offset from the top-left corner."
            },
            {
                "y": "y: The vertical offset from the top-left corner."
            },
            {
                "width": "The width of the target area."
            },
            {
                "height": "The height of the target area."
            }
        ],
        "returns": "",
        "is_hide": true
    },
    {
        "id": "api_enablewebsdkinteroperability",
        "name": "enableWebSdkInteroperability",
        "description": "Enables interoperability with the Agora Web SDK (applicable only in the live streaming scenarios).\nDeprecated:\n The SDK automatically enables interoperability with the Web SDK, so you no longer need to call this method. This method enables or disables interoperability with the Agora Web SDK. If the channel has Web SDK users, ensure that you call this method, or the video of the Native user will be a black screen for the Web user.\n This method is only applicable in live streaming scenarios, and interoperability is enabled by default in communication scenarios.",
        "parameters": [
            {
                "enabled": "Whether to enable interoperability with the Agora Web SDK.\n true: Enable interoperability.\n false: (Default) Disable interoperability.\n "
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "api_startrtmpstreamwithtranscoding",
        "name": "startRtmpStreamWithTranscoding",
        "description": "Starts Media Push and sets the transcoding configuration.\nYou can call this method to push a live audio-and-video stream to the specified CDN address and set the transcoding configuration. This method can push media streams to only one CDN address at a time, so if you need to push streams to multiple addresses, call this method multiple times.\n After you call this method, the SDK triggers the onRtmpStreamingStateChanged callback on the local client to report the state of the streaming. Call this method after joining a channel.\n Only hosts in the LIVE_BROADCASTING profile can call this method.\n If you want to retry pushing streams after a failed push, make sure to call stopRtmpStream first, then call this method to retry pushing streams; otherwise, the SDK returns the same error code as the last failed push.",
        "parameters": [
            {
                "url": "The address of Media Push. The format is RTMP or RTMPS. The character length cannot exceed 1024 bytes. Special characters such as Chinese characters are not supported."
            },
            {
                "transcoding": "The transcoding configuration for Media Push. See LiveTranscoding .\n "
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "class_irtcengineeventhandler",
        "name": "RtcEngineEventHandler",
        "description": "The SDK uses the RtcEngineEventHandler interface to send event notifications to your app. Your app can get those notifications through methods that inherit this interface.\n",
        "parameters": [],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "api_imediaplayer_registerplayersourceobserver",
        "name": "registerPlayerSourceObserver",
        "description": "Registers a media player observer.\n",
        "parameters": [
            {
                "observer": "The player observer, listening for events during the playback. See MediaPlayerSourceObserver ."
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "api_imediaplayer_getduration",
        "name": "getDuration",
        "description": "Gets the duration of the media resource.\n",
        "parameters": [],
        "returns": "The total duration (ms) of the media file.",
        "is_hide": false
    },
    {
        "id": "enum_videopixelformat",
        "name": "VideoPixelFormat",
        "description": "The video pixel format.\n",
        "parameters": [
            {
                "videoPixelI420": "1: The format is I420."
            },
            {
                "videoPixelBgra": "2: The format is BGRA."
            },
            {
                "videoPixelNv12": "8: The format is NV12."
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "api_irtcengineex_getconnectionstateex",
        "name": "getConnectionStateEx",
        "description": "Gets the current connection state of the SDK.\nYou can call this method either before or after joining a channel.",
        "parameters": [
            {
                "connection": "The connection information. See RtcConnection ."
            }
        ],
        "returns": "The current connection state.  ConnectionStateType",
        "is_hide": false
    },
    {
        "id": "enum_audioencodingtype",
        "name": "AudioEncodingType",
        "description": "Audio encoding type.\n",
        "parameters": [
            {
                "audioEncodingTypeAac16000Low": "AAC encoding format, 16000 Hz sampling rate, bass quality. A file with an audio duration of 10 minutes is approximately 1.2 MB after encoding."
            },
            {
                "audioEncodingTypeAac16000Medium": "AAC encoding format, 16000 Hz sampling rate, medium sound quality. A file with an audio duration of 10 minutes is approximately 2 MB after encoding."
            },
            {
                "audioEncodingTypeAac32000Low": "AAC encoding format, 32000 Hz sampling rate, bass quality. A file with an audio duration of 10 minutes is approximately 1.2 MB after encoding."
            },
            {
                "audioEncodingTypeAac32000Medium": "AAC encoding format, 32000 Hz sampling rate, medium sound quality. A file with an audio duration of 10 minutes is approximately 2 MB after encoding."
            },
            {
                "audioEncodingTypeAac32000High": "AAC encoding format, 32000 Hz sampling rate, high sound quality. A file with an audio duration of 10 minutes is approximately 3.5 MB after encoding."
            },
            {
                "audioEncodingTypeAac48000Medium": "AAC encoding format, 48000 Hz sampling rate, medium sound quality. A file with an audio duration of 10 minutes is approximately 2 MB after encoding."
            },
            {
                "audioEncodingTypeAac48000High": "AAC encoding format, 48000 Hz sampling rate, high sound quality. A file with an audio duration of 10 minutes is approximately 3.5 MB after encoding."
            },
            {
                "audioEncodingTypeOpus16000Low": "OPUS encoding format, 16000 Hz sampling rate, bass quality. A file with an audio duration of 10 minutes is approximately 2 MB after encoding."
            },
            {
                "audioEncodingTypeOpus16000Medium": "OPUS encoding format, 16000 Hz sampling rate, medium sound quality. A file with an audio duration of 10 minutes is approximately 2 MB after encoding."
            },
            {
                "audioEncodingTypeOpus48000Medium": "OPUS encoding format, 48000 Hz sampling rate, medium sound quality. A file with an audio duration of 10 minutes is approximately 2 MB after encoding."
            },
            {
                "audioEncodingTypeOpus48000High": "OPUS encoding format, 48000 Hz sampling rate, high sound quality. A file with an audio duration of 10 minutes is approximately 3.5 MB after encoding."
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "api_updatescreencaptureparameters",
        "name": "updateScreenCaptureParameters",
        "description": "Updates the screen sharing parameters.\n",
        "parameters": [
            {
                "captureParams": "The screen sharing encoding parameters. The default video dimension is 1920 x 1080, that is, 2,073,600 pixels. Agora uses the value of this parameter to calculate the charges. See ScreenCaptureParameters ."
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "api_stopscreencapture",
        "name": "stopScreenCapture",
        "description": "Stops screen sharing.\n",
        "parameters": [],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "api_setremotevoice3dposition",
        "name": "setRemoteVoice3DPosition",
        "description": "Sets the 3D position of the remote user's voice.\n通过设置远端用户声音的水平角、垂直角和声源距离，让远端用户的声音听起来有方位感。\n This method applies to massive multiplayer online games, such as Battle Royale games.\n 该方法和 setRemoteVoicePosition 的区别是：该方法设置声音的 3D 位置；setRemoteVoicePosition 设置声音的 2D 位置，即水平面上的位置。 使用该方法需要在加入频道前调用 enableSoundPositionIndication 开启远端用户声音的立体声。\n For the best voice positioning, Agora recommends using a wired headset.\n Call this method after joining a channel.",
        "parameters": [
            {
                "uid": "The user ID of the remote user."
            },
            {
                "azimuth": "水平角。 取值范围为 [0,360]，单位为度。 其中： 0:（默认）0 度，表示水平面的正前方。\n 90: 90 度，表示水平面的正左方。\n 180: 180 度，表示水平面的正右方。\n "
            },
            {
                "elevation": "垂直角。 取值范围为 [-90,90]，单位为度。 其中： 0:（默认）0 度，表示水平面无旋转。\n -90: -90 度，表示水平面向下旋转 90 度。\n 90: 90 度，表示水平面向上旋转 90 度。\n "
            },
            {
                "distance": "声源距离。 取值不能小于 0.5，单位为米。 为避免设置距离过大导致声音较小，Agora 推荐设置范围为 [0.5,50]。"
            }
        ],
        "returns": "0: Success.\n < 0: Failure.",
        "is_hide": true
    },
    {
        "id": "enum_contentinspectresult",
        "name": "CONTENT_INSPECT_RESULT",
        "description": "鉴黄结果。\n",
        "parameters": [
            {
                "CONTENT_INSPECT_NEUTRAL": "1：正常图片。"
            },
            {
                "CONTENT_INSPECT_SEXY": "2：性感图片。"
            },
            {
                "CONTENT_INSPECT_PORN": "3：色情图片。"
            }
        ],
        "returns": "",
        "is_hide": true
    },
    {
        "id": "api_removehandler",
        "name": "unregisterEventHandler",
        "description": "Removes the specified IRtcEngineEventHandler instance.\nThis method removes the specified callback handler. For callback events that you want to listen for only once, call this method to remove the relevant callback handler after you have received them.",
        "parameters": [
            {
                "eventHandler": "The callback handler to be deleted. See RtcEngineEventHandler ."
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "callback_onstreamunpublished",
        "name": "onStreamUnpublished",
        "description": "Occurs when the media push stops.\nDeprecated:\n Please use onRtmpStreamingStateChanged instead.",
        "parameters": [
            {
                "url": "Removes an RTMP or RTMPS URL of the media push."
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "enum_remoteaudiostate",
        "name": "RemoteAudioState",
        "description": "Remote audio states.\n",
        "parameters": [
            {
                "remoteAudioStateStopped": "0: The remote audo is in the initial state. The SDK reports this state in the case of remoteAudioReasonLocalMuted, remoteAudioReasonRemoteMuted, or remoteAudioReasonRemoteOffline."
            },
            {
                "remoteAudioStateStarting": "1: The first remote audio packet is received."
            },
            {
                "remoteAudioStateDecoding": "2: The remote audio stream is decoded and plays normally. The SDK reports this state in the case of remoteAudioReasonNetworkRecovery, remoteAudioReasonLocalUnmuted, or remoteAudioReasonRemoteUnmuted."
            },
            {
                "remoteAudioStateFrozen": "3: The remote audio is frozen. The SDK reports this state in the case of remoteAudioReasonNetworkCongestion."
            },
            {
                "remoteAudioStateFailed": "4: The remote audio fails to start. The SDK reports this state in the case of remoteAudioReasonInternal."
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "api_joinchannel2_ng",
        "name": "joinChannelWithOptions",
        "description": "Joins a channel with media options.\nThis method enables users to join a channel. Users in the same channel can talk to each other, and multiple users in the same channel can start a group chat. Users with different App IDs cannot call each other.\n A successful call of this method triggers the following callbacks: \n The local client: The onJoinChannelSuccess and onConnectionStateChanged callbacks.\n The remote client: onUserJoined , if the user joining the channel is in the Communication profile or is a host in the Live-broadcasting profile. When the connection between the client and Agora's server is interrupted due to poor network conditions, the SDK tries reconnecting to the server. When the local client successfully rejoins the channel, the SDK triggers the onRejoinChannelSuccess callback on the local client. This method allows users to join only one channel at a time.\n Ensure that the app ID you use to generate the token is the same app ID that you pass in the initialize method; otherwise, you may fail to join the channel by token.",
        "parameters": [
            {
                "token": "The token generated on your server for authentication. See \n "
            },
            {
                "channelId": "The channel name. This parameter signifies the channel in which users engage in real-time audio and video interaction. Under the premise of the same App ID, users who fill in the same channel ID enter the same channel for audio and video interaction. The string length must be less than 64 bytes. Supported characters:\n All lowercase English letters: a to z.\n All uppercase English letters: A to Z.\n All numeric characters: 0 to 9.\n Space\n \"!\", \"#\", \"$\", \"%\", \"&amp;\", \"(\", \")\", \"+\", \"-\", \":\", \";\", \"&lt;\", \"= \", \".\", \"&gt;\", \"?\", \"@\", \"[\", \"]\", \"^\", \"_\", \"{\", \"}\", \"|\", \"~\", \",\" "
            },
            {
                "uid": "The user ID. This parameter is used to identify the user in the channel for real-time audio and video interaction. You need to set and manage user IDs yourself, and ensure that each user ID in the same channel is unique. This parameter is a 32-bit unsigned integer. The value range is 1 to 232-1. If the user ID is not assigned (or set to 0), the SDK assigns a random user ID and returns it in the onJoinChannelSuccess callback. Your application must record and maintain the returned user ID, because the SDK does not do so."
            },
            {
                "options": "The channel media options. See ChannelMediaOptions .\n "
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "api_imediaplayer_play",
        "name": "play",
        "description": "Plays the media file.\nAfter calling open or seek, you can call this method to play the media file.",
        "parameters": [],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "callback_onfirstremotevideodecoded",
        "name": "onFirstRemoteVideoDecoded",
        "description": "Occurs when the first remote video frame is received and decoded.\nThe SDK triggers this callback under one of the following circumstances:\n The remote user joins the channel and sends the video stream.\n The remote user stops sending the video stream and re-sends it after 15 seconds. Reasons for such an interruption include:\n The remote user leaves the channel.\n The remote user drops offline.\n The remote user calls muteLocalVideoStream to stop sending the video stream.\n The remote user calls disableVideo to disable video.",
        "parameters": [
            {
                "connection": "The connection information. See RtcConnection ."
            },
            {
                "remoteUid": "The ID of the remote user sending the video stream."
            },
            {
                "width": "The width (px) of the video stream."
            },
            {
                "height": "The height (px) of the video stream."
            },
            {
                "elapsed": "The time elapsed (ms) from the local user calling joinChannelWithOptions until the SDK triggers this callback."
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "enum_audioprofiletype",
        "name": "AudioProfileType",
        "description": "The audio profile.\n",
        "parameters": [
            {
                "audioProfileDefault": "0: The default audio profile.\n For the interactive streaming profile: A sample rate of 48 kHz, music encoding, mono, and a bitrate of up to 64 Kbps.\n For the communication profile: \n Windows: A sample rate of 16 kHz, audio encoding, mono, and a bitrate of up to 16 Kbps.\n Android/macOS/iOS: "
            },
            {
                "audioProfileSpeechStandard": "1: A sample rate of 32 kHz, audio encoding, mono, and a bitrate of up to 18 Kbps."
            },
            {
                "audioProfileMusicStandard": "2: A sample rate of 48 kHz, music encoding, mono, and a bitrate of up to 64 Kbps."
            },
            {
                "audioProfileMusicStandardStereo": "3: A sample rate of 48 kHz, music encoding, stereo, and a bitrate of up to 80 Kbps.To implement stereo audio, you also need to call setAdvancedAudioOptions and set audioProcessingChannels to AdvancedAudioOptions in audioProcessingStereo.\n "
            },
            {
                "audioProfileMusicHighQuality": "4: A sample rate of 48 kHz, music encoding, mono, and a bitrate of up to 96 Kbps."
            },
            {
                "audioProfileMusicHighQualityStereo": "5: A sample rate of 48 kHz, music encoding, stereo, and a bitrate of up to 128 Kbps.To implement stereo audio, you also need to call setAdvancedAudioOptions and set audioProcessingChannels to AdvancedAudioOptions in audioProcessingStereo.\n "
            },
            {
                "audioProfileIot": ""
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "callback_onlocalaudiostats",
        "name": "onLocalAudioStats",
        "description": "Reports the statistics of the local audio stream.\nThe SDK triggers this callback once every two seconds.",
        "parameters": [
            {
                "connection": "The connection information. See RtcConnection ."
            },
            {
                "stats": "Local audio statistics. See LocalAudioStats ."
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "api_release",
        "name": "release",
        "description": "Releases the RtcEngine instance.\nThis method releases all resources used by the Agora SDK. Use this method for apps in which users occasionally make voice or video calls. When users do not make calls, you can free up resources for other operations.\n After a successful method call, you can no longer use any method or callback in the SDK anymore. If you want to use the real-time communication functions again, you must call createAgoraRtcEngine and initialize to create a new RtcEngine instance.\n If you want to create a new RtcEngine instance after destroying the current one, ensure that you wait till the release method execution to complete.",
        "parameters": [
            {
                "sync": "true: Synchronous call. Agora suggests calling this method in a sub-thread to avoid congestion in the main thread because the synchronous call and the app cannot move on to another task until the resources used by RtcEngine are released. Besides, you cannot call release in any method or callback of the SDK. Otherwise, the SDK cannot release the resources until the callbacks return results, which may result in a deadlock. The SDK automatically detects the deadlock and converts this method into an asynchronous call, causing the test to take additional time.\n false: Asynchronous call. The app can move on to another task, no matter the resources used by RtcEngine are released or not. Do not immediately uninstall the SDK's dynamic library after the call; otherwise, it may cause a crash due to the SDK clean-up thread not quitting.\n "
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "class_directcdnstreamingmediaoptions",
        "name": "DirectCdnStreamingMediaOptions",
        "description": "The media setting options for the host.\n",
        "parameters": [
            {
                "publishCameraTrack": "Set whether to publish the video captured by the camera.\n true: Publish the video captured by the camera.\n false: (default) Do not publish video captured by the camera.\n "
            },
            {
                "publishMicrophoneTrack is set as ": "Set whether to publish the audio captured by the microphone.\n true: Publish the audio captured by the microphone.\n false: (default) Do not publish audio captured by the microphone.\n "
            },
            {
                "publishCustomAudioTrack": "Set whether to publish custom captured audio.\n true: Publish custom captured audio.\n false: (default) Do not publish custom captured audio.\n "
            },
            {
                "publishCustomVideoTrack": "Set whether to publish custom captured videos.\n true: Publish custom captured videos.\n false: (default) Do not publish custom captured videos.\n "
            }
        ],
        "returns": "",
        "is_hide": true
    },
    {
        "id": "api_imediaplayer_adjustpublishsignalvolume",
        "name": "adjustPublishSignalVolume",
        "description": "Adjusts the volume of the media file for publishing.\nAfter connected to the Agora server, you can call this method to adjust the volume of the media file heard by the remote user.",
        "parameters": [
            {
                "volume": "The volume, which ranges from 0 to 400:\n 0: Mute.\n 100: (Default) The original volume.\n 400: Four times the original volume (amplifying the audio signals by four times).\n "
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "enum_maxdeviceidlengthtype",
        "name": "MaxDeviceIdLengthType",
        "description": "The maximum length of the device ID.\n",
        "parameters": [
            {
                "maxDeviceIdLength": "The maximum length of the device ID is 512 bytes."
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "api_setvoicebeautifierpreset",
        "name": "setVoiceBeautifierPreset",
        "description": "Sets a preset voice beautifier effect.\nCall this method to set a preset voice beautifier effect for the local user who sends an audio stream. After setting a voice beautifier effect, all users in the channel can hear the effect. You can set different voice beautifier effects for different scenarios. \n For better voice effects, Agora recommends that you call setAudioProfile and set scenario to audioScenarioGameStreaming(3) and profile to audioProfileMusicHighQuality(4) or audioProfileMusicHighQualityStereo(5) before calling this method. You can call this method either before or after joining a channel.\n Do not set the profile parameter in setAudioProfile to audioProfileSpeechStandard(1) or audioProfileIot(6), or the method does not take effect.\n This method works best with the human voice. Agora does not recommend using this method for audio containing music.\n After calling setVoiceBeautifierPreset , Agora recommends not calling the following methods, because they can override settings in setVoiceBeautifierPreset: \n setAudioEffectPreset \n setAudioEffectParameters \n setLocalVoicePitch \n setLocalVoiceEqualization \n setLocalVoiceReverb \n setVoiceBeautifierParameters \n setVoiceConversionPreset",
        "parameters": [
            {
                "preset": "The preset voice beautifier effect options: VoiceBeautifierPreset .\n "
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "class_channelmediaoptions_ng",
        "name": "ChannelMediaOptions",
        "description": "The channel media options.\nAgora supports publishing multiple audio streams and one video stream at the same time and in the same RtcConnection . For example, publishAudioTrack, publishCustomAudioTrack and publishMediaPlayerAudioTrack can be true at the same time; but only one of publishCameraTrack, publishScreenTrack, publishCustomVideoTrack, and publishEncodedVideoTrack can be true at the same time.",
        "parameters": [
            {
                "publishCameraTrack": "Whether to publish the video captured by the camera:\n true: (Default) Publish the video captured by the camera.\n false: Do not publish the video captured by the camera.\n "
            },
            {
                "publishSecondaryCameraTrack": "Whether to publish the video captured by the second camera:\n true: (Default) Publish the video captured by the second camera.\n false: Do not publish the video captured by the second camera.\n "
            },
            {
                "publishAudioTrack": "Whether to publish the captured audio:\n true: (Default) Publish the captured audio.\n false: Do not publish the captured audio.\n "
            },
            {
                "publishScreenTrack": "Whether to publish the captured video from the screen:\n true: Publish the captured video from the screen.\n false: (Default) Do not publish the captured video from the screen. "
            },
            {
                "publishSecondaryScreenTrack": "Whether to publish the captured video from the secondary screen:\n true: Publish the captured video from the second screen.\n false: (Default) Do not publish the captured video from the second screen. "
            },
            {
                "publishTrancodedVideoTrack": "Whether to publish the local transcoded video.\n true: Publish the local transcoded video.\n false: (Default) Do not publish the local transcoded video. "
            },
            {
                "publishCustomAudioTrack": "Whether to publish the captured audio from a custom source:\n true: Publish the captured audio from a custom source.\n false: (Default) Do not publish the captured audio from the custom source.\n "
            },
            {
                "publishCustomAudioSourceId": "The ID of the custom audio source to publish. The default value is 0."
            },
            {
                "publishCustomAudioTrackEnableAec": "Whether to enable AEC when publishing the captured audio from a custom source:\n true Enable AEC when publishing the captured audio from a custom source.\n false : (Default) Do not enable AEC when publishing the captured audio from a custom source.\n "
            },
            {
                "publishCustomVideoTrack": "Whether to publish the captured video from a custom source:\n true: Publish the captured video from a custom source.\n false: (Default) Do not publish the captured video from the custom source.\n "
            },
            {
                "publishEncodedVideoTrack": "Whether to publish the encoded video:\n true: Publish the encoded video.\n false: (Default) Do not publish the encoded video.\n "
            },
            {
                "publishMediaPlayerAudioTrack": "Whether to publish the audio from the media player:\n true: Publish the audio from the media player.\n false: (Default) Do not publish the audio from the media player.\n "
            },
            {
                "publishMediaPlayerVideoTrack": "Whether to publish the video from the media player:\n true: Publish the video from the media player.\n false: (Default) Do not publish the video from the media player.\n "
            },
            {
                "autoSubscribeAudio": "Whether to automatically subscribe to all remote audio streams when the user joins a channel:\n true: (Default) Subscribe to all remote audio streams.\n false: Do not subscribe to any remote audio stream.\n "
            },
            {
                "autoSubscribeVideo": "Whether to subscribe to all remote video streams when the user joins the channel:\n true: (Default) Subscribe to all remote video streams.\n false: Do not subscribe to any remote video stream.\n "
            },
            {
                "enableAudioRecordingOrPlayout": "Whether to enable audio capturing or playback.\n true: (Default) Enable audio capturing and playback.\n false Do not enable audio capturing or playback.\n "
            },
            {
                "publishMediaPlayerId": "The ID of the media player to be published. The default value is 0."
            },
            {
                "clientRoleType": ""
            },
            {
                "defaultVideoStreamType": "The default video-stream type, see VideoStreamType .\n "
            },
            {
                "channelProfile": "The channel profile. See ChannelProfileType .\n "
            },
            {
                "audioDelayMs": "The delay in ms for sending audio frames. This is used for explicit control of A/V sync.\n To switch off the delay, set the value to 0.\n "
            },
            {
                "token": "(Optional) The token generated on your server for authentication. This parameter takes effect only when calling updateChannelMediaOptions or updateChannelMediaOptionsEx .\n Ensure that the App ID, channel name, and user name used for creating the token are the same ones as those used by the initialize method for initializing the RTC engine, and those used by the joinChannelWithOptions and joinChannelEx methods for joining the channel.\n "
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "enum_audioeffectpreset",
        "name": "AudioEffectPreset",
        "description": "Preset voice effects.\nFor better voice effects, Agora recommends settingthe profile parameter of setAudioProfile to audioProfileMusicHighQuality or audioProfileMusicHighQualityStereo before using the following presets: roomAcousticsKtv\n roomAcousticsVocalConcert\n roomAcousticsStudio\n roomAcousticsPhonograph\n roomAcousticsSpacial\n roomAcousticsEthereal\n voiceChangerEffectUncle\n voiceChangerEffectOldman\n voiceChangerEffectBoy\n voiceChangerEffectSister\n voiceChangerEffectGirl\n voiceChangerEffectPigking\n voiceChangerEffectHulk\n pitchCorrection",
        "parameters": [
            {
                "audioEffectOff": "Turn off voice effects, that is, use the original voice."
            },
            {
                "roomAcousticsKtv": "The voice effect typical of a KTV venue."
            },
            {
                "roomAcousticsVocalConcert": "The voice effect typical of a concert hall."
            },
            {
                "roomAcousticsStudio": "The voice effect typical of a recording studio."
            },
            {
                "roomAcousticsPhonograph": "The voice effect typical of a vintage phonograph."
            },
            {
                "roomAcousticsVirtualStereo": "The virtual stereo effect, which renders monophonic audio as stereo audio.\n Before using this preset, set the profile parameter of setAudioProfile to audioProfileMusicHighQuality or audioProfileMusicHighQualityStereo; otherwise, the preset setting is invalid. "
            },
            {
                "roomAcousticsSpacial": "A more spatial voice effect."
            },
            {
                "roomAcousticsEthereal": "A more ethereal voice effect."
            },
            {
                "roomAcoustics3dVoice": "A 3D voice effect that makes the voice appear to be moving around the user. The default movement cycle is 10 seconds. After setting this effect, you can call to setAudioEffectParameters modify the movement period. Before using this preset, set the profile parameter of setAudioProfile to audioProfileMusicStandardStereo or audioProfileMusicHighQualityStereo; otherwise, the preset setting is invalid.\n If the 3D voice effect is enabled, users need to use stereo audio playback devices to hear the anticipated voice effect.\n "
            },
            {
                "voiceChangerEffectUncle": "A middle-aged man's voice.\n Agora recommends using this preset to process a male-sounding voice; otherwise, you may not hear the anticipated voice effect. "
            },
            {
                "voiceChangerEffectOldman": "A senior man's voice.\n Agora recommends using this preset to process a male-sounding voice; otherwise, you may not hear the anticipated voice effect. "
            },
            {
                "voiceChangerEffectBoy": "A boy's voice.\n Agora recommends using this preset to process a male-sounding voice; otherwise, you may not hear the anticipated voice effect. "
            },
            {
                "voiceChangerEffectSister": "A young woman's voice.\n Agora recommends using this preset to process a female-sounding voice; otherwise, you may not hear the anticipated voice effect. "
            },
            {
                "voiceChangerEffectGirl": "A girl's voice.\n Agora recommends using this preset to process a female-sounding voice; otherwise, you may not hear the anticipated voice effect. "
            },
            {
                "voiceChangerEffectPigking": "The voice of Pig King, a character in Journey to the West who has a voice like a growling bear."
            },
            {
                "voiceChangerEffectHulk": "The Hulk's voice."
            },
            {
                "styleTransformationRnb": "The voice effect typical of R&B music.\n Before using this preset, set the profile parameter of setAudioProfile to audioProfileMusicHighQuality or audioProfileMusicHighQualityStereo; otherwise, the preset setting is invalid. "
            },
            {
                "styleTransformationPopular": "The voice effect typical of popular music.\n Before using this preset, set the profile parameter of setAudioProfile to audioProfileMusicHighQuality or audioProfileMusicHighQualityStereo; otherwise, the preset setting is invalid. "
            },
            {
                "pitchCorrection": "A pitch correction effect that corrects the user's pitch based on the pitch of the natural C major scale. After setting this voice effect, you can call setAudioEffectParameters to adjust the basic mode of tuning and the pitch of the main tone."
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "api_setscreencaptureorientation",
        "name": "setScreenCaptureOrientation",
        "description": "Since\n v",
        "parameters": [
            {
                "null": ""
            }
        ],
        "returns": "",
        "is_hide": true
    },
    {
        "id": "api_imediaplayer_registervideoframeobserver",
        "name": "registerVideoFrameObserver",
        "description": "Registers a video frame observer object.\n",
        "parameters": [
            {}
        ],
        "returns": "",
        "is_hide": true
    },
    {
        "id": "class_videocanvas_ng",
        "name": "VideoCanvas",
        "description": "Attributes of video canvas object.\n",
        "parameters": [
            {
                "view": "Video display window."
            },
            {
                "renderMode": "The rendering mode of the video. See RenderModeType .\n "
            },
            {
                "mirrorMode": "The mirror mode of the view. See VideoMirrorModeType . For the mirror mode of the local video view: If you use a front camera, the SDK enables the mirror mode by default; if you use a rear camera, the SDK disables the mirror mode by default.\n For the remote user: The mirror mode is disabled by default.\n "
            },
            {
                "uid": "The user ID."
            },
            {
                "sourceType": "The type of the video source, see VideoSourceType ."
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "class_ilocalspatialaudioengine",
        "name": "ILocalSpatialAudioEngine",
        "description": "This class calculates user positions through the SDK to implement the spatial audio effect.\nThis class inherits from IBaseSpatialAudioEngine . Before calling other APIs in this class, you need to call the initialize method to initialize this class.",
        "parameters": [],
        "returns": "",
        "is_hide": true
    },
    {
        "id": "api_muteremoteaudiostream",
        "name": "muteRemoteAudioStream",
        "description": "Cancels or resumes subscribing to the specified remote user's audio stream.\nCall this method after joining a channel.",
        "parameters": [
            {
                "uid": "The user ID of the specified user."
            },
            {
                "mute": "Whether to stop subscribing to the audio stream of the specified user. true: Unsubscribe from the specified user's audio stream.\n false: (Default) Subscribes to the specified user's audio stream. "
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "class_encryptionconfig",
        "name": "EncryptionConfig",
        "description": "Built-in encryption configurations.\n",
        "parameters": [
            {
                "encryptionMode": "The built-in encryption mode. See EncryptionMode . Agora recommends using aes128Gcm2 or aes256Gcm2 encrypted mode. These two modes support the use of salt for higher security.\n "
            },
            {
                "encryptionKey": "Encryption key in string type with unlimited length. Agora recommends using a 32-byte key.\n If you do not set an encryption key or set it as NULL, you cannot use the built-in encryption, and the SDK returns -2. "
            },
            {
                "encryptionKdfSalt": "Salt, 32 bytes in length. Agora recommends that you use OpenSSL to generate salt on the server side. See Media Stream Encryption for details. \n This parameter takes effect only in aes128Gcm2 or aes256Gcm2 encrypted mode. In this case, ensure that this parameter is not 0.\n "
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "api_switchchannel",
        "name": "",
        "description": "Switches to a different channel.\nWhen the viewers in the live channel want to switch from one channel to another, this method can be called to achieve fast switching.\n After the user successfully switches to another channel, the onLeaveChannel and onJoinChannelSuccess callbacks are triggered to indicate that the user has left the original channel and joined a new one.\n Once the user switches to another channel, the user subscribes to the audio and video streams of all the other users in the channel by default, giving rise to usage and billing calculation. To stop subscribing to a specified stream or all remote streams, call the corresponding mute methods.",
        "parameters": [
            {
                "token": "The token generated at your server.\n In scenarios with low security requirements, token is optional and can be set as NULL.\n In scenarios with high security requirements, set the value to the token generated from your server. If you enable the App Certificate, you must use a token to join the channel. Ensure that the App ID, channel name, and user name used for creating the token are the same ones as those used by the initialize method for initializing the RTC engine, and those used by the and methods for joining the channel. \n "
            },
            {
                "null": "The channel name. This parameter signifies the channel in which users engage in real-time audio and video interaction. Under the premise of the same App ID, users who fill in the same channel ID enter the same channel for audio and video interaction. The string length must be less than 64 bytes. Supported characters:\n All lowercase English letters: a to z.\n All uppercase English letters: A to Z.\n All numeric characters: 0 to 9.\n Space\n \"!\", \"#\", \"$\", \"%\", \"&amp;\", \"(\", \")\", \"+\", \"-\", \":\", \";\", \"&lt;\", \"= \", \".\", \"&gt;\", \"?\", \"@\", \"[\", \"]\", \"^\", \"_\", \"{\", \"}\", \"|\", \"~\", \",\" "
            }
        ],
        "returns": "0: Success.\n < 0: Failure.\n -1: A general error occurs (no specified reason).\n -2: The parameter is invalid.\n -5: The request is rejected. Probably because the user is not an audience member.\n -7: The SDK is not initialized.\n -102: The channel name is invalid. Please use a valid channel name.\n -113: The user is not in the channel.",
        "is_hide": true
    },
    {
        "id": "api_iaudiodevicemanager_stoprecordingdevicetest",
        "name": "stopRecordingDeviceTest",
        "description": "Stops the audio capture device test.\nThis method stops the audio capture device test. You must call this method to stop the test after calling the startRecordingDeviceTest method.\n Ensure that you call this method before joining a channel.",
        "parameters": [],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "api_setenablespeakerphone_ng",
        "name": "setEnableSpeakerphone",
        "description": "Enables/Disables the speakerphone temporarily.\nThis method is for Android and iOS only.\n After a successful method call, the SDK triggers the onAudioRoutingChanged callback.\n You can call this method before joining a channel, when in a channel, or after leaving a channel. However, Agora recommends calling this method only when you are in a channel to change the audio route temporarily. If you do not have a clear requirement for transient settings, Agora recommends calling the steady API setDefaultAudioRouteToSpeakerphone to set the audio route.\n Any user behavior or audio-related API call might change the transient setting of setEnableSpeakerphone.",
        "parameters": [
            {
                "speakerOn": "Whether to set the speakerphone as the default audio route:\n true: Set the speakerphone as the audio route temporarily.\n false: Do not set the speakerphone as the audio route. "
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "callback_onuploadlogresult",
        "name": "onUploadLogResult",
        "description": "Reports the result of uploading the SDK log files.\n调用 \n uploadLogFile \nAfter is called, the SDK triggers the callback to report the result of uploading the SDK log files. 如果上传失败，请参考\nreason\n排查问题。",
        "parameters": [
            {
                "connection": "The connection information. See RtcConnection ."
            },
            {
                "requestId": "The request ID. 该请求 ID 与\nuploadLogFile\n 中返回的\nrequestId\nthe object file name. You can use\nrequestId\n将特定的上传和回调对应起来。\n "
            },
            {
                "success": "Whether the log file is uploaded successfully:\n true : Successfully upload the log files.\n false : Fails to upload the log files. 失败的原因详见\nreason\n参数。 "
            },
            {
                "reason": "The reason for the upload failure. See \n UploadErrorReason \n 。\n "
            }
        ],
        "returns": "",
        "is_hide": true
    },
    {
        "id": "api_ibasespatialaudioengine_updateplayerpositioninfo",
        "name": "updatePlayerPositionInfo",
        "description": "Updates the spatial position of the media player.\nAfter a successful update, the local user can hear the change in the spatial position of the media player.",
        "parameters": [
            {
                "playerId": "The media player ID, which can be obtained in getMediaPlayerId ."
            },
            {
                "positionInfo": "The spatial position of the media player. See RemoteVoicePositionInfo for details."
            }
        ],
        "returns": "",
        "is_hide": true
    },
    {
        "id": "enum_audiocodecprofiletype",
        "name": "AudioCodecProfileType",
        "description": "Self-defined audio codec profile.\n",
        "parameters": [
            {
                "audioCodecProfileLcAac": "0: (Default) LC-AAC."
            },
            {
                "audioCodecProfileHeAac": "1: HE-AAC."
            },
            {
                "audioCodecProfileHeAacV2": "2: HE-AAC v2."
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "api_getvolumeofeffect",
        "name": "getVolumeOfEffect",
        "description": "Gets the volume of a specified audio effect.\n",
        "parameters": [
            {
                "soundId": "The ID of the audio effect."
            }
        ],
        "returns": "≥ 0: The volume of the specified audio effect, if the method call succeeds. The value range is [0,100]. 100 represents the original volume.\n < 0: Failure.",
        "is_hide": false
    },
    {
        "id": "callback_onpermissionerror",
        "name": "onPermissionError",
        "description": "Occurs when the SDK cannot get the device permission.\nWhen the SDK fails to get the device permission, the SDK triggers this callback to report which device permission cannot be got.\n This method is for Android and iOS only.",
        "parameters": [
            {
                "permissionType": "The type of the device permission. See PermissionType ."
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "api_adjustloopbackrecordingvolume",
        "name": "adjustLoopbackRecordingVolume",
        "description": "Adjusts the volume of the signal captured by the sound card.\nAfter calling enableLoopbackRecording to enable loopback audio capturing, you can call this method to adjust the volume of the signal captured by the sound card.",
        "parameters": [
            {
                "volume": "Audio mixing volume. The value ranges between 0 and 100. The default value is 100, the original volume."
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "class_transcodinguser",
        "name": "TranscodingUser",
        "description": "Transcoding configurations of each host.\n",
        "parameters": [
            {
                "uid": "The user ID of the host.\n "
            },
            {
                "x": "The x coordinate (pixel) of the host's video on the output video frame (taking the upper left corner of the video frame as the origin). The value range is [0, width], where width is thewidth set in LiveTranscoding .\n "
            },
            {
                "y": "The y coordinate (pixel) of the host's video on the output video frame (taking the upper left corner of the video frame as the origin). The value range is [0, height], where height is the height set in LiveTranscoding ."
            },
            {
                "width": "The width (pixel) of the host's video."
            },
            {
                "height": "The height (pixel) of the host's video.\n "
            },
            {
                "zOrder": "The number of the layer to which the video for the video mixing on the local client belongs. The value range is [0,100].\n 0: (Default) The layer is at the bottom.\n 100: The layer is at the top. \n If the value is less than 0 or greater than 100, the error ERR_INVALID_ARGUMENT is returned.\n Starting from v2.3, setting zOrder to 0 is supported.\n "
            },
            {
                "alpha": "The transparency of the video for the video mixing on the local client. The value range is [0.0,1.0].\n 0.0: Completely transparent.\n 1.0: (Default) Opaque. "
            },
            {
                "audioChannel": "The audio channel used by the host's audio in the output audio. The default value is 0, and the value range is [0, 5].\n 0: (Recommended) The defaut setting, which supports dual channels at most and depends on the upstream of the host.\n 1: The host's audio uses the FL audio channel. If the host's upstream uses multiple audio channels, the Agora server mixes them into mono first.\n 2: The host's audio uses the FC audio channel. If the host's upstream uses multiple audio channels, the Agora server mixes them into mono first.\n 3: The host's audio uses the FR audio channel. If the host's upstream uses multiple audio channels, the Agora server mixes them into mono first.\n 4: The host's audio uses the BL audio channel. If the host's upstream uses multiple audio channels, the Agora server mixes them into mono first.\n 5: The host's audio uses the BR audio channel. If the host's upstream uses multiple audio channels, the Agora server mixes them into mono first.\n 0xFF or a value greater than 5: The host's audio is muted, and the Agora server removes the host's audio. If the value is not 0, a special player is required.\n "
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "enum_networktype",
        "name": "NetworkType",
        "description": "Network types.\n",
        "parameters": [
            {
                "networkTypeUnknown": "-1: The network type is unknown.</pd>\n"
            },
            {
                "networkTypeDisconnected": "0: The SDK disconnects from the network."
            },
            {
                "networkTypeLan": "1: The network type is LAN."
            },
            {
                "networkTypeWifi": "2: The network type is Wi-Fi (including hotspots)."
            },
            {
                "networkTypeMobile2g": "3: The network type is mobile 2G."
            },
            {
                "networkTypeMobile3g": "4: The network type is mobile 3G."
            },
            {
                "networkTypeMobile4g": "5: The network type is mobile 4G."
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "api_stopaudioframedump",
        "name": "stopAudioFrameDump",
        "description": "",
        "parameters": [
            {
                "null": ""
            }
        ],
        "returns": "",
        "is_hide": true
    },
    {
        "id": "class_externalvideoframe_ng",
        "name": "ExternalVideoFrame",
        "description": "The external video frame.\n",
        "parameters": [
            {
                "format": ""
            },
            {
                "stride": "Line spacing of the incoming video frame, which must be in pixels instead of bytes. For textures, it is the width of the texture."
            },
            {
                "height": "Height of the incoming video frame."
            },
            {
                "textureId": "This parameter only applies to video data in Texture format. Incoming 4 x 4 transformational matrix. The typical value is a unit matrix."
            },
            {
                "cropLeft": "Raw data related parameter. The number of pixels trimmed from the left. The default value is 0."
            },
            {
                "cropTop": "Raw data related parameter. The number of pixels trimmed from the top. The default value is 0."
            },
            {
                "cropRight": "Raw data related parameter. The number of pixels trimmed from the right. The default value is 0."
            },
            {
                "cropBottom": "Raw data related parameter. The number of pixels trimmed from the bottom. The default value is 0."
            },
            {
                "rotation": ""
            }
        ],
        "returns": "",
        "is_hide": true
    },
    {
        "id": "callback_onchannelmediarelaystatechanged",
        "name": "onChannelMediaRelayStateChanged",
        "description": "Occurs when the state of the media stream relay changes.\nThe SDK returns the state of the current media relay with any error message.",
        "parameters": [
            {
                "state": "The state code. See ChannelMediaRelayState .\n "
            },
            {
                "code": "The error code of the channel media relay. See ChannelMediaRelayError .\n "
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "api_setremotevoiceposition",
        "name": "setRemoteVoicePosition",
        "description": "Sets the 2D position (the position on the horizontal plane) of the remote user's voice.\nThis method sets the 2D position and volume of a remote user, so that the local user can easily hear and identify the remote user's position.\n When the local user calls this method to set the voice position of a remote user, the voice difference between the left and right channels allows the local user to track the real-time position of the remote user, creating a sense of space. This method applies to massive multiplayer online games, such as Battle Royale games. For this method to work, enable stereo panning for remote users by calling the enableSoundPositionIndication method before joining a channel.\n For the best voice positioning, Agora recommends using a wired headset.\n Call this method after joining a channel.",
        "parameters": [
            {
                "uid": "The user ID of the remote user."
            },
            {
                "pan": "The voice position of the remote user. The value ranges from -1.0 to 1.0:\n 0.0: (Default) The remote voice comes from the front.\n -1.0: The remote voice comes from the left.\n 1.0: The remote voice comes from the right.\n "
            },
            {
                "gain": "The volume of the remote user. The value ranges from 0.0 to 100.0. The default value is 100.0 (the original volume of the remote user). The smaller the value, the lower the volume."
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "api_imediaplayer_adjustplayoutvolume",
        "name": "adjustPlayoutVolume",
        "description": "Adjusts the local playback volume.\n",
        "parameters": [
            {
                "volume": "The local playback volume, which ranges from 0 to 100:\n 0: Mute.\n 100: (Default) The original volume.\n "
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "api_setvolumeofeffect",
        "name": "setVolumeOfEffect",
        "description": "Sets the volume of a specified audio effect.\n",
        "parameters": [
            {
                "soundId": "The audio effect ID. The ID of each audio effect file is unique."
            },
            {
                "volume": "The playback volume. The value range is [0, 100]. The default value is 100, which represents the original volume."
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "callback_ivideoframeobserver_onscreencapturevideoframe",
        "name": "onScreenCaptureVideoFrame",
        "description": "Occurs each time the SDK receives a video frame captured by the screen.\nThis callback does not support sending processed RGBA video data back to the SDK.\n The video data obtained through this callback has not undergone preprocessing, such as watermarking, cropping content, rotating, or image enhancement.\n After you successfully register the video frame observer, the SDK triggers this callback each time it receives a video frame. In this callback, you can get the video data for screen sharing. You can then pre-process the data according to your scenarios.\n After pre-processing, you can send the processed video data back to the SDK by this callback.",
        "parameters": [
            {
                "videoFrame": "The video frame. "
            }
        ],
        "returns": "true: Sets the SDK to receive the video frame.\n false: Sets the SDK to discard the video frame.",
        "is_hide": true
    },
    {
        "id": "api_enablefacedetection",
        "name": "enableFaceDetection",
        "description": "Enables/Disables face detection for the local user.\nYou can call this method either before or after joining a channel.\n This method is for Android and iOS only.\n Once face detection is enabled, the SDK triggers the onFacePositionChanged callback toreport the face information of the local user, which includes the following:\n The width and height of the local video.\n The position of the human face in the local view.\n The distance between the human face and the screen. This method needs to be called after the camera is started (for example, by calling startPreview joinChannelWithOptions).",
        "parameters": [
            {
                "enabled": "Whether to enable face detection for the local user:\n true: Enable face detection.\n false: (Default) Disable face detection.\n "
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "api_sendstreammessage",
        "name": "sendStreamMessage",
        "description": "Sends data stream messages.\nSends data stream messages to all users in a channel. The SDK has the following restrictions on this method:\n Up to 30 packets can be sent per second in a channel with each packet having a maximum size of 1 KB.\n Each client can send up to 6 KB of data per second.\n Each user can have up to five data streams simultaneously. A successful method call triggers the onStreamMessage callback on the remote client, from which the remote user gets the stream message. \nA failed method call triggers the onStreamMessageError callback on the remote client. Ensure that you call createDataStream to create a data channel before calling this method.\n In live streaming scenarios, this method only applies to hosts.",
        "parameters": [
            {
                "streamId": "The data stream ID. You can get the data stream ID by calling createDataStream."
            },
            {
                "data": "The data to be sent."
            },
            {
                "length": "The length of the data."
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "api_getaudiomixingcurrentposition",
        "name": "getAudioMixingCurrentPosition",
        "description": "Retrieves the playback position (ms) of the music file.\nThis method retrieves the playback position (ms) of the current music file. You need to call this method after calling startAudioMixing and receiving the onAudioMixingStateChanged(audioMixingStatePlaying) callback.\n If you need to call getAudioMixingCurrentPosition multiple times, ensure that the call interval is longer than 500 ms.",
        "parameters": [],
        "returns": "≥ 0: The current playback position (ms) of the audio mixing, if this method call succeeds. 0 represents that the current music file does not start playing.\n < 0: Failure.",
        "is_hide": false
    },
    {
        "id": "callback_onmicrophoneenabled",
        "name": "onMicrophoneEnabled",
        "description": "Occurs when the microphone is enabled/disabled.\nDeprecated: 请改用 onLocalAudioStateChanged 回调的： localAudioStreamStateStopped(0)。\n localAudioStreamStateRecording(1)。 该回调是由本地用户调用 enableLocalAudio 方法开启或关闭本地音频采集触发的。",
        "parameters": [
            {
                "enabled": "麦克风状态： true：麦克风已启用。\n false：麦克风已禁用。 "
            }
        ],
        "returns": "",
        "is_hide": true
    },
    {
        "id": "api_ibasespatialaudioengine_setdistanceunit",
        "name": "setDistanceUnit",
        "description": "Sets the length (in meters) of the game engine distance per unit.\nIn a game engine, the unit of distance is customized, while in the Agora spatial audio algorithm, distance is measured in meters. By default, the SDK converts the game engine distance per unit to one meter. You can call this method to convert the game engine distance per unit to a specified number of meters.\n Agora recommends calling this method before enterRoom .",
        "parameters": [
            {
                "unit": "The number of meters that the game engine distance per unit is equal to. This parameter must be greater than 0.00. For example, setting unit as 2.00 means the game engine distance per unit equals 2 meters.The larger the value is, the faster the sound heard by the local user attenuates when the remote user moves far away from the local user.\n "
            }
        ],
        "returns": "",
        "is_hide": true
    },
    {
        "id": "class_transcodingvideostream",
        "name": "TranscodingVideoStream",
        "description": "The video streams for the video mixing on the local client.\n",
        "parameters": [
            {
                "sourceType": "The source type of video for the video mixing on the local client. See VideoSourceType .\n "
            },
            {
                "remoteUserUid": "The ID of the remote user.Use this parameter only when the source type of the video for the video mixingonthe local client is videoSourceRemote.\n "
            },
            {
                "imageUrl": "The URL of the image.Use this parameter only when the source type of the video for the video mixing on the local client is \n "
            },
            {
                "x": "The horizontal displacement of the top-left corner of the video for the video mixing on the client relative to the top-left corner (origin) of the canvas for this video mixing.\n "
            },
            {
                "y": "The vertical displacement of the top-left corner of the video for the video mixing on the client relative to the top-left corner (origin) of the canvas for this video mixing.\n "
            },
            {
                "width": "The width (px) of the video for the video mixing on the local client.\n "
            },
            {
                "height": "The height (px) of the video for the video mixing on the local client.\n "
            },
            {
                "zOrder": "The number of the layer to which the video for the video mixing on the local client belongs. The value range is [0,100].\n 0: (Default) The layer is at the bottom.\n 100: The layer is at the top.\n "
            },
            {
                "alpha": "The transparency of the video for the video mixing on the local client. The value range is [0.0,1.0]. 0.0 means the transparency is completely transparent. 1.0 means the transparency is opaque.\n "
            },
            {
                "mirror": "Whether to mirror the video for the video mixing on the local client.\n true: Mirror the captured video.\n false: (Default) Do not mirror the captured video. The paramter only works for videos with the source type CAMERA"
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "class_watermarkratio",
        "name": "WatermarkRatio",
        "description": "The position and size of the watermark on the screen.\nThe position and size of the watermark on the screen are determined by xRatio, yRatio, and widthRatio:\n (xRatio, yRatio) refers to the coordinates of the upper left corner of the watermark, which determines the distance from the upper left corner of the watermark to the upper left corner of the screen.\n The widthRatio determines the width of the watermark.",
        "parameters": [
            {
                "xRatio": "The x-coordinate of the upper left corner of the watermark. The x-coordinate of the upper left corner of the watermark. The horizontal position relative to the origin, where the upper left corner of the screen is the origin, and the x-coordinate is the upper left corner of the watermark. The value range is [0.0,1.0], and the default value is 0."
            },
            {
                "yRatio": "The y-coordinate of the upper left corner of the watermark. The vertical position relative to the origin, where the upper left corner of the screen is the origin, and the y-coordinate is the upper left corner of the screen. The value range is [0.0,1.0], and the default value is 0."
            },
            {
                "widthRatio": "The width of the watermark. The SDK calculates the height of the watermark proportionally according to this parameter value to ensure that the enlarged or reduced watermark image is not distorted. The value range is [0,1], and the default value is 0, which means no watermark is displayed."
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "callback_onfirstlocalvideoframe",
        "name": "onFirstLocalVideoFrame",
        "description": "Occurs when the first local video frame is displayed on the local video view.\nThis callback is triggered when the first local video frame is displayed on the local view.",
        "parameters": [
            {
                "connection": "The connection information. See RtcConnection ."
            },
            {
                "width": "The width (px) of the first local video frame."
            },
            {
                "height": "The height (px) of the first local video frame."
            },
            {
                "elapsed": "The time elapsed (ms) from the local user calling joinChannelWithOptions until the SDK triggers this callback. If you call startPreview before calling joinChannelWithOptions, then this parameter is the time elapsed from calling the startPreview method until the SDK triggers this callback."
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "api_getvideodevicemanager",
        "name": "getVideoDeviceManager",
        "description": "Gets the VideoDeviceManager object to manage video devices.\n",
        "parameters": [],
        "returns": "A VideoDeviceManager object.",
        "is_hide": false
    },
    {
        "id": "api_icloudspatialaudioengine_release",
        "name": "release",
        "description": "Destroys ICloudSpatialAudioEngine .\nThis method releases all resources under ICloudSpatialAudioEngine. When the user does not need to use the spatial audio effect, you can call this method to release resources for other operations.\n After calling this method, you can no longer use any of the APIs under ICloudSpatialAudioEngine or ICloudSpatialAudioEventHandler . To use the spatial audio effect again, you need to wait until the release method execution to complete before calling initialize to create a new ICloudSpatialAudioEngine.Call this method before the release method under RtcEngine .",
        "parameters": [],
        "returns": "",
        "is_hide": true
    },
    {
        "id": "api_registermediametadataobserver",
        "name": "registerMediaMetadataObserver",
        "description": "Registers the metadata observer.\nYou need to implement the MetadataObserver class and specify the metadata type in this method. This method enables you to add synchronized metadata in the video stream for more diversified\n live interactive streaming, such as sending shopping links, digital coupons, and online quizzes.\n A successful call of this method triggers the getMaxMetadataSize callback.\n Call this method before joinChannelWithOptions.",
        "parameters": [
            {
                "observer": "The metadata observer. See MetadataObserver ."
            },
            {
                "type": "The metadata type. The SDK currently only supports videoMetadata. \n "
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "api_irtcengineex_muteremoteaudiostreamex",
        "name": "muteRemoteAudioStreamEx",
        "description": "Stops or resumes receiving the audio stream of a specified user.\nThis method is used to stops or resumes receiving the audio stream of a specified user. You can call this method before or after joining a channel. If a user leaves a channel, the settings in this method become invalid.",
        "parameters": [
            {
                "connection": "The connection information. See RtcConnection ."
            },
            {
                "uid": "The ID of the specified user."
            },
            {
                "mute": "Whether to stop receiving the audio stream of the specified user: true: Stop receiving the audio stream of the specified user.\n false: (Default) Resume receiving the audio stream of the specified user.\n "
            }
        ],
        "returns": "0: Success.\n < 0: Failure.",
        "is_hide": false
    },
    {
        "id": "enum_maxchannelidlengthtype",
        "name": "MAX_CHANNEL_ID_LENGTH_TYPE",
        "description": "The maximum length of the channel name.\n",
        "parameters": [
            {
                "MAX_CHANNEL_ID_LENGTH": "The maximum length of the channel name is 64 bytes."
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "class_thumbimagebuffer",
        "name": "ThumbImageBuffer",
        "description": "The image content of the thumbnail or icon. Set in ScreenCaptureSourceInfo .\nThe default image is in the RGBA format. If you need to use another format, you need to convert the image on your own.",
        "parameters": [
            {
                "buffer": "The buffer of the thumbnail ot icon."
            },
            {
                "length": "The buffer length of the thumbnail or icon, in bytes."
            },
            {
                "width": "The actual width (px) of the thumbnail or icon."
            },
            {
                "height": "The actual height (px) of the thumbnail or icon."
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "api_unloadalleffects",
        "name": "unloadAllEffects",
        "description": "Releases a specified preloaded audio effect from the memory.\n",
        "parameters": [],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "api_registeraudioencodedframeobserver",
        "name": "registerAudioEncodedFrameObserver",
        "description": "Registers an encoded audio observer.\nCall this method after joining a channel.\n This method and startAudioRecording both set the audio content and audio quality. Agora recommends not using this method and startAudioRecording together; otherwise, only the method called later will take effect.",
        "parameters": [
            {
                "config": ""
            }
        ],
        "returns": "",
        "is_hide": true
    },
    {
        "id": "callback_onactivespeaker",
        "name": "onActiveSpeaker",
        "description": "Occurs when the most active remote speaker is detected.\nAfter a successful call of enableAudioVolumeIndication , the SDK continuously detects which remote user has the loudest volume. During the current period, the remote user, who is detected as the loudest for the most times, is the most active user.\n When the number of users is no less than two and an active remote speaker exists, the SDK triggers this callback and reports the uid of the most active remote speaker.\n If the most active remote speaker is always the same user, the SDK triggers the onActiveSpeaker callback only once.\n If the most active remote speaker changes to another user, the SDK triggers this callback again and reports the uid of the new active remote speaker.",
        "parameters": [
            {
                "connection": "The connection information. See RtcConnection ."
            },
            {
                "uid": "The user ID of the most active remote speaker."
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "api_irtcengineex_sendstreammessageex",
        "name": "sendStreamMessageEx",
        "description": "Sends data stream messages.\nAfter calling createDataStreamEx , you can call this method to send data stream messages to all users in the channel.\n The SDK has the following restrictions on this method:\n Up to 30 packets can be sent per second in a channel with each packet having a maximum size of 1 kB.\n Each client can send up to 6 KB of data per second.\n Each user can have up to five data streams simultaneously. A successful method call triggers the onStreamMessage callback on the remote client, from which the remote user gets the stream message. \nA failed method call triggers the onStreamMessageError callback on the remote client. Ensure that you call createDataStreamEx to create a data channel before calling this method.\n This method applies only to the `COMMUNICATION` profile or to the hosts in the `LIVE_BROADCASTING` profile. If an audience in the `LIVE_BROADCASTING` profile calls this method, the audience may be switched to a host.",
        "parameters": [
            {
                "connection": "The connection information. See RtcConnection ."
            },
            {
                "streamId": "The data stream ID. You can get the data stream ID by calling createDataStreamEx."
            },
            {
                "data": "The data to be sent."
            },
            {
                "length": "The length of the data."
            }
        ],
        "returns": "0: Success.\n < 0: Failure.",
        "is_hide": false
    },
    {
        "id": "api_updatescreencaptureparameters",
        "name": "updateScreenCaptureParameters",
        "description": "Updates the screen sharing parameters.\n",
        "parameters": [
            {
                "captureParams": "The screen sharing encoding parameters. The default video dimension is 1920 x 1080, that is, 2,073,600 pixels. Agora uses the value of this parameter to calculate the charges. See ScreenCaptureParameters ."
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "class_agorarhythmplayerconfig",
        "name": "AgoraRhythmPlayerConfig",
        "description": "The metronome configuration.\n",
        "parameters": [
            {
                "beatsPerMeasure": "The number of beats per measure, which ranges from 1 to 9. The default value is 4, which means that each measure contains one downbeat and three upbeats."
            },
            {
                "beatsPerMinute": "The beat speed (beats/minute), which ranges from 60 to 360. The default value is 60, which means that the metronome plays 60 beats in one minute."
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "class_ibasespatialaudioengine",
        "name": "IBaseSpatialAudioEngine",
        "description": "This class contains the API shared by the ICloudSpatialAudioEngine and ILocalSpatialAudioEngine classes.\nThe ICloudSpatialAudioEngine and ILocalSpatialAudioEngine classes inherit from IBaseSpatialAudioEngine.",
        "parameters": [],
        "returns": "",
        "is_hide": true
    },
    {
        "id": "api_imediaplayer_pause",
        "name": "pause",
        "description": "Pauses the playback.\n",
        "parameters": [],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "class_imediaplayer",
        "name": "MediaPlayer",
        "description": "This class provides media player functions and supports multiple instances.\n",
        "parameters": [],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "api_startlastmileprobetest",
        "name": "startLastmileProbeTest",
        "description": "Starts the last mile network probe test.\nThis method starts the last-mile network probe test before joining a channel to get the uplink and downlink last mile network statistics, including the bandwidth, packet loss, jitter, and round-trip time (RTT).\n Once this method is enabled, the SDK returns the following callbacks: \n onLastmileQuality : The SDK triggers this callback within two seconds depending on the network conditions. This callback rates the network conditions and is more closely linked to the user experience. \n onLastmileProbeResult : The SDK triggers this callback within 30 seconds depending on the network conditions. This callback returns the real-time statistics of the network conditions and is more objective. This method applies to the following scenarios:\n Before a user joins a channel, call this method to check the uplink network quality.\n In a live streaming channel, call this method to check the uplink network quality before an audience member switches to a host. \n Do not call other methods before receiving the onLastmileQuality and onLastmileProbeResult callbacks. Otherwise, the callbacks may be interrupted.\n A host should not call this method after joining a channel (when in a call).",
        "parameters": [
            {
                "config": "The configurations of the last-mile network probe test. See LastmileProbeConfig ."
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "callback_onleavechannel",
        "name": "onLeaveChannel",
        "description": "Occurs when a user leaves a channel.\nThis callback notifies the app that the user leaves the channel by calling leaveChannel . From this callback, the app can get information such as the call duration and quality statistics.",
        "parameters": [
            {
                "connection": "The connection information. See RtcConnection ."
            },
            {
                "stats": "The statistics of the call. See RtcStats ."
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "api_startpreview2",
        "name": "startPreview",
        "description": "Enables the local video preview and specifies the video source for the preview.\nThe local preview enables the mirror mode by default.\n After the local video preview is enabled, if you call leaveChannel to exit the channel, the local preview remains until you call stopPreview to disable it.\n The video source type set in this method needs to be consistent with the video source type of VideoCanvas you set in setupLocalVideo .",
        "parameters": [
            {
                "sourceType": "The type of the video source, see VideoSourceType ."
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "enum_audioencodedframeobserverposition",
        "name": "AudioEncodedFrameObserverPosition",
        "description": "Audio profile.\n",
        "parameters": [
            {
                "audioEncodedFrameObserverPositionRecord": "1: Only records the audio of the local user."
            },
            {
                "audioEncodedFrameObserverPositionPlayback": "2: Only records the audio of all remote users."
            },
            {
                "audioEncodedFrameObserverPositionMixed": "3: Records the mixed audio of the local and all remote users."
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "api_imediaengine_registeraudioframeobserver",
        "name": "registerAudioFrameObserver",
        "description": "Registers an audio frame observer object.\nCall this method to register an audio frame observer object (register a callback). When you need the SDK to trigger onRecordAudioFrame or onPlaybackAudioFrame callback, you need to use this method to register the callbacks.\n Ensure that you call this method before joining a channel.",
        "parameters": [
            {}
        ],
        "returns": "",
        "is_hide": true
    },
    {
        "id": "api_setscreencapturecontenthint",
        "name": "setScreenCaptureContentHint",
        "description": "Sets the content hint for screen sharing.\nA content hint suggests the type of the content being shared, so that the SDK applies different optimization algorithms to different types of content. If you don't call this method, the default content hint is contentHintNone.\n You can call this method either before or after you start screen sharing.",
        "parameters": [
            {
                "contentHint": "The content hint for screen sharing. See VideoContentHint ."
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "callback_onaudiomixingstatechanged_ng",
        "name": "onAudioMixingStateChanged",
        "description": "Occurs when the playback state of the music file changes.\nThis callback occurs when the playback state of the music file changes, and reports the current state and error code.",
        "parameters": [
            {
                "state": "The playback state of the music file. See AudioMixingStateType ."
            },
            {
                "errorCode": "The error code. See AudioMixingErrorType ."
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "callback_onstreampublished",
        "name": "onStreamPublished",
        "description": "Occurs when an RTMP or RTMPS stream is published.\nDeprecated:\n Please use onRtmpStreamingStateChanged instead. Reports the result of publishing an RTMP or RTMPS stream.",
        "parameters": [
            {
                "url": "The CDN streaming URL."
            },
            {
                "error": "Error codes of the RTMP or RTMPS streaming.\n ERR_OK (0): The publishing succeeds.\n ERR_FAILED (1): The publishing fails.\n ERR_INVALID_ARGUMENT (-2): Invalid argument used.\n If you do not call setLiveTranscoding to configure \n LiveTranscoding before calling addPublishStreamUrl , the SDK reports\n ERR_INVALID_ARGUMENT.\n ERR_TIMEDOUT (10): The publishing timed out.\n ERR_ALREADY_IN_USE (19): The chosen URL address is\n already in use for CDN live streaming.\n ERR_ENCRYPTED_STREAM_NOT_ALLOWED_PUBLISH (130): You\n cannot publish an encrypted stream.\n ERR_PUBLISH_STREAM_CDN_ERROR (151): CDN related\n error. Remove the original URL address and add a new one by calling\n the removePublishStreamUrl and addPublishStreamUrl methods.\n ERR_PUBLISH_STREAM_NUM_REACH_LIMIT (152): The host\n manipulates more than 10 URLs. Delete the unnecessary URLs before\n adding new ones.\n ERR_PUBLISH_STREAM_NOT_AUTHORIZED (153): The host\n manipulates other hosts' URLs. Please check your app logic.\n ERR_PUBLISH_STREAM_INTERNAL_SERVER_ERROR (154): An\n error occurs in Agora's streaming server. Call the removePublishStreamUrl method to publish the streaming\n again.\n ERR_PUBLISH_STREAM_FORMAT_NOT_SUPPORTED (156): The\n format of the CDN streaming URL is not supported. Check whether the\n URL format is correct. "
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "callback_imediaplayersourceobserver_onplayersourcestatechanged",
        "name": "onPlayerSourceStateChanged",
        "description": "Reports the playback state change.\nWhen the state of the media player changes, the SDK triggers this callback to report the current playback state.",
        "parameters": [
            {
                "state": "The playback state, see MediaPlayerState ."
            },
            {
                "ec": "The error code. See MediaPlayerError ."
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "callback_onlocalpublishfallbacktoaudioonly",
        "name": "onLocalPublishFallbackToAudioOnly",
        "description": "本地发布流已回退为音频流回调。\n如果你调用了 setLocalPublishFallbackOption 接口并将 option 设置为 streamFallbackOptionAudioOnly ，当上行网络环境不理想、本地发布的媒体流回退为音频流时，或当上行网络改善、媒体流恢复为音视频流时，会触发该回调。\n 如果本地发流已回退为音频流，远端的 App 上会收到 onUserMuteVideo \n的回调事件。",
        "parameters": [
            {
                "isFallbackOrRecover": "true:\n由于网络环境不理想，本地发布的媒体流已回退为音频流。\n false: 由于网络环境改善，发布的音频流已恢复为音视频流。\n "
            }
        ],
        "returns": "",
        "is_hide": true
    },
    {
        "id": "api_iaudiodevicemanager_setrecordingdevice",
        "name": "setRecordingDevice",
        "description": "Sets the audio recording device.\n",
        "parameters": [
            {
                "deviceId": "The ID of the audio recording device. You can get the device ID by calling enumerateRecordingDevices . Plugging or unplugging the audio device does not change the value of deviceId.\n "
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "api_getvideosourcetype",
        "name": "getVideoSourceType",
        "description": "Gets the video frame type.\n",
        "parameters": [],
        "returns": "≥ 0: The type of the video source, if the method call succe\n < 0: Failure.",
        "is_hide": true
    },
    {
        "id": "enum_channelmediarelayerror",
        "name": "ChannelMediaRelayError",
        "description": "The error code of the channel media relay.\n",
        "parameters": [
            {
                "relayOk": "0: No error."
            },
            {
                "relayErrorServerErrorResponse": "1: An error occurs in the server response."
            },
            {
                "relayErrorServerNoResponse": "2: No server response.\n You can call leaveChannel to leave the channel.\n This error can also occur if your project has not enabled co-host token authentication. You can to enable the service for cohosting across channels before starting a channel media relay.\n "
            },
            {
                "relayErrorNoResourceAvailable": "3: The SDK fails to access the service, probably due to limited resources of the server."
            },
            {
                "relayErrorFailedJoinSrc": "4: Fails to send the relay request."
            },
            {
                "relayErrorFailedJoinDest": "5: Fails to accept the relay request."
            },
            {
                "relayErrorFailedPacketReceivedFromSrc": "6: The server fails to receive the media stream."
            },
            {
                "relayErrorFailedPacketSentToDest": "7: The server fails to send the media stream."
            },
            {
                "relayErrorServerConnectionLost": "8: The SDK disconnects from the server due to poor network connections. You can call leaveChannel method to leave the channel."
            },
            {
                "relayErrorInternalError": "9: An internal error occurs in the server."
            },
            {
                "relayErrorSrcTokenExpired": "10: The token of the source channel has expired."
            },
            {
                "relayErrorDestTokenExpired": "11: The token of the destination channel has expired."
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "enum_audioreverbpreset",
        "name": "AUDIO_REVERB_PRESET",
        "description": "Voice reverb presets.\nDeprecated:\n Deprecated as of v3.2.0.",
        "parameters": [
            {
                "AUDIO_REVERB_OFF": "Turn off voice reverb, that is, to use the original voice."
            },
            {
                "AUDIO_REVERB_FX_KTV": "The reverb style typical of a KTV venue (enhanced)."
            },
            {
                "AUDIO_REVERB_FX_VOCAL_CONCERT": "The reverb style typical of a concert hall (enhanced)."
            },
            {
                "AUDIO_REVERB_FX_UNCLE": "A middle-aged man's voice."
            },
            {
                "AUDIO_REVERB_FX_SISTER": "The reverb style typical of a young woman's voice."
            },
            {
                "AUDIO_REVERB_FX_STUDIO": "The reverb style typical of a recording studio (enhanced)."
            },
            {
                "AUDIO_REVERB_FX_POPULAR": "The reverb style typical of popular music (enhanced)."
            },
            {
                "AUDIO_REVERB_FX_RNB": "The reverb style typical of R&B music (enhanced)."
            },
            {
                "AUDIO_REVERB_FX_PHONOGRAPH": "The voice effect typical of a vintage phonograph."
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "callback_onfirstlocalaudioframepublished",
        "name": "onFirstLocalAudioFramePublished",
        "description": "Occurs when the first audio frame is published.\nThe SDK triggers this callback under one of the following circumstances:\n The local client enables the audio module and calls joinChannelWithOptions successfully.\n The local client calls muteLocalAudioStream (true) and muteLocalAudioStream(false) in sequence.\n The local client calls disableAudio and enableAudio in sequence.",
        "parameters": [
            {
                "connection": "The connection information. See RtcConnection ."
            },
            {
                "elapsed": "Time elapsed (ms) from the local user calling the joinChannelWithOptions method until this callback is triggered."
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "api_setlocalpublishfallbackoption",
        "name": "setLocalPublishFallbackOption",
        "description": "Sets the fallback option for the published video stream based on the network conditions.\nAn unstable network affects the audio and video quality in a video call or interactive live video streaming. If option is set as streamFallbackOptionAudioOnly (2), the SDK disables the upstream video but enables audio only when the network conditions deteriorate and cannot support both video and audio. The SDK monitors the network quality and restores the video stream when the network conditions improve. When the published video stream falls back to audio-only or when the audio-only stream switches back to the video, the SDK triggers the onLocalPublishFallbackToAudioOnly callback. Agora does not recommend using this method for CDN live streaming, because the remote CDN live user will have a noticeable lag when the published video stream falls back to STREAM_FALLBACK_OPTION_AUDIO_ONLY(2). Therefore, Agora recommends not to enable this function in scenarios with bypass streaming.streamFallbackOptionAudioOnly\n Ensure that you call this method before joining a channel.",
        "parameters": [
            {
                "option": "The stream fallback option. See StreamFallbackOptions ."
            }
        ],
        "returns": "",
        "is_hide": true
    },
    {
        "id": "callback_onfirstremoteaudioframe",
        "name": "onFirstRemoteAudioFrame",
        "description": "Occurs when the first audio frame sent by a specified remote user is received.\nDeprecated:\n Use instead. onRemoteAudioStateChanged",
        "parameters": [
            {
                "connection": "The connection information. See RtcConnection ."
            },
            {
                "userId": "The ID of the remote user sending the audio frames."
            },
            {
                "elapsed": "The time elapsed (ms) from the local user calling the joinChannelWithOptions method until the SDK triggers this callback."
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "api_createmediaplayer",
        "name": "createMediaPlayer",
        "description": "Creates a media player instance.\n",
        "parameters": [],
        "returns": "The MediaPlayer instance, if the method call succeeds.\n An empty pointer, if the method call fails.",
        "is_hide": false
    },
    {
        "id": "callback_ivideoframeobserver_onsecondarypreencodecameravideoframe",
        "name": "onSecondaryPreEncodeCameraVideoFrame",
        "description": "Gets the video data captured from the second camera before encoding.\nAfter you successfully register the video frame observer, the SDK triggers this callback each time it receives a video frame. In this callback, you can get the video data captured from the second camera before encoding and then process the data according to your particular scenarios.\n After processing, you can send the processed video data back to the SDK in this callback. This method applies to Windows only.\n You need to set getObservedFramePosition before you can use this callback to get the video data captured from the second screen and before encoding.\n The video data that this callback gets has been preprocessed, with its content cropped and rotated, and the image enhanced.\n This callback does not support sending processed RGBA video data back to the SDK.",
        "parameters": [
            {
                "videoFrame": "The video frame. "
            }
        ],
        "returns": "true: Sets the SDK to receive the video frame.\n false: Sets the SDK to discard the video frame.",
        "is_hide": true
    },
    {
        "id": "api_setvoiceconversionpreset",
        "name": "setVoiceConversionPreset",
        "description": "Sets a preset voice beautifier effect.\nCall this method to set a preset voice beautifier effect for the local user who sends an audio stream. After setting an audio effect, all users in the channel can hear the effect. You can set different voice beautifier effects for different scenarios. \n To achieve better audio effect quality, Agora recommends that you call setAudioProfile and set the profile to audioProfileMusicHighQuality(4) or audioProfileMusicHighQualityStereo(5) and scenario to audioScenarioGameStreaming(3) before calling this method. You can call this method either before or after joining a channel.\n Do not set the profile parameter in setAudioProfile to audioProfileSpeechStandard(1) or audioProfileIot(6), or the method does not take effect.\n This method works best with the human voice. Agora does not recommend using this method for audio containing music.\n After calling setVoiceConversionPreset , Agora recommends not calling the following methods, or the settings in setVoiceConversionPreset are overridden: \n setAudioEffectPreset \n setAudioEffectParameters \n setVoiceBeautifierPreset \n setVoiceBeautifierParameters \n setLocalVoicePitch \n setLocalVoiceEqualization \n setLocalVoiceReverb",
        "parameters": [
            {
                "preset": "The options for the preset voice beautifier effects: VoiceConversionPreset .\n "
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "callback_imediaplayersourceobserver_onpreloadevent",
        "name": "onPreloadEvent",
        "description": "Reports the events of preloaded media resources.\n",
        "parameters": [
            {
                "src": "The URL of the media resource."
            },
            {
                "event": "Events that occur when media resources are preloaded. See PlayerPreloadEvent ."
            }
        ],
        "returns": "",
        "is_hide": true
    },
    {
        "id": "class_ivideoframeobserver",
        "name": "IVideoFrameObserver",
        "description": "The IVideoFrameObserver class.\n",
        "parameters": [],
        "returns": "",
        "is_hide": true
    },
    {
        "id": "api_icloudspatialaudioengine_addeventhandler",
        "name": "addEventHandler",
        "description": "Adds the ICloudSpatialAudioEventHandler event handler.\n",
        "parameters": [
            {
                "eventHandler": "The callback to be added. See ICloudSpatialAudioEventHandler for details."
            }
        ],
        "returns": "",
        "is_hide": true
    },
    {
        "id": "api_disableaudiospectrummonitor",
        "name": "disableAudioSpectrumMonitor",
        "description": "Disables audio spectrum monitoring.\nAfter calling enableAudioSpectrumMonitor , if you want to disable audio spectrum monitoring, you can call this method. You can call this method either before or after joining a channel.",
        "parameters": [],
        "returns": "",
        "is_hide": true
    },
    {
        "id": "class_sdkbuildinfo",
        "name": "SDKBuildInfo",
        "description": "SDK version information.\n",
        "parameters": [
            {
                "build": "SDK build index."
            },
            {
                "version": "SDK version information. String, such as 4.0.0."
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "enum_qualityreportformattype",
        "name": "QUALITY_REPORT_FORMAT_TYPE",
        "description": "Formats of the quality report.\n",
        "parameters": [
            {
                "QUALITY_REPORT_JSON": "0: The quality report in JSON format."
            },
            {
                "QUALITY_REPORT_HTML": "1: The quality report in HTML format."
            }
        ],
        "returns": "",
        "is_hide": true
    },
    {
        "id": "api_playeffect3",
        "name": "playEffect",
        "description": "Plays the specified local or online sound effect file.\nTo play multiple audio effect files at the same time, call this method multiple times with different soundId and filePath. For the best user experience, Agora recommends playing no more than three audio effect files at the same time. After the playback of an audio effect file completes, the SDK triggers the onAudioEffectFinished callback.Call this method after joining a channel.",
        "parameters": [
            {
                "soundId": "The audio effect ID. The ID of each audio effect file is unique.If you have loaded the audio effect into memory via preloadEffect , make sure this parameter is the same as the soundId set in preloadEffect. "
            },
            {
                "filePath": "Windows: The absolute path or URL address (including the suffixes of the filename) of the audio effect file. For example: C:\\music\\audio.mp4. Supported audio formats include MP3, AAC, M4A, MP4, WAV, and 3GP. See supported audio formats.\n If you have loaded the audio effect into memory via preloadEffect , make sure this parameter is the same as the filePath set in preloadEffect. "
            },
            {
                "loopCount": "The number of times the audio effect loops.\n ≥ 0: The number of playback times. For example, 1 means loop one time, which means playing the audio effect two times in total.\n -1: Play the music file in an infinite loop. "
            },
            {
                "pitch": "The pitch of the audio effect. The value range is 0.5 to 2.0. The default value is 1.0, which means the original pitch. The lower the value, the lower the pitch."
            },
            {
                "pan": "The spatial position of the audio effect. The value ranges between -1.0 and 1.0, where:\n -1.0: The audio effect displays to the left.\n 0.0: The audio effect displays ahead.\n 1.0: The audio effect displays to the right. "
            },
            {
                "gain": "The volume of the audio effect. The value range is 0.0 to 100.0. The default value is 100.0, which means the original volume. The smaller the value, the lower the volume."
            },
            {
                "publish": "Whether to publish the audio effect to the remote users.\n true: Publish the audio effect to the remote users. Both the local user and remote users can hear the audio effect.\n false: Do not publish the audio effect to the remote users. Only the local user can hear the audio effect. "
            },
            {
                "startPos": "The playback position (ms) of the audio effect file.\n "
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "api_enabledualstreammode3",
        "name": "enableDualStreamMode",
        "description": "Enables/Disables dual-stream mode.\nYou can call this method to enable or disable the dual-stream mode on the publisher side. Dual streams are a hybrid of a high-quality video stream and a low-quality video stream:\n High-quality video stream: High bitrate, high resolution.\n Low-quality video stream: Low bitrate, low resolution. After you enable the dual-stream mode, you can call setRemoteVideoStreamType to choose toreceive the high-quality video stream or low-quality video stream on the subscriber side. You can call this method either before or after joining a channel.",
        "parameters": [
            {
                "enabled": "Whether to enable dual-stream mode.\n true: Enable dual-stream mode.\n false: Disable dual-stream mode. "
            },
            {
                "sourceType": "The capture type of the custom video source. See VideoSourceType .\n "
            },
            {
                "streamConfig": "The configuration of the low-quality video stream. See SimulcastStreamConfig .\n "
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "api_imediaplayer_getplayposition",
        "name": "getPlayPosition",
        "description": "Gets current local playback progress.\n",
        "parameters": [],
        "returns": "Returns the current playback progress (ms) if the call succeeds.\n < 0: Failure. See MediaPlayerError .",
        "is_hide": false
    },
    {
        "id": "api_resumeaudiomixing",
        "name": "resumeAudioMixing",
        "description": "Resumes playing and mixing the music file.\nThis method resumes playing and mixing the music file. Call this method when you are in a channel.",
        "parameters": [],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "api_setcloudproxy",
        "name": "setCloudProxy",
        "description": "Sets the Agora cloud proxy service.\nWhen users' network access is restricted by a firewall, , configure the firewall to allow specific IP addresses and ports provided by Agora; then, call this method to enable the cloud proxy and set the cloud proxy type with the proxyType parameter.\n After successfully connecting to the cloud proxy, the SDK triggers the onConnectionStateChanged (connectionStateConnecting, connectionChangedSettingProxyServer) callback.\n To disable the cloud proxy that has been set, call the setCloudProxy (noneProxy).\n To change the cloud proxy type that has been set, call the setCloudProxy (noneProxy) first, and then call the setCloudProxy to set the proxyType you want. Agora recommends that you call this method before joining the channel or after leaving the channel.\n When a user is behind a firewall and uses the Force UDP cloud proxy, the services for Media Push and cohosting across channels are not available.\n When you use the Force TCP cloud proxy, note that an error would occur when calling the startAudioMixing method to play online music files in the HTTP protocol. The services for Media Push and cohosting across channels use the cloud proxy with the TCP protocol.",
        "parameters": [
            {
                "proxyType": "The type of the cloud proxy. See CloudProxyType .\n This parameter is mandatory. The SDK reports an error if you do not pass in a value.\n "
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "api_imediaplayer_getstreamcount",
        "name": "getStreamCount",
        "description": "Gets the number of the media streams in the media resource.\nCall this method after calling open .",
        "parameters": [],
        "returns": "The number of the media streams in the media resource if the method call succeeds.\n < 0: Failure. See MediaPlayerError .",
        "is_hide": false
    },
    {
        "id": "enum_videoframetype_ng",
        "name": "VideoFrameType",
        "description": "The video frame type.\n",
        "parameters": [
            {
                "videoFrameTypeBlankFrame": "0: A black frame."
            },
            {
                "videoFrameTypeKeyFrame": "3: Keyframe."
            },
            {
                "videoFrameTypeDeltaFrame": "4: Delta frame."
            },
            {
                "videoFrameTypeBFrame": "5:The B frame."
            },
            {
                "videoFrameTypeDroppableFrame": "6: A discarded frame."
            },
            {
                "videoFrameTypeUnknow": "Unknown frame."
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "api_stopsecondarycameracapture",
        "name": "stopSecondaryCameraCapture",
        "description": " Stops capturing video through the secondary camera.\nYou can call this method to stop capturing video through the secondary camera after calling the startSecondaryCameraCapture .",
        "parameters": [],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "callback_iaudioencodedframeobserver_onrecordaudioencodedframe",
        "name": "OnRecordAudioEncodedFrame",
        "description": "Gets the encoded audio data of the local user.\nAfter calling registerAudioEncodedFrameObserver and setting the encoded audio as audioEncodedFrameObserverPositionRecord, you can get the encoded audio data of the local user from this callback.",
        "parameters": [
            {
                "channels": ""
            }
        ],
        "returns": "",
        "is_hide": true
    },
    {
        "id": "enum_permissiontype",
        "name": "PermissionType",
        "description": "The type of the device permission.\n",
        "parameters": [
            {
                "recordAudio": "0: Permission for the audio capture device."
            },
            {
                "camera": "1: Permission for the camera."
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "api_updatechannelmediaoptions",
        "name": "updateChannelMediaOptions",
        "description": "Updates the channel media options after joining the channel.\n",
        "parameters": [
            {
                "options": "The channel media options. See ChannelMediaOptions ."
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "callback_oncameraready",
        "name": "onCameraReady",
        "description": "Occurs when the camera turns on and is ready to capture the video.\nDeprecated: Please use localVideoStreamStateCapturing(1) in onLocalVideoStateChanged instead. \n This callback indicates that the camera has been successfully turned on and you can start to capture video.",
        "parameters": [],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "api_irtcengineex_addvideowatermarkex",
        "name": "addVideoWatermarkEx",
        "description": "Adds a watermark image to the local video.\nThis method adds a PNG watermark image to the local video in the live streaming. Once the watermark image is added, all the audience in the channel (CDN audience included), and the capturing device can see and capture it. Agora supports adding only one watermark image onto the local video, and the newly watermark image replaces the previous one.\n The watermark coordinatesare dependent on the settings in the setVideoEncoderConfigurationEx method:\n If the orientation mode of the encoding video ( OrientationMode ) is fixed landscape mode or the adaptive landscape mode, the watermark uses the landscape orientation.\n If the orientation mode of the encoding video (OrientationMode) is fixed portrait mode or the adaptive portrait mode, the watermark uses the portrait orientation.\n When setting the watermark position, the region must be less than thesetVideoEncoderConfigurationEx dimensions set in the method; otherwise, the watermark image will be cropped. \n Ensure that you have called enableVideo before calling this method.\n This method supports adding a watermark image in the PNG file format only. Supported pixel formats of the PNG image are RGBA, RGB, Palette, Gray, and Alpha_gray.\n If the dimensions of the PNG image differ from your settings in this method, the image will be cropped or zoomed to conform to your settings.\n If you have enabled the local video preview by calling the startPreview method, you can use the visibleInPreview member to set whether or not the watermark is visible in the preview.\n If you have enabled the mirror mode for the local video, the watermark on the local video is also mirrored. To avoid mirroring the watermark, Agora recommends that you do not use the mirror and watermark functions for the local video at the same time. You can implement the watermark function in your application layer.",
        "parameters": [
            {
                "connection": "The connection information. See RtcConnection ."
            },
            {
                "options": "The options of the watermark image to be added. "
            },
            {
                "watermarkUrl": "The local file path of the watermark image to be added. This method supports adding a watermark image from the local absolute or relative file path."
            }
        ],
        "returns": "0: Success.\n < 0: Failure.",
        "is_hide": false
    },
    {
        "id": "class_videodimensions",
        "name": "VideoDimensions",
        "description": "The video dimension.\n",
        "parameters": [
            {
                "width": "The width (pixels) of the video.\n "
            },
            {
                "height": "The height (pixels) of the video."
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "api_setvoicebeautifierparameters",
        "name": "setVoiceBeautifierParameters",
        "description": "Sets parameters for the preset voice beautifier effects.\nCall this method to set a gender characteristic and a reverberation effect for the singing beautifier effect. This method sets parameters for the local user who sends an audio stream. After setting the audio parameters, all users in the channel can hear the effect.\n For better voice effects, Agora recommends that you call setAudioProfile and set scenario to audioScenarioGameStreaming(3) and profile to audioProfileMusicHighQuality(4) or audioProfileMusicHighQualityStereo(5) before calling this method. You can call this method either before or after joining a channel.\n Do not set the profile parameter in setAudioProfile to audioProfileSpeechStandard(1) or audioProfileIot(6), or the method does not take effect.\n This method works best with the human voice. Agora does not recommend using this method for audio containing music.\n After calling setVoiceBeautifierParameters, Agora recommends not calling the following methods, because they can override settings in setVoiceBeautifierParameters: \n setAudioEffectPreset \n setAudioEffectParameters \n setVoiceBeautifierPreset \n setLocalVoicePitch \n setLocalVoiceEqualization \n setLocalVoiceReverb \n setVoiceConversionPreset",
        "parameters": [
            {
                "preset": "The option for the preset audio effect:\n SINGING_BEAUTIFIER: The singing beautifier effect.\n "
            },
            {
                "param1": "The gender characteristics options for the singing voice:\n 1: A male-sounding voice.\n 2: A female-sounding voice.\n "
            },
            {
                "param2": "The reverberation effect options for the singing voice:\n 1: The reverberation effect sounds like singing in a small room.\n 2: The reverberation effect sounds like singing in a large room.\n 3: The reverberation effect sounds like singing in a hall.\n "
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "api_addpublishstreamurlex",
        "name": "addPublishStreamUrlEx",
        "description": "Publishes the local stream to a specified CDN live streaming URL.\nAfter calling this method, you can push media streams in RTMP or RTMPS protocol to the CDN according to RtcConnection . The SDK triggers the onRtmpStreamingStateChanged callback on the local client to report the state of adding a local stream to the CDN. Call this method after joining a channel.\n Ensure that the Media Push function is enabled.\n This method takes effect only when you are a host in live interactive streaming.\n This method adds only one streaming URL to the CDN each time it is called. To push multiple URLs, call this method multiple times.",
        "parameters": [
            {
                "connection": "The connection information. See RtcConnection ."
            },
            {
                "transcodingEnabled": "Whether to enable transcoding. Transcoding in a CDN live streaming converts the audio and video streams before pushing them to the CDN server. It applies to scenarios where a channel has multiple broadcasters and composite layout is needed.\n true: Enable transcoding.\n false: Disable transcoding. If you set this parameter as true, ensurethat you call the setLiveTranscoding method before calling this method. "
            },
            {
                "url": "The Media Push URL in the RTMP or RTMPS format. The maximum length of this parameter is 1024 bytes. The URL address must not contain special characters, such as Chinese language characters."
            }
        ],
        "returns": "0: Success.\n < 0: Failure.\n -2: Invalid parameter, usually an empty URL or a string with a length of 0.\n -7: The engine is not initialized when streaming.",
        "is_hide": false
    },
    {
        "id": "api_iaudiodevicemanager_enumerateplaybackdevices",
        "name": "enumeratePlaybackDevices",
        "description": "Enumerates the audio playback devices.\n",
        "parameters": [],
        "returns": "A AudioDeviceInfo array, which includes all the audio playback devices, if the method call succeeds.\n Failure: An empty array.",
        "is_hide": false
    },
    {
        "id": "callback_onlocalaudiostatechanged",
        "name": "onLocalAudioStateChanged",
        "description": "Occurs when the local audio stream state changes.\nWhen the state of the local audio stream changes (including the state of the audio capture and encoding), the SDK triggers this callback to report the current state. This callback indicates the state of the local audio stream, and allows you to troubleshoot issues when audio exceptions occur.\n When the state is localAudioStreamStateFailed (3), you can view the error information in the error parameter.",
        "parameters": [
            {
                "connection": "The connection information. See RtcConnection ."
            },
            {
                "state": "The state of the local audio. See localaudiostreamstate ."
            },
            {
                "error": "Local audio state error codes. See LocalAudioStreamError ."
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "api_iaudiodevicemanager_release",
        "name": "release",
        "description": "Releases all the resources occupied by the AudioDeviceManager object.\n",
        "parameters": [],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "enum_audiomixingstatetype",
        "name": "AudioMixingStateType",
        "description": "The playback state of the music file.\n",
        "parameters": [
            {
                "audioMixingStatePlaying": "710: The music file is playing.\n "
            },
            {
                "audioMixingStatePaused": "711: The music file pauses playing.\n "
            },
            {
                "audioMixingStateStopped": "713: The music file stops playing.\n "
            },
            {
                "audioMixingStateFailed": "714: An error occurs during the playback of the audio mixing file.\n "
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "callback_iaudioframeobserver_onrecordaudioframe",
        "name": "onRecordAudioFrame",
        "description": "Gets the captured audio frame.\nIf you want to set the format of the captured audio frame, Agora recommends that you call the setRecordingAudioFrameParameters method to set the format of the audio frame after calling registerAudioFrameObserver method to register an audio frame observer.",
        "parameters": [],
        "returns": "Reserved for future use.",
        "is_hide": true
    },
    {
        "id": "api_imediaplayer_setspatialaudioparams",
        "name": "setSpatialAudioParams",
        "description": "Enables or disables the spatial audio effect for the media player.\nAfter successfully setting the spatial audio effect parameters of the media player, the SDK enables the spatial audio effect for the media player, and the local user can hear the media resources with a sense of space.\n If you need to disable the spatial audio effect for the media player, set the params parameter to null.",
        "parameters": [
            {
                "params": "The spatial audio effect parameters of the media player. See SpatialAudioParams for details."
            }
        ],
        "returns": "",
        "is_hide": true
    },
    {
        "id": "api_startdirectcdnstreaming",
        "name": "startDirectCdnStreaming",
        "description": "Starts pushing media streams to the CDN directly.\nAogra does not support pushing streams to one URL repeatedly.\n Media options\n Agora does not support setting the value of publishCameraTrack and publishCustomVideoTrack as true, or the value of publishMicrophoneTrack and publishCustomAudioTrack as true at the same time. When choosing media setting options ( DirectCdnStreamingMediaOptions ) you can refer to the following examples:\n If you want to push audio and video streams published by the host to the CDN, the media setting options should be set as follows: publishCameraTrack is set as true\n publishMicrophoneTrack is set as true\n publishCustomAudioTrack is set as false (Default)\n publishCustomVideoTrack is set as false (Default)",
        "parameters": [
            {
                "eventHandler": "See onDirectCdnStreamingStateChanged and onDirectCdnStreamingStats ."
            },
            {
                "publishUrl": "The CDN live streaming URL."
            },
            {
                "options": "The media setting options for the host. See DirectCdnStreamingMediaOptions ."
            }
        ],
        "returns": "",
        "is_hide": true
    },
    {
        "id": "enum_saedeployregion",
        "name": "SAE_DEPLOY_REGION",
        "description": "Agora 空间音效服务器的访问区域。\n",
        "parameters": [
            {
                "SAE_DEPLOY_REGION_CN": "（默认）中国大陆。"
            },
            {
                "SAE_DEPLOY_REGION_NA": "North America."
            },
            {
                "SAE_DEPLOY_REGION_EU": "Europe."
            },
            {
                "SAE_DEPLOY_REGION_AS": "Asia, excluding Mainland China."
            }
        ],
        "returns": "",
        "is_hide": true
    },
    {
        "id": "class_lastmileprobeonewayresult",
        "name": "LastmileProbeOneWayResult",
        "description": "Results of the uplink or downlink last-mile network test.\n",
        "parameters": [
            {
                "packetLossRate": "The packet loss rate (%)."
            },
            {
                "jitter": "The network jitter (ms)."
            },
            {
                "availableBandwidth": "The estimated available bandwidth (bps)."
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "api_imediaplayer_open",
        "name": "open",
        "description": "Opens the media resource.\n",
        "parameters": [
            {
                "url": "The path of the media file. Both local path and online path are supported.On the Android platform, the URI format is not supported.\n "
            },
            {
                "startPos": "The starting position (ms) for playback. Default value is 0."
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "api_disableaudio",
        "name": "disableAudio",
        "description": "Disables the audio module.\nThis method disables the internal engine and can be called anytime after initialization. It is still valid after one leaves channel.\n This method resets the internal engine and takes some time to take effect. Agora recommends using the following API methods to control the audio modules separately: \n enableLocalAudio : Whether to enable the microphone to create the local audio stream. \n muteLocalAudioStream : Whether to publish the local audio stream. \n muteRemoteAudioStream : Whether to subscribe and play the remote audio stream. \n muteAllRemoteAudioStreams : Whether to subscribe to and play all remote audio streams.",
        "parameters": [],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "api_leavechannel",
        "name": "leaveChannel [1/2]",
        "description": "Leaves a channel.\nThis method releases all resources related to the session. This method call is asynchronous. When this method returns, it does not necessarily mean that the user has left the channel.\n A successful call of this method triggers the following callbacks: \n The local client: onLeaveChannel .\n The remote client: onUserOffline , if the user joining the channel is in the Communication profile, or is a host in the Live-broadcasting profile. \n If you call release immediately after calling this method, the SDK does not trigger the onLeaveChannel callback.\n If you call this method during a CDN live streaming, the SDK automatically calls the removePublishStreamUrl method.",
        "parameters": [],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "api_setavsyncsource",
        "name": "setAVSyncSource",
        "description": "Sets the pitch of the local music file.\nThe same user may use two devices to send audio streams and video streams respectively. To ensure the time synchronization of the audio and video heard and seen by the receiver, you can call this method on the video sender and pass in the channel of the audio sender. name, user ID. The SDK will automatically adjust the sent video stream based on the timestamp of the sent audio stream to ensure that even when the upstream network conditions of the two senders are inconsistent (such as using Wi-Fi and 4G networks respectively), the The received audio and video have time synchronization.\n Agora recommends calling this method before .",
        "parameters": [
            {
                "channelId": "Identifies the channel name of the channel where the audio sender is located."
            },
            {
                "uid": "User ID of the audio sender."
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "enum_videocodectype",
        "name": "VideoCodecType",
        "description": "Video codec types.\n",
        "parameters": [
            {
                "videoCodecVp8": "1: Standard VP8."
            },
            {
                "videoCodecH264": "2: Standard H.264."
            },
            {
                "videoCodecH265": "3: Standard H.265."
            },
            {
                "videoCodecGeneric": "6: Generic.\n This type is used for transmitting raw video data, such as encrypted video frames. The SDK returns this type of video frames in callbacks, and you need to decode and render the frames yourself.\n "
            },
            {
                "videoCodecGenericJpeg": "20: Generic JPEG.This type consumes minimum computing resources and applies to IoT devices.\n "
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "api_imediaplayer_preloadsrc",
        "name": "preloadSrc",
        "description": "Preloads a media resource.\nYou can call this method to preload a media resource into the playlist. If you need to preload multiple media resources, you can call this method multiple times.\n If the preload is successful and you want to play the media resource, call playPreloadedSrc ; if you want to clear the playlist, call stop . Agora does not support preloading duplicate media resources to the playlist. However, you can preload the media resources that are being played to the playlist again.",
        "parameters": [
            {
                "src": "The URL of the media resource."
            },
            {
                "startPos": "The starting position (ms) for playing after the media resource is preloaded to the playlist. When preloading a live stream, set this parameter to 0."
            }
        ],
        "returns": "",
        "is_hide": true
    },
    {
        "id": "api_setrecordingaudioframeparameters",
        "name": "setRecordingAudioFrameParameters",
        "description": "Sets the format of the captured raw audio data.\nSets the audio format for the onRecordAudioFrame callback. Ensure that you call this method before joining a channel.\n The SDK calculates the sampling interval based on the samplesPerCall, sampleRate, and channel parameters set in this method. The calculation formula is as follows:Sample interval = samplePerCall/(sampleRate × channel). Ensure that the sample interval ≥ 0.01 (s).",
        "parameters": [
            {
                "sampleRate": ""
            },
            {
                "channel": "The number of channels returned in the onRecordAudioFrame callback:\n 1: Mono (one channel).\n 2: Stereo (two channels). "
            },
            {
                "mode": "The use mode of the audio frame. See RawAudioFrameOpModeType .\n "
            },
            {
                "samplesPerCall": "The number of data samples returned in the onRecordAudioFrame callback, such as 1024 for the media push."
            }
        ],
        "returns": "",
        "is_hide": true
    },
    {
        "id": "api_setextensionproperty",
        "name": "setExtensionProperty",
        "description": "Sets the properties of the extension.\nAfter enabling the extension, you can call this method to set the properties of the extension.",
        "parameters": [
            {
                "provider": "The name of the extension provider."
            },
            {
                "extension": "The name of the extension."
            },
            {
                "key": "The key of the extension."
            },
            {
                "value": "The value of the extension key."
            },
            {
                "type": "The type of the video source, see MediaSourceType ."
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "api_imediaplayer_unloadsrc",
        "name": "unloadSrc",
        "description": "Unloads media resources that are preloaded.\nThis method cannot release the media resource being played.",
        "parameters": [
            {
                "src": "The URL of the media resource."
            }
        ],
        "returns": "",
        "is_hide": true
    },
    {
        "id": "callback_onaudiopublishstatechanged",
        "name": "onAudioPublishStateChanged",
        "description": "Occurs when the audio publishing state changes.\n",
        "parameters": [
            {
                "channel": "The channel name."
            },
            {
                "oldState": "The previous subscribing status. See StreamPublishState ."
            },
            {
                "newState": "The current subscribing status. See StreamPublishState."
            },
            {
                "elapseSinceLastState": "The time elapsed (ms) from the previous state to the current state."
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "enum_videoframeprocessmode",
        "name": "VIDEO_FRAME_PROCESS_MODE",
        "description": "The process mode of the video frame:\n",
        "parameters": [
            {
                "PROCESS_MODE_READ_ONLY": "Read-only mode.\n In this mode, you do not modify the video frame. The video frame observer is a renderer.\n "
            },
            {
                "PROCESS_MODE_READ_WRITE": "Read and write mode.\n In this mode, you modify the video frame. The video frame observer is a video filter.\n "
            }
        ],
        "returns": "",
        "is_hide": true
    },
    {
        "id": "enum_audiofilerecordingtype",
        "name": "AudioFileRecordingType",
        "description": "Recording content. Set in startAudioRecording .\n",
        "parameters": [
            {
                "audioFileRecordingMic": "1: Only records the audio of the local user."
            },
            {
                "audioFileRecordingPlayback": "2: Only records the audio of all remote users."
            },
            {
                "audioFileRecordingMixed": "3: Records the mixed audio of the local and all remote users."
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "class_lastmileproberesult",
        "name": "LastmileProbeResult",
        "description": "Results of the uplink and downlink last-mile network tests.\n",
        "parameters": [
            {
                "state": "The status of the last-mile probe test. See LastmileProbeResultState .\n "
            },
            {
                "uplinkReport": "Results of the uplink last-mile network test. See LastmileProbeOneWayResult ."
            },
            {
                "downlinkReport": "Results of the downlink last-mile network test. See LastmileProbeOneWayResult ."
            },
            {
                "rtt": "The round-trip time (ms)."
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "class_iaudiospectrumobserver",
        "name": "IAudioSpectrumObserver",
        "description": "The audio spectrum observer.\n",
        "parameters": [],
        "returns": "",
        "is_hide": true
    },
    {
        "id": "api_getcameramaxzoomfactor",
        "name": "getCameraMaxZoomFactor",
        "description": "Gets the maximum zoom ratio supported by the camera.\nThis method is for Android and iOS only.\n Call this method after enabling the local camera, for example, by calling joinChannelWithOptions , enableVideo , or enableLocalVideo , depending on which method you use to turn on your local camera.",
        "parameters": [],
        "returns": "The maximum zoom factor.",
        "is_hide": false
    },
    {
        "id": "api_stoprtmpstream",
        "name": "stopRtmpStream",
        "description": "Stops pushing media streams to a CDN.\nYou can call this method to stop the live stream on the specified CDN address. This method can stop pushing media streams to only one CDN address at a time, so if you need to stop pushing streams to multiple addresses, call this method multiple times.\n After you call this method, the SDK triggers the onRtmpStreamingStateChanged callback on the local client to report the state of the streaming.",
        "parameters": [
            {
                "url": "The address of Media Push. The format is RTMP or RTMPS. The character length cannot exceed 1024 bytes. Special characters such as Chinese characters are not supported."
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "api_imediaplayer_getcurrentagoracdnindex",
        "name": "getCurrentAgoraCDNIndex",
        "description": "Gets the CDN routes index of the current media resource.\n",
        "parameters": [],
        "returns": "",
        "is_hide": true
    },
    {
        "id": "enum_mediadevicetype",
        "name": "MediaDeviceType",
        "description": "Media device types.\n",
        "parameters": [
            {
                "unknownAudioDevice": "-1: Unknown device type."
            },
            {
                "audioPlayoutDevice": "0: Audio playback device."
            },
            {
                "audioRecordingDevice": "1: Audio capturing device."
            },
            {
                "videoRenderDevice": "2: Video renderer."
            },
            {
                "videoCaptureDevice": "3: Video capturer."
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "api_imediaplayer_playpreloadedsrc",
        "name": "playPreloadedSrc",
        "description": "Plays preloaded media resources.\nAfter calling the preloadSrc method to preload the media resource into the playlist, you can call this method to play the preloaded media resource. After calling this method, if you receive the onPlayerSourceStateChanged callback which reports the playerStatePlaying state, the playback is successful.\n If you want to change the preloaded media resource to be played, you can call this method again and specify the URL of the new media resource that you want to preload. If you want to replay the media resource, you need to call preloadSrc to preload the media resource to the playlist again before playing. If you want to clear the playlist, call the stop method. If you call this method when playback is paused, this method does not take effect until playback is resumed.",
        "parameters": [
            {
                "src": "The URL of the media resource in the playlist must be consistent with the src set by the preloadSrc method; otherwise, the media resource cannot be played."
            }
        ],
        "returns": "",
        "is_hide": true
    },
    {
        "id": "api_adjustaudiomixingpublishvolume",
        "name": "adjustAudioMixingPublishVolume",
        "description": "Adjusts the volume of audio mixing for publishing.\nThis method adjusts the audio mixing volume on the remote clients. Call this method after calling startAudioMixing and receiving the onAudioMixingStateChanged(audioMixingStatePlaying) callback.",
        "parameters": [
            {
                "volume": "The volume of audio mixing for local playback. The value ranges between 0 and 100 (default). 100 represents the original volume."
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "class_screencaptureparameters",
        "name": "ScreenCaptureParameters",
        "description": "Screen sharing configurations.\n",
        "parameters": [
            {
                "dimensions": "The maximum dimensions of encoding the shared region.  The default value is 1920 x 1080, that is, 2,073,600 pixels. Agora uses the value of this parameter to calculate the charges. VideoDimensions \n If the aspect ratio is different between the encoding dimensions and screen dimensions, Agora applies the following algorithms for encoding. Suppose dimensions are 1920 x 1080:\n If the value of the screen dimensions is lower than that of dimensions, for example, 1000 x 1000 pixels, the SDK uses 1000 x 1000 pixels for encoding.\n If the value of the screen dimensions is higher than that of dimensions, for example, 2000 x 1500, the SDK uses the maximum value under dimensions with the aspect ratio of the screen dimension (4:3) for encoding, that is, 1440 x 1080. "
            },
            {
                "frameRate": "On Windows and macOS, it represents the video encoding frame rate (fps) of the shared screen stream. The frame rate (fps) of the shared region. The default value is 5. We do not recommend setting this to a value greater than 15."
            },
            {
                "bitrate": "On Windows and macOS, it represents the video encoding bitrate of the shared screen stream. The bitrate (Kbps) of the shared region. The default value is 0 (the SDK works out a bitrate according to the dimensions of the current screen)."
            },
            {
                "captureMouseCursor": "Whether to capture the mouse in screen sharing:\n true: (Default) Capture the mouse.\n false: Do not capture the mouse. "
            },
            {
                "windowFocus": " startScreenCaptureByWindowId Whether to bring the window to the front when calling the method to share it:\n true:Bring the window to the front.\n false: (Default) Do not bring the window to the front. "
            },
            {
                "excludeWindowList": "A list of IDs of windows to be blocked. When calling startScreenCaptureByScreenRect to start screen sharing, you can use this parameter to block a specified window. When calling to updateScreenCaptureParameters update screen sharing configurations, you can use this parameter to dynamically block the specified windows during screen sharing.\n "
            },
            {
                "excludeWindowCount": "The number of windows to be blocked.\n "
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "class_watermarkoptions_ng",
        "name": "WatermarkOptions",
        "description": "Configurations of the watermark image.\n",
        "parameters": [
            {
                "visibleInPreview": "Reserved for future use."
            },
            {
                "positionInLandscapeMode": "When the adaptation mode of the watermark isfitModeCoverPosition, it is used to set the area of the watermark image in landscape mode. See Rectangle ."
            },
            {
                "positionInPortraitMode": "When the adaptation mode of the watermark isfitModeCoverPosition , it is used to set the area of the watermark image in portrait mode. See Rectangle ."
            },
            {
                "watermarkRatio": "When the watermark adaptation mode is fitModeUseImageRatio, this parameter is used to set the watermark coordinates. See WatermarkRatio ."
            },
            {
                "mode": "The adaptation mode of the watermark. See WatermarkFitMode ."
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "class_cloudspatialaudioconfig",
        "name": "CloudSpatialAudioConfig",
        "description": "The configuration of ICloudSpatialAudioEngine.\n",
        "parameters": [
            {}
        ],
        "returns": "",
        "is_hide": true
    },
    {
        "id": "api_setlocalvoicereverbpreset",
        "name": "setLocalVoiceReverbPreset",
        "description": "设置本地语音混响（含虚拟立体声效果）。\nDeprecated:\n Use setAudioEffectPreset or setVoiceBeautifierPreset instead. 通信场景下的用户或直播场景下的主播均可调用该方法设置本地语音混响。 成功设置以后，频道内的所有用户均可听到声音效果。 当使用以 AUDIO_REVERB_FX 为前缀的枚举值时，请确保在调用该方法前将 setAudioProfile 的 profile 参数设置为 audioProfileMusicHighQuality (4) 或 audioProfileMusicHighQualityStereo (5) ，否则该方法设置无效。\n This method works best with the human voice, and Agora does not recommend using it for audio containing music and a human voice.\n 该方法不能与 setLocalVoiceChanger 方法一同使用，否则先调的方法会不生效。 更多注意事项，详见进阶功能《变声与混响》。\n You can call this method either before or after joining a channel.",
        "parameters": [
            {
                "preset": "本地语音混响选项，默认值为 AUDIO_REVERB_OFF ，即原声。 For more details, see AUDIO_REVERB_PRESET . 为达到更好的混响效果，Agora 推荐使用以 AUDIO_REVERB_FX 为前缀的枚举值。\n "
            }
        ],
        "returns": "",
        "is_hide": true
    },
    {
        "id": "api_joinchannelwithuseraccount2",
        "name": "joinChannelWithUserAccount",
        "description": "Joins the channel with a user account, and configures whether to automatically subscribe to audio or video streams after joining the channel.\nThis method allows a user to join the channel with the user account. After the user successfully joins the channel, the SDK triggers the following callbacks:\n The local client: onLocalUserRegistered , onJoinChannelSuccess and onConnectionStateChanged callbacks.\n The remote client: The onUserJoined callback if the user is in the COMMUNICATION profile, and the onUserInfoUpdated callback if the user is a host in the LIVE_BROADCASTING profile. Once a user joins the channel, the user subscribes to the audio and video streams of all the other users in the channel by default, giving rise to usage and billing calculation. To stop subscribing to a specified stream or all remote streams, call the corresponding mute methods.\n To ensure smooth communication, use the same parameter type to identify the user. For example, if a user joins the channel with a user ID, then ensure all the other users use the user ID too. The same applies to the user account. If a user joins the channel with the Agora Web SDK, ensure that the ID of the user is set to the same parameter type.",
        "parameters": [
            {
                "options": "The channel media options. See ChannelMediaOptions .\n "
            },
            {
                "token": "The token generated on your server for authentication. See \n "
            },
            {
                "channelId": "The channel name. This parameter signifies the channel in which users engage in real-time audio and video interaction. Under the premise of the same App ID, users who fill in the same channel ID enter the same channel for audio and video interaction. The string length must be less than 64 bytes. Supported characters:\n All lowercase English letters: a to z.\n All uppercase English letters: A to Z.\n All numeric characters: 0 to 9.\n Space\n \"!\", \"#\", \"$\", \"%\", \"&amp;\", \"(\", \")\", \"+\", \"-\", \":\", \";\", \"&lt;\", \"= \", \".\", \"&gt;\", \"?\", \"@\", \"[\", \"]\", \"^\", \"_\", \"{\", \"}\", \"|\", \"~\", \",\" "
            },
            {
                "userAccount": "The user account. This parameter is used to identify the user in the channel for real-time audio and video engagement. You need to set and manage user accounts yourself and ensure that each user account in the same channel is unique. The maximum length of this parameter is 255 bytes. Ensure that you set this parameter and do not set it as NULL. Supported characters are (89 in total):\n The 26 lowercase English letters: a to z.\n The 26 uppercase English letters: A to Z.\n All numeric characters: 0 to 9.\n Space\n \"!\", \"#\", \"$\", \"%\", \"&amp;\", \"(\", \")\", \"+\", \"-\", \":\", \";\", \"&lt;\", \"= \", \".\", \"&gt;\", \"?\", \"@\", \"[\", \"]\", \"^\", \"_\", \"{\", \"}\", \"|\", \"~\", \",\" "
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "api_irtcengineex_updatechannelmediaoptionsex",
        "name": "updateChannelMediaOptionsEx",
        "description": "Updates the channel media options after joining the channel.\n",
        "parameters": [
            {
                "connection": "The connection information. See RtcConnection ."
            },
            {
                "options": "The channel media options. See ChannelMediaOptions ."
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "api_imediaplayer_setaudiodualmonomode",
        "name": "setAudioDualMonoMode",
        "description": "Sets the channel mode of the current audio file.\nIn a stereo music file, the left and right channels can store different audio data. According to your needs, you can set the channel mode to original mode, left channel mode, right channel mode, or mixed channel mode. For example, in the KTV scenario, the left channel of the music file stores the musical accompaniment, and the right channel stores the singing voice. If you only need to listen to the accompaniment, call this method to set the channel mode of the music file to left channel mode; if you need to listen to the accompaniment and the singing voice at the same time, call this method to set the channel mode to mixed channel mode. Call this method after calling open .\n This method only applies to stereo audio files.",
        "parameters": [
            {
                "mode": "The channel mode. See AudioDualMonoMode ."
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "class_localspatialaudioconfig",
        "name": "LocalSpatialAudioConfig",
        "description": "The configuration of ILocalSpatialAudioEngine .\n",
        "parameters": [
            {}
        ],
        "returns": "",
        "is_hide": true
    },
    {
        "id": "api_ilocalspatialaudioengine_clearremotepositions",
        "name": "clearRemotePositions",
        "description": "Removes the spatial positions of all remote users.\nAfter successfully calling this method, the local user no longer hears any remote users.\n After leaving the channel, to avoid wasting resources, you can also call this method to delete the spatial positions of all remote users.",
        "parameters": [],
        "returns": "",
        "is_hide": true
    },
    {
        "id": "api_setremotevoicepositionex",
        "name": "setRemoteVoicePositionEx",
        "description": "Sets the 2D position (the position on the horizontal plane) of the remote user's voice.\nThis method sets the voice position and volume of a remote user.\n When the local user calls this method to set the voice position of a remote user, the voice difference between the left and right channels allows the local user to track the real-time position of the remote user, creating a sense of space. This method applies to massive multiplayer online games, such as Battle Royale games. For the best voice positioning, Agora recommends using a wired headset.\n Call this method after joining a channel.",
        "parameters": [
            {
                "connection": "The connection information. See RtcConnection ."
            },
            {
                "uid": "The user ID of the remote user."
            },
            {
                "pan": "The voice position of the remote user. The value ranges from -1.0 to 1.0:\n -1.0: The remote voice comes from the left.\n 0.0: (Default) The remote voice comes from the front.\n 1.0: The remote voice comes from the right.\n "
            },
            {
                "gain": "The volume of the remote user. The value ranges from 0.0 to 100.0. The default value is 100.0 (the original volume of the remote user). The smaller the value, the lower the volume."
            }
        ],
        "returns": "0: Success.\n < 0: Failure.",
        "is_hide": false
    },
    {
        "id": "api_iaudiodevicemanager_getplaybackdevice",
        "name": "getPlaybackDevice",
        "description": "Retrieves the current audio playback device.\n",
        "parameters": [],
        "returns": "The current audio playback device.",
        "is_hide": false
    },
    {
        "id": "enum_rtmpstreampublishstate",
        "name": "RtmpStreamPublishState",
        "description": "States of the Media Push.\n",
        "parameters": [
            {
                "rtmpStreamPublishStateIdle": "0: The Media Push has not started or has ended.\nThis state is also triggered after you remove a RTMP or RTMPS stream from the CDN by calling removePublishStreamUrl ."
            },
            {
                "rtmpStreamPublishStateConnecting": "1: The SDK is connecting to Agora's streaming server and the CDN server.\nThis state is triggered after you call the addPublishStreamUrl method."
            },
            {
                "rtmpStreamPublishStateRunning": "2: The RTMP or RTMPS streaming publishes. The SDK successfully publishes the RTMP or RTMPS streaming and returns this state."
            },
            {
                "rtmpStreamPublishStateRecovering": "3: The RTMP or RTMPS streaming is recovering.\nWhen exceptions occur to the CDN, or the streaming is interrupted, the SDK tries to resume RTMP or RTMPS streaming and returns this state. If the SDK successfully resumes the streaming, rtmpStreamPublishStateRunning(2) returns. If the streaming does not resume within 60 seconds or server errors occur, rtmpStreamPublishStateFailure(4) returns.\nYou can also reconnect to the server by calling the removePublishStreamUrl and addPublishStreamUrl methods.\n "
            },
            {
                "rtmpStreamPublishStateFailure": "3: Fails to push streams to the CDN.\nSee the errCode parameter for the detailed error information.You can also call the addPublishStreamUrl method to publish the RTMP or RTMPS streaming again."
            },
            {
                "rtmpStreamPublishStateDisconnecting": "5: The SDK is disconnecting from the Agora streaming server and CDN. When you call removePublishStreamUrl or stopRtmpStream to stop the streaming normally, the SDK reports the streaming state as rtmpStreamPublishStateDisconnecting and rtmpStreamPublishStateIdle in sequence."
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "api_leavechannel2",
        "name": "leaveChannel",
        "description": "Leaves a channel.\nIf you call release immediately after calling this method, the SDK does not trigger the onLeaveChannel callback.\n If you call this method during a CDN live streaming, the SDK automatically calls the removePublishStreamUrl method.\n This method lets the user leave the channel, for example, by hanging up or exiting the call.\n After joining the channel, you must call this method or leaveChannel to end the call, otherwise, the next call cannot be started.\n No matter whether you are currently in a call or not, you can call this method without side effects. This method releases all resources related to the session.\n This method call is asynchronous. When this method returns, it does not necessarily mean that the user has left the channel. After you leave the channel, the SDK triggers the onLeaveChannel callback. A successful call of this method triggers the following callbacks: The local client: onLeaveChannel\nThe remote client: onUserOffline , if the user joining the channel is in the COMMUNICATION profile, or is a host in the LIVE_BROADCASTING profile.",
        "parameters": [
            {
                "options": "The options for leaving the channel. See LeaveChannelOptions ."
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "api_icloudspatialaudioengine_enablespatializer",
        "name": "enableSpatializer",
        "description": "Enables or disables the calculation of spatial audio effect parameters by the Agora Spatial Audio Server.\nOnce enabled, users can hear the spatial audio effect of remote users, as well as their spatial position changes.You can call this method either before or after enterRoom , with the following differences:\n If you call this method before enterRoom, this method takes effect when entering the room.\n If you call this method after enterRoom, this method takes effect immediately.",
        "parameters": [
            {
                "enable": "Whether to enable the calculation of spatial audio effect parameters within the audio reception range:\n true: Enable the calculation.\n false: Disable the calculation.\n "
            },
            {
                "applyToTeam": "Whether to enable the calculation of spatial audio effect parameters in the team:\n true: Enable the calculation.\n false: Disable the calculation.\n This parameter only takes effect when the enable parameter is true."
            }
        ],
        "returns": "",
        "is_hide": true
    },
    {
        "id": "enum_lastmileproberesultstate",
        "name": "LastmileProbeResultState",
        "description": "The status of the last-mile probe test.\n",
        "parameters": [
            {
                "lastmileProbeResultComplete": "1: The last-mile network probe test is complete."
            },
            {
                "lastmileProbeResultIncompleteNoBwe": "2: The last-mile network probe test is incomplete because the bandwidth estimation is not available due to limited test resources. One possible reason is that testing resources are temporarily limited."
            },
            {
                "lastmileProbeResultUnavailable": "3: The last-mile network probe test is not carried out. Probably due to poor network conditions."
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "api_setcameradeviceorientation",
        "name": "setCameraDeviceOrientation",
        "description": "Sets the rotation angle of the captured video.\nWhen the video capture device does not have the gravity sensing function, you can call this method to manually adjust the rotation angle of the captured video.",
        "parameters": [
            {
                "type": "The video source type. See VideoSourceType .\n "
            },
            {
                "orientation": "The clockwise rotation angle. See VideoOrientation .\n "
            }
        ],
        "returns": "0: Success.\n < 0: Failure.",
        "is_hide": false
    },
    {
        "id": "api_startscreencapturebywindowid",
        "name": "startScreenCaptureByWindowId",
        "description": "Shares the whole or part of a window by specifying the window ID.\nThere are two ways to start screen sharing, you can choose one according to the actual needs:\n Call this method before joining a channel, and then call joinChannelWithOptions to join a channel and set publishScreenTrack true to start screen sharing.\n Call this method after joining a channel, and then call updateChannelMediaOptions and set publishScreenTrack true to start screen sharing. This method shares a window or part of the window. You need to specify the ID of the window to be shared.\n Applies to the macOS and Windows platforms only.\n This method supports window sharing of UWP (Universal Windows Platform) applications. Agora tests the mainstream UWP applications by using the lastest SDK, see details as follows:",
        "parameters": [
            {
                "windowId": "The ID of the window to be shared."
            },
            {
                "regionRect": "(Optional) Sets the relative location of the region to the screen. If you do not set this parameter, the SDK shares the whole screen. See Rectangle . If the specified region overruns the window, the SDK shares only the region within it; if you set width or height as 0, the SDK shares the whole window."
            },
            {
                "captureParams": "Screen sharing configurations. The default video dimension is 1920 x 1080, that is, 2,073,600 pixels. Agora uses the value of this parameter to calculate the charges. See ScreenCaptureParameters ."
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "callback_imediaplayersourceobserver_onmetadata",
        "name": "onMetaData",
        "description": "Occurs when the media metadata is received.\nThe callback occurs when the player receives the media metadata and reports the detailed information of the media metadata.",
        "parameters": [
            {
                "data": "The detailed data of the media metadata."
            },
            {
                "length": "The data length (bytes)."
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "enum_remoteaudiostatereason",
        "name": "RemoteAudioStateReason",
        "description": "The reason for the remote audio state change.\n",
        "parameters": [
            {
                "remoteAudioReasonInternal": "0: The SDK reports this reason when the audio state changes."
            },
            {
                "remoteAudioReasonNetworkCongestion": "1: Network congestion."
            },
            {
                "remoteAudioReasonNetworkRecovery": "2: Network recovery."
            },
            {
                "remoteAudioReasonLocalMuted": "3: The local user stops receiving the remote audio stream or disables the audio module."
            },
            {
                "remoteAudioReasonLocalUnmuted": "4: The local user resumes receiving the remote audio stream or enables the audio module."
            },
            {
                "remoteAudioReasonRemoteMuted": "5: The remote user stops sending the audio stream or disables the audio module."
            },
            {
                "remoteAudioReasonRemoteUnmuted": "6: The remote user resumes sending the audio stream or enables the audio module."
            },
            {
                "remoteAudioReasonRemoteOffline": "7: The remote user leaves the channel."
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "enum_mediasourcetype",
        "name": "MediaSourceType",
        "description": "Media source type.\n",
        "parameters": [
            {
                "audioPlayoutSource": "0: Audio playback device."
            },
            {
                "audioRecordingSource": "1: Audio capturing device."
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "api_imediaplayer_switchagoracdnsrc",
        "name": "switchAgoraCDNSrc",
        "description": "Switches the media resource being played.\nIf you want to improve the security of the connection and the privacy of media files, contact to determine the sign and the ts fields for authentication. Once the fields are determined, use them as the query parameter of the URL to update the URL of the media resource. For example:\n The URL of the media file to be opened :rtmp://$domain/$appName/$streamName.\n The URL updated by the authentication of the media file to be opened: rtmp://$domain/$appName/$streamName?ts=$ts&sign=$sign Authentication information:\n sign: An encrypted string calculated according to the MD5 algorithm based on authKey, appName, streamName and ts. You need to for your authKey.\n ts: The timestamp when the authentication information expires. You can set the validity period of the authentication information according to your scenarios. For example, 24h or 1h30m20s. If you want to customize the CDN routes for playing the media resource, call this method to switch media resources. Agora changes the CDN route through the self-developed scheduling center to improve the viewing experience. If you do not need to customize CDN routes for playing the media resource, call the switchSrc method to switch media resources. \n Call this method after calling openWithAgoraCDNSrc .\n You can call this method either before or after play . If you call this method before play, the SDK waits for you to call play before completing the route switch.",
        "parameters": [
            {
                "src": "The URL of the media resource."
            },
            {
                "syncPts": "Whether to synchronize the playback position (ms) before and after the switch:\n true: Synchronize the playback position before and after the switch.\n false: (Default) Do not synchronize the playback position before and after the switch. falseMake sure to set this parameter as if you need to play live streams, or the switch fails. If you need to play on-demand streams, you can set the value of this parameter according to your scenarios.\n "
            }
        ],
        "returns": "",
        "is_hide": true
    },
    {
        "id": "callback_onapicallexecuted",
        "name": "onApiCallExecuted",
        "description": "Occurs when a method is executed by the SDK.\n",
        "parameters": [
            {
                "err": "The error code returned by the SDK when the method call fails. If the SDK returns 0, then the method call is successful."
            },
            {
                "api": "The method executed by the SDK."
            },
            {
                "result": "The result of the method call."
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "class_spatialaudioparams",
        "name": "SpatialAudioParams",
        "description": "The spatial audio parameters.\n",
        "parameters": [
            {
                "speaker_azimuth": "The azimuth angle of the remote user or media player relative to the local user. The value range is [0,360], and the unit is degrees, as defined by the following main directions:\n 0: (Default) 0 degrees, which means directly to the front on the horizontal plane.\n 90: 90 degrees, which means directly to the left on the horizontal plane.\n 180: 180 degrees, which means directly to the back on the horizontal plane.\n 270: 270 degrees, which means directly to the right on the horizontal plane.\n 360: 360 degrees, which means directly to the front on the horizontal plane.\n "
            },
            {
                "speaker_elevation": "The elevation angle of the remote user or media player relative to the local user. The value range is [-90,90], and the unit is degrees, as defined by the following main directions:\n 0: (Default) 0 degrees, which means that the horizontal plane is not rotated.\n -90: -90 degrees, which means that the horizontal plane is rotated 90 degrees downwards.\n 90: 90 degrees, which means that the horizontal plane is rotated 90 degrees upwards.\n "
            },
            {
                "speaker_distance": "The distance of the remote user or media player relative to the local user. The value range is [1,50], and the unit is meters. The default value is 1 meter."
            },
            {
                "speaker_orientation": "The orientation of the remote user or media player relative to the local user. The value range is [0,180], and the unit is degrees, as defined by the following main directions:\n 0: (Default) 0 degrees, which means that the sound source and listener face the same direction.\n 180: 180 degrees, which means that the sound source and listener face each other.\n "
            },
            {
                "enable_blur": "Whether to enable audio blurring:\n true: Enable blurring.\n false: (Default) Disable blurring.\n "
            },
            {
                "enable_air_absorb": "Whether to enable air absorption. This function simulates the energy attenuation of audio when the audio transmits in the air:\n true: (Default) Enable air absorption.\n false: Disable air absorption.\n "
            }
        ],
        "returns": "",
        "is_hide": true
    },
    {
        "id": "callback_onremoteaudiostatechanged",
        "name": "onRemoteAudioStateChanged",
        "description": "Occurs when the remote audio state changes. \nThis callback indicates the state change of the remote audio stream.\n This callback can be inaccurate when the number of users (in the communication profile) or hosts (in the live broadcasting profile) in a channel exceeds 17.",
        "parameters": [
            {
                "connection": "The connection information. See RtcConnection ."
            },
            {
                "remoteUid": "The ID of the remote user whose audio state changes."
            },
            {
                "state": "The state of the remote audio, see RemoteAudioState ."
            },
            {
                "reason": "The reason of the remote audio state change, see RemoteAudioStateReason ."
            },
            {
                "elapsed": "Time elapsed (ms) from the local user calling the joinChannelWithOptions method until the SDK triggers this callback."
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "enum_rendermodetype",
        "name": "RenderModeType",
        "description": "Video display modes.\n",
        "parameters": [
            {
                "renderModeHidden": "1: Uniformly scale the video until one of its dimension fits the boundary (zoomed to fit). The window is filled. One dimension of the video might have clipped contents."
            },
            {
                "renderModeFit": "2: Uniformly scale the video until one of its dimension fits the boundary (zoomed to fit). Priority is to ensuring that all video content is displayed. Areas that are not filled due to disparity in the aspect ratio are filled with black."
            },
            {
                "renderModeAdaptive": " Deprecated:\n 3: This mode is deprecated. "
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "api_irtcengineex_createdatastreamex2",
        "name": "createDataStreamEx",
        "description": "Creates a data stream.\nCreates a data stream. Each user can create up to five data streams in a single channel.\n Compared with createDataStreamEx , this method does not support data reliability. If a data packet is not received five seconds after it was sent, the SDK directly discards the data.",
        "parameters": [
            {
                "connection": "The connection information. See RtcConnection ."
            },
            {
                "config": "The configurations for the data stream. See DataStreamConfig ."
            }
        ],
        "returns": "< 0: Failure.",
        "is_hide": false
    },
    {
        "id": "api_pauseeffect",
        "name": "pauseEffect",
        "description": "Pauses playing a specified audio effect file.\n",
        "parameters": [
            {
                "soundId": "The audio effect ID. The ID of each audio effect file is unique."
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "class_audiovolumeinfo",
        "name": "AudioVolumeInfo",
        "description": "The volume information of users.\n",
        "parameters": [
            {
                "uid": "The user ID. In the local user's callback, uid = 0.\n In the remote users' callback, uid is the user ID of a remote user whose instantaneous volume is one of the three highest. "
            },
            {
                "volume": "The volume of the user. The value ranges between 0 (the lowest volume) and 255 (the highest volume). "
            },
            {
                "vad": "Voice activity status of the local user. 0: The local user is not speaking.\n 1: The local user is speaking. \n The vad parameter does not report the voice activity status of remote users. In a remote user's callback, the value of vad is always 1.\n To use this parameter, you must set reportVad to true when calling enableAudioVolumeIndication .\n "
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "callback_ivideoframeobserver_onpreencodescreenvideoframe",
        "name": "onPreEncodeScreenVideoFrame",
        "description": "Gets the video data captured from the screen before encoding.\nAfter you successfully register the video frame observer, the SDK triggers this callback each time it receives a video frame. In this callback, you can get the video data captured from the screen before encoding and then process the data according to your particular scenarios.\n After processing, you can send the processed video data back to the SDK in this callback. To get the video data captured from the second screen before encoding, you need to set getObservedFramePosition .\n The video data that this callback gets has been preprocessed, with its content cropped and rotated, and the image enhanced.\n This callback does not support sending processed RGBA video data back to the SDK.",
        "parameters": [
            {
                "videoFrame": "The video frame. "
            }
        ],
        "returns": "true: Sets the SDK to receive the video frame.\n false: Sets the SDK to discard the video frame.",
        "is_hide": true
    },
    {
        "id": "api_startscreencapturebyscreenrect",
        "name": "startScreenCaptureByScreenRect",
        "description": "Shares the whole or part of a screen by specifying the screen rect.\nThere are two ways to start screen sharing, you can choose one according to the actual needs:\n Call this method before joining a channel, and then call joinChannelWithOptions to join a channel and set publishScreenTrack true to start screen sharing.\n Call this method after joining a channel, and then call updateChannelMediaOptions and set publishScreenTrack true to start screen sharing. This method shares a screen or part of the screen. You need to specify the area of the screen to be shared.\n This method applies to Windows only.",
        "parameters": [
            {
                "screenRect": "Sets the relative location of the screen to the virtual screen."
            },
            {
                "regionRect": " Rectangle . If the specified region overruns the screen, the SDK shares only the region within it; if you set width or height as 0, the SDK shares the whole screen."
            },
            {
                "captureParams": "The screen sharing encoding parameters. The default video dimension is 1920 x 1080, that is, 2,073,600 pixels. Agora uses the value of this parameter to calculate the charges. See ScreenCaptureParameters ."
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "enum_backgroundblurdegree",
        "name": "backgroundblurdegree",
        "description": "The degree of blurring applied to the custom background image.\n",
        "parameters": [
            {
                "blurDegreeLow": "1: The degree of blurring applied to the custom background image is low. The user can almost see the background clearly."
            },
            {
                "blurDegreeMedium": "The degree of blurring applied to the custom background image is medium. It is difficult for the user to recognize details in the background."
            },
            {
                "blurDegreeHigh": "(Default) The degree of blurring applied to the custom background image is high. The user can barely see any distinguishing features in the background."
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "callback_imetadataobserver_getmaxmetadatasize",
        "name": "getMaxMetadataSize",
        "description": "Occurs when the SDK requests the maximum size of the metadata.\nAfter successfully complete the registration by calling registerMediaMetadataObserver , the SDK triggers this callback once every video frame is sent. You need to specify the maximum size of the metadata in the return value of this callback.",
        "parameters": [],
        "returns": "The maximum size of the buffer of the metadata that you want to use. The highest value is 1024 bytes. Ensure that you set the return value.",
        "is_hide": false
    },
    {
        "id": "api_enableaudiovolumeindication",
        "name": "enableAudioVolumeIndication",
        "description": "Enables the reporting of users' volume indication.\nThis method enables the SDK to regularly report the volume information of the local user who sends a stream and remote users (up to three) whose instantaneous volumes are the highest to the app. Once you call this method and users send streams in the channel, the SDK triggers the onAudioVolumeIndication callback at the time interval set in this method.\n You can call this method either before or after joining a channel.",
        "parameters": [
            {
                "interval": "Sets the time interval between two consecutive volume indications:\n ≤ 0: Disables the volume indication.\n > 0: Time interval (ms) between two consecutive volume indications. You need to set this parameter to an integer multiple of 200. If the value is lower than 200, the SDK automatically adjusts the value to 200.\n "
            },
            {
                "smooth": "The smoothing factor sets the sensitivity of the audio volume indicator. The value ranges between 0 and 10. The recommended value is 3. The greater the value, the more sensitive the indicator."
            },
            {
                "reportVad": "true: Enable the voice activity detection of the local user. Once it is enabled,the vad parameter of the onAudioVolumeIndication callback reports the voice activity status of the local user.\n false: (Default) Disable the voice activity detection of the local user. Once it is disabled, the vad parameter of the onAudioVolumeIndication callbackdoes not report the voice activity status of the local user, except for the scenario where the engine automatically detects the voice activity of the local user.\n "
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "callback_onaudioeffectfinished",
        "name": "onAudioEffectFinished",
        "description": "Occurs when the playback of the local music file finishes.\nThis callback occurs when the local audio effect file finishes playing.",
        "parameters": [
            {
                "soundId": "The audio effect ID. The ID of each audio effect file is unique."
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "api_imediaplayer_getpublishsignalvolume",
        "name": "getPublishSignalVolume",
        "description": "Gets the volume of the media file for publishing.\n",
        "parameters": [],
        "returns": "≥ 0: The remote playback volume.\n < 0: Failure.",
        "is_hide": false
    },
    {
        "id": "callback_onextensionerrored",
        "name": "onExtensionErrored",
        "description": "Occurs when the extension runs incorrectly.\nWhen calling enableExtension (true) fails or the extension runs in error, the extension triggers this callback and reports the error code and reason.",
        "parameters": [
            {
                "provider": "The name of the extension provider."
            },
            {
                "extName": "The name of the extension."
            },
            {
                "error": "The error code. For details, see the extension documentation provided by the extension provider."
            },
            {
                "msg": "Reason. For details, see the extension documentation provided by the extension provider."
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "enum_rtmpstreamingevent",
        "name": "RtmpStreamingEvent",
        "description": "Events during the media push.\n",
        "parameters": [
            {
                "rtmpStreamingEventFailedLoadImage": "1: An error occurs when you add a background image or a watermark image in the media push."
            },
            {
                "rtmpStreamingEventUrlAlreadyInUse": "2: The streaming URL is already being used for CDN live streaming.\nIf you want to start new streaming, use a new streaming URL."
            },
            {
                "rtmpStreamingEventAdvancedFeatureNotSupport": "3: The feature is not supported."
            },
            {
                "rtmpStreamingEventRequestTooOften": "4: Reserved."
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "callback_onmediadevicechanged",
        "name": "onMediaDeviceChanged",
        "description": "",
        "parameters": [
            {
                "null": ""
            }
        ],
        "returns": "",
        "is_hide": true
    },
    {
        "id": "enum_rtmpstreampublisherrortype",
        "name": "RtmpStreamPublishErrorType",
        "description": "Error codes of the RTMP or RTMPS streaming.\n",
        "parameters": [
            {
                "rtmpStreamPublishErrorOk": "0: The RTMP or RTMPS streaming publishes successfully."
            },
            {
                "rtmpStreamPublishErrorInvalidArgument": "1: Invalid argument used. Check the parameter setting. For example, if you do not call setLiveTranscoding to set the transcoding parameters before calling addPublishStreamUrl , the SDK returns this error."
            },
            {
                "rtmpStreamPublishErrorEncryptedStreamNotAllowed": "2: The RTMP or RTMPS streaming is encrypted and cannot be published."
            },
            {
                "rtmpStreamPublishErrorConnectionTimeout": "3: Timeout for the RTMP or RTMPS streaming. Call the addPublishStreamUrl method to publish the streaming again."
            },
            {
                "rtmpStreamPublishErrorInternalServerError": "4: An error occurs in Agora's streaming server. Call the addPublishStreamUrl method to publish the streaming again."
            },
            {
                "rtmpStreamPublishErrorRtmpServerError": "5: An error occurs in the CDN server."
            },
            {
                "rtmpStreamPublishErrorTooOften": "6: Reserved parameter"
            },
            {
                "rtmpStreamPublishErrorReachLimit": "7: The host publishes more than 10 URLs. Delete the unnecessary URLs before adding new ones."
            },
            {
                "rtmpStreamPublishErrorNotAuthorized": "8: The host manipulates other hosts' URLs. For example, the host updates or stops other hosts' streams. Check your app logic."
            },
            {
                "rtmpStreamPublishErrorStreamNotFound": "9: Agora's server fails to find the RTMP or RTMPS streaming."
            },
            {
                "rtmpStreamPublishErrorFormatNotSupported": "10: The format of the RTMP or RTMPS streaming URL is not supported. Check whether the URL format is correct."
            },
            {
                "rtmpStreamPublishErrorNotBroadcaster": "11: The user role is not host, so the user cannot use the CDN live streaming function. Check your app code logic."
            },
            {
                "rtmpStreamPublishErrorTranscodingNoMixStream": "13: The updateRtmpTranscoding or setLiveTranscoding method is called to update the transcoding configuration in a scenario where there is streaming without transcoding. Check your app code logic."
            },
            {
                "rtmpStreamPublishErrorNetDown": "14: Errors occurred in the host's network."
            },
            {
                "rtmpStreamPublishErrorInvalidAppid": "15: Your App ID does not have permission to use the CDN live streaming function. "
            },
            {
                "rtmpStreamUnpublishErrorOk": "100: The streaming has been stopped normally. After you call removePublishStreamUrl to stop streaming, the SDK returns this value."
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "callback_onremotevideotransportstats",
        "name": "onRemoteVideoTransportStats",
        "description": "Reports the transport-layer statistics of each remote video stream.\nDeprecated:\n This callback is deprecated, use onRemoteVideoStats instead. This callback reports the transport-layer statistics, such as the packet loss rate and network time delay, once every two seconds after the local user receives a video packet from a remote user.\n During a call, when the user receives the video packet sent by the remote user/host, the callback is triggered every 2 seconds.",
        "parameters": [
            {
                "connection": "The connection information. See RtcConnection ."
            },
            {
                "remoteUid": "The ID of the remote user sending the video packets."
            },
            {
                "delay": "The network delay (ms) from the sender to the receiver."
            },
            {
                "lost": "The packet loss rate (%) of the video packet sent from the remote user."
            },
            {
                "rxKBitRate": "The bitrate of the received video (Kbps)."
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "enum_videomirrormodetype",
        "name": "VideoMirrorModeType",
        "description": "Video mirror mode.\n",
        "parameters": [
            {
                "videoMirrorModeAuto": "0: (Default) The SDK determines the mirror mode."
            },
            {
                "videoMirrorModeEnabled": "1: Enable mirror mode."
            },
            {
                "videoMirrorModeDisabled": "2: Disable mirror mode."
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "callback_onvideosourceframesizechanged",
        "name": "onVideoSourceFrameSizeChanged",
        "description": "",
        "parameters": [
            {
                "null": ""
            }
        ],
        "returns": "",
        "is_hide": true
    },
    {
        "id": "callback_onconnectioninterrupted",
        "name": "onConnectionInterrupted",
        "description": "Occurs when the connection between the SDK and the server is interrupted.\nDeprecated:\n Use onConnectionStateChanged instead. The SDK triggers this callback when it loses connection with the server for more than four seconds after the connection is established. After triggering this callback, the SDK tries to reconnect to the server. You can use this callback to implement pop-up reminders. The difference between this callback and onConnectionLost is:\n The SDK triggers the onConnectionInterrupted callback when it loses connection with the server for more than four seconds after it successfully joins the channel.\n The SDK triggers the onConnectionLost callback when it loses connection with the server for more than 10 seconds, whether or not it joins the channel.\n If the SDK fails to rejoin the channel 20 minutes after being disconnected from Agora's edge server, the SDK stops rejoining the channel.",
        "parameters": [
            {
                "connection": "The connection information. See RtcConnection ."
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "enum_audioequalizationbandfrequency",
        "name": "AudioEqualizationBandFrequency",
        "description": "The midrange frequency for audio equalization.\n",
        "parameters": [
            {
                "audioEqualizationBand31": "0: 31 Hz"
            },
            {
                "audioEqualizationBand62": "1: 62 Hz"
            },
            {
                "audioEqualizationBand125": "2: 125 Hz"
            },
            {
                "audioEqualizationBand250": "3: 250 Hz"
            },
            {
                "audioEqualizationBand500": "4: 500 Hz"
            },
            {
                "audioEqualizationBand1k": "5: 1 kHz"
            },
            {
                "audioEqualizationBand2k": "6: 2 kHz"
            },
            {
                "audioEqualizationBand4k": "7: 4 kHz"
            },
            {
                "audioEqualizationBand8k": "8: 8 kHz"
            },
            {
                "audioEqualizationBand16k": "9: 16 kHz"
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "api_imediaplayer_getagoracdnlinecount",
        "name": "getAgoraCDNLineCount",
        "description": "Gets the number of CDN routes for the media resource.\n",
        "parameters": [],
        "returns": "",
        "is_hide": true
    },
    {
        "id": "class_icloudspatialaudioeventhandler",
        "name": "ICloudSpatialAudioEventHandler",
        "description": "The class that sends event notifications relating to the spatial audio effect.\n",
        "parameters": [],
        "returns": "",
        "is_hide": true
    },
    {
        "id": "class_ivideodevicemanager",
        "name": "VideoDeviceManager",
        "description": "Video device management methods.\n",
        "parameters": [],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "api_setaudiosessionoperationrestriction",
        "name": "setAudioSessionOperationRestriction",
        "description": "Sets the operation permissions of the SDK on the Audio Session.\nBy default, both the SDK and the app have permission to operate the Audio Session. If you only need to use the app to operate the Audio Session, you can call this method to restrict the SDK's operation permissions to the Audio Session.\n You can call this method either before or after joining a channel. Once this method is called to restrict the SDK's operation permissions to the Audio Session, the restriction taks effect when the SDK needs to change the Audio Session. This method applies to iOS only.\n This method does not affect the operation permissions of the app on the Audio Session.",
        "parameters": [
            {
                "restriction": "The operation permissions of the SDK on the audio session, see AudioSessionOperationRestriction . This parameter is Bit Mask, and each Bit corresponds to a permission."
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "class_ivideodevicecollection",
        "name": "IVideoDeviceCollection",
        "description": "You can get the information of video devices with this interface.\n",
        "parameters": [],
        "returns": "",
        "is_hide": true
    },
    {
        "id": "api_setaudiomixingpitch",
        "name": "setAudioMixingPitch",
        "description": "Sets the pitch of the local music file.\nWhen a local music file is mixed with a local human voice, call this method to set the pitch of the local music file only.\n You need to call this method after calling startAudioMixing and receiving the onAudioMixingStateChanged(audioMixingStatePlaying) callback.",
        "parameters": [
            {
                "pitch": "Sets the pitch of the local music file by the chromatic scale. The default value is 0, which means keeping the original pitch. The value ranges from -12 to 12, and the pitch value between consecutive values is a chromatic value. The greater the absolute value of this parameter, the higher or lower the pitch of the local music file."
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "callback_onvideostopped",
        "name": "onVideoStopped",
        "description": "Occurs when the video stops playing.\nDeprecated:\n Please use localVideoStreamStateStopped(0) in the onLocalVideoStateChanged callback instead. The application can use this callback to change the configuration of the view (for example, displaying other pictures in the view) after the video stops playing.",
        "parameters": [],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "api_addvideowatermark",
        "name": "addVideoWatermark [1/2]",
        "description": "Adds a watermark image to the local video.\nDeprecated:\n This method is deprecated.\nUse addVideoWatermark instead. This method adds a PNG watermark image to the local video stream in a live streaming session. Once the watermark image is added, all the users in the channel (CDN audience included) and the video capturing device can see and capture it. If you only want to add a watermark to the CDN live streaming, see descriptions in setLiveTranscoding . The URL descriptions are different for the local video and CDN live streaming: In a local video stream, URL refers to the absolute path of the added watermark image file in the local video stream. In a CDN live stream, URL refers to the URL address of the added watermark image in the CDN live streaming.\n The source file of the watermark image must be in the PNG file format. If the width and height of the PNG file differ from your settings in this method, the PNG file will be cropped to conform to your settings.\n The Agora SDK supports adding only one watermark image onto a local video or CDN live stream. The newly added watermark image replaces the previous one.",
        "parameters": [
            {
                "watermark": "The watermark image to be added to the local live streaming: RtcImage ."
            }
        ],
        "returns": "",
        "is_hide": true
    },
    {
        "id": "enum_mediaplayererror",
        "name": "MediaPlayerError",
        "description": "Error codes of the media player.\n",
        "parameters": [
            {
                "playerErrorNone": "0: No error."
            },
            {
                "playerErrorInvalidArguments": "-1: Invalid arguments."
            },
            {
                "null": "-2: Internal errors."
            },
            {
                "playerErrorNoResource": "-3: No resource."
            },
            {
                "playerErrorInvalidMediaSource": "-4: Invalid media resource."
            },
            {
                "playerErrorUnknownStreamType": "-5: The type of the media stream is unknown."
            },
            {
                "playerErrorObjNotInitialized": "-6: The object is not initialized."
            },
            {
                "playerErrorCodecNotSupported": "-7: The codec is not supported."
            },
            {
                "playerErrorVideoRenderFailed": "-8: Invalid renderer."
            },
            {
                "playerErrorInvalidState": "-9: An error with the internal state of the player occurs."
            },
            {
                "playerErrorUrlNotFound": "-10: The URL of the media resource can not be found."
            },
            {
                "playerErrorInvalidConnectionState": "-11: Invalid connection between the player and Agora's server."
            },
            {
                "playerErrorSrcBufferUnderflow": "-12: The playback buffer is insufficient."
            },
            {
                "playerErrorInterrupted": "-13: The playback is interrupted."
            },
            {
                "playerErrorNotSupported": "-14: The SDK does support the method being called."
            },
            {
                "playerErrorUnknown": "-17: An unknown error."
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "enum_mediaplayerevent",
        "name": "MediaPlayerEvent",
        "description": "Media player events.\n",
        "parameters": [
            {
                "playerEventSeekBegin": "0: The player begins to seek to a new playback position."
            },
            {
                "playerEventSeekComplete": "1: The player finishes seeking to a new playback position."
            },
            {
                "playerEventSeekError": "2: An error occurs when seeking to a new playback position."
            },
            {
                "playerEventAudioTrackChanged": "5: The audio track used by the player has been changed."
            },
            {
                "playerEventBufferLow": "6: The currently buffered data is not enough to support playback."
            },
            {
                "playerEventBufferRecover": "7: The currently buffered data is just enough to support playback."
            },
            {
                "playerEventFreezeStart": "8: The audio or video playback freezes."
            },
            {
                "playerEventFreezeStop": "9: The audio or video playback resumes without freezing."
            },
            {
                "playerEventSwitchBegin": "10: The player starts switching the media resource."
            },
            {
                "playerEventSwitchComplete": "11: Media resource switching is complete."
            },
            {
                "playerEventSwitchError": "12: Media resource switching error."
            },
            {
                "playerEventFirstDisplayed": "13: The first video frame is rendered."
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "enum_logfiltertype",
        "name": "LogFilterType",
        "description": "The output log level of the SDK.\n",
        "parameters": [
            {
                "logFilterOff": "0: Do not output any log information."
            },
            {
                "logFilterDebug": "0x080f: Output all log information. Set your log filter to this level if you want to get the most complete log file."
            },
            {
                "logFilterInfo": "0x000f: Output logFilterCritical, logFilterError, logFilterWarn, and logFilterInfo level log information. We recommend setting your log filter to this level."
            },
            {
                "logFilterWarn": "0x000e: Output logFilterCritical, logFilterError, and logFilterWarn level log information."
            },
            {
                "logFilterError": "0x000c: Output logFilterCritical and logFilterError level log information."
            },
            {
                "logFilterCritical": "0x0008: Output logFilterCritical level log information."
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "api_irtcengineex_setremotevoice3dpositionex",
        "name": "setRemoteVoice3DPositionEx",
        "description": "Sets the 3D position of the remote user's voice.\n通过设置远端用户声音的水平角、垂直角和声源距离，让远端用户的声音听起来有方位感。\n This method applies to massive multiplayer online games, such as Battle Royale games.\n 该方法和 setRemoteVoicePositionEx 的区别是：该方法设置声音的 3D 位置；setRemoteVoicePositionEx 设置声音的 2D 位置，即水平面上的位置。 使用该方法需要在加入频道前调用 enableSoundPositionIndication 开启远端用户声音的立体声。\n For the best voice positioning, Agora recommends using a wired headset.\n Call this method after joining a channel.",
        "parameters": [
            {
                "connection": "The connection information. See RtcConnection ."
            },
            {
                "distance": "声源距离。 取值不能小于 0.5，单位为米。 为避免设置距离过大导致声音较小，Agora 推荐设置范围为 [0.5,50]。"
            },
            {
                "elevation": "垂直角。 取值范围为 [-90,90]，单位为度。 其中： 0:（默认）0 度，表示水平面无旋转。\n -90: -90 度，表示水平面向下旋转 90 度。\n 90: 90 度，表示水平面向上旋转 90 度。\n "
            },
            {
                "azimuth": "水平角。 取值范围为 [0,360]，单位为度。 其中： 0:（默认）0 度，表示水平面的正前方。\n 90: 90 度，表示水平面的正左方。\n 180: 180 度，表示水平面的正右方。\n "
            },
            {
                "uid": "The user ID of the remote user."
            }
        ],
        "returns": "0: Success.\n < 0: Failure.",
        "is_hide": true
    },
    {
        "id": "enum_channelmediarelaystate",
        "name": "ChannelMediaRelayState",
        "description": "The state code of the channel media relay.\n",
        "parameters": [
            {
                "relayStateIdle": "0: The initial state. After you successfullystop the channel media relay by calling stopChannelMediaRelay , the onChannelMediaRelayStateChanged callback returns this state."
            },
            {
                "relayStateConnecting": "1: The SDK tries to relay the media stream to the destination channel."
            },
            {
                "relayStateRunning": "2: The SDK successfully relays the media stream to the destination channel."
            },
            {
                "relayStateFailure": "3: An error occurs. See code in onChannelMediaRelayStateChanged for the error code."
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "class_videoframe_ng",
        "name": "VideoFrame",
        "description": "Video frame information.\nThe video data format is YUV420. The buffer provides a pointer to a pointer. This interface cannot modify the pointer of the buffer but can modify the content of the buffer.",
        "parameters": [
            {
                "type": ""
            },
            {
                "width": "Video pixel width."
            },
            {
                "height": "Video pixel height."
            },
            {
                "yStride": "For YUV data, the line span of the Y buffer; for RGBA data, the total data length."
            },
            {
                "uStride": "For YUV data, the line span of the U buffer; for RGBA data, the value is 0."
            },
            {
                "vStride": "For YUV data, the line span of the V buffer; for RGBA data, the value is 0."
            },
            {
                "yBuffer": "For YUV data, the pointer to the Y buffer; for RGBA data, the data buffer."
            },
            {
                "uBuffer": "For YUV data, the pointer to the U buffer; for RGBA data, the value is NULL."
            },
            {
                "vBuffer": "For YUV data, the pointer to the V buffer; for RGBA data, the value is 0."
            },
            {
                "rotation": "The clockwise rotation angle of the video frame before rendering. The supported values are 0, 90, 180, or 270 degrees."
            },
            {
                "renderTimeMs": "The Unix timestamp (ms) when the video frame is rendered. This timestamp can be used to guide the rendering of the video frame. It is required."
            },
            {
                "avsync_type": "Reserved for future use."
            }
        ],
        "returns": "",
        "is_hide": true
    },
    {
        "id": "api_stoprhythmplayer",
        "name": "stopRhythmPlayer",
        "description": "Disables the virtual metronome.\nAfter calling startRhythmPlayer , you can call this method to disable the virtual metronome.\n This method is for Android and iOS only.",
        "parameters": [],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "api_sendmetadata",
        "name": "sendMetaData",
        "description": "Sends media affiliate information.\nIf the media attachment information is successfully sent, the receiver will receive the onMetadataReceived callback.",
        "parameters": [
            {
                "metadata": "Media metadata. See Metadata ."
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "api_enablelocalvideo",
        "name": "enableLocalVideo",
        "description": "Enables/Disables the local video capture.\nThis method disables or re-enables the local video capturer, and does not affect receiving the remote video stream.\n After calling enableVideo , the local video capturer is enabled by default. You can call enableLocalVideo (false) to disable the local video capturer. If you want to re-enable the local video, call enableLocalVideo(true).\n After the local video capturer is successfully disabled or re-enabled, the SDK triggers the onRemoteVideoStateChanged callback on the remote client. You can call this method either before or after joining a channel.\n This method enables the internal engine and is valid after leaving the channel.",
        "parameters": [
            {
                "enabled": "Whether to enable the local video capture. true: (Default) Enable the local video capture.\n false: Disables the local video capture. Once the local video is disabled, the remote users can no longer receive the video stream of this user, while this user can still receive the video streams of the other remote users. falseWhen set to , this method does not require a local camera.\n "
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "class_audiospectrumdata",
        "name": "AudioSpectrumData",
        "description": "The audio spectrum data.\n",
        "parameters": [
            {
                "audioSpectrumData": "The audio spectrum data. Agora divides the audio frequency into 160 frequency domains, and reports the energy value of each frequency domain through this parameter. The value range of each energy type is [0, 1].\n "
            },
            {
                "dataLength": "The length of the audio spectrum data in byte.\n "
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "api_icloudspatialaudioengine_getteammates",
        "name": "getTeammates",
        "description": "Gets the information of teammates.\nAfter calling setTeamId to set the team ID and calling enterRoom to enter the room, you can call this method to get the information of remote users in the same team (teammates).",
        "parameters": [
            {
                "uids": "Output parameter. The user IDs of teammates."
            }
        ],
        "returns": "",
        "is_hide": true
    },
    {
        "id": "api_unloadeffect",
        "name": "unloadEffect",
        "description": "Releases a specified preloaded audio effect from the memory.\n",
        "parameters": [
            {
                "soundId": "The audio effect ID. The ID of each audio effect file is unique."
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "api_getaudiomixingpublishvolume",
        "name": "getAudioMixingPublishVolume",
        "description": "Retrieves the audio mixing volume for publishing.\nThis method helps troubleshoot audio volume‑related issues.\n You need to call this method after calling startAudioMixing and receiving the onAudioMixingStateChanged(audioMixingStatePlaying) callback.",
        "parameters": [],
        "returns": "≥ 0: The audio mixing volume, if this method call succeeds. The value range is [0,100].\n < 0: Failure.",
        "is_hide": false
    },
    {
        "id": "api_setupremotevideo",
        "name": "setupRemoteVideo",
        "description": "Initializes the video view of a remote user.\nThis method initializes the video view of a remote stream on the local device. It affects only the video view that the local user sees. Call this method to bind the remote video stream to a video view and to set the rendering and mirror modes of the video view.\n You need to specify the ID of the remote user in this method. If the remote user ID is unknown to the application, set it after the app receives the onUserJoined callback.\n To unbind the remote user from the view, set the view parameter to NULL.\n Once the remote user leaves the channel, the SDK unbinds the remote user. To update the rendering or mirror mode of the remote video view during a call, use the setRemoteRenderMode method.\n If you use the Agora recording feature, the recording client joins the channel as a dummy client, triggering the onUserJoined callback. Do not bind the dummy client to the app view because the dummy client does not send any video streams. If your app does not recognize the dummy client, bind the remote user to the view when the SDK triggers the onFirstRemoteVideoDecoded callback.",
        "parameters": [
            {
                "canvas": "The remote video view and settings. See VideoCanvas .\n "
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "api_muteallremotevideostreams",
        "name": "muteAllRemoteVideoStreams",
        "description": "Stops or resumes subscribing to the video streams of all remote users.\nAfter successfully calling this method, the local user stops or resumes subscribing to the audio streams of all remote users, including all subsequent users. Call this method after joining a channel.",
        "parameters": [
            {
                "mute": "Whether to stop subscribing to the video streams of all remote users.\n true: Stop subscribing to the video streams of all remote users.\n false: (Default) Subscribe to the audio streams of all remote users by default. "
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "enum_videocodecprofiletype",
        "name": "VIDEO_CODEC_PROFILE_TYPE",
        "description": "Video codec profile types.\n",
        "parameters": [
            {
                "VIDEO_CODEC_PROFILE_BASELINE": "66: Baseline video codec profile. Generally used for video calls on mobile phones."
            },
            {
                "VIDEO_CODEC_PROFILE_MAIN": "77: Main video codec profile. Generally used in mainstream electronics such as MP4 players, portable video players, PSP, and iPads."
            },
            {
                "VIDEO_CODEC_PROFILE_HIGH": "100: (Default) High video codec profile. Generally used in high-resolution live streaming or television."
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "api_setbeautyeffectoptions",
        "name": "setBeautyEffectOptions",
        "description": "Sets the image enhancement options.\nEnables or disables image enhancement, and sets the options.\n Enabling the image enhancement function involves a series of method calls. The calling sequence is as follows:\n Call loadExtensionProvider (libagora_video_process_extension.dll) during RtcEngine initialization to specify the extension library path.\n Call enableExtension (agora, beauty, true) to enable the extension.\n Call enableVideo to enable the video module.\n Call this method to enable the image enhancement function.",
        "parameters": [
            {
                "type": "The type of the video source, see MediaSourceType ."
            },
            {
                "enabled": "Whether to enable the image enhancement function:\n true: Enable the image enhancement function.\n false: (Default) Disable the image enhancement function.\n "
            },
            {
                "options": "The image enhancement options. See BeautyOptions ."
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "api_imediaplayer_setaudiopitch",
        "name": "setAudioPitch",
        "description": "Sets the pitch of the current media resource.\nCall this method after calling open .",
        "parameters": [
            {
                "pitch": "Sets the pitch of the local music file by the chromatic scale. The default value is 0, which means keeping the original pitch. The value ranges from -12 to 12, and the pitch value between consecutive values is a chromatic value. The greater the absolute value of this parameter, the higher or lower the pitch of the local music file."
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "class_screencapturesourceinfo",
        "name": "ScreenCaptureSourceInfo",
        "description": "The information about the specified shareable window or screen. \n",
        "parameters": [
            {
                "type": "The type of the shared target. See ScreenCaptureSourceType ."
            },
            {
                "sourceId": "The window ID for a window or the display ID for a screen."
            },
            {
                "sourceName": "The name of the window or screen. UTF-8 encoding."
            },
            {
                "thumbImage": "The image content of the thumbnail. See ThumbImageBuffer ."
            },
            {
                "iconImage": "The image content of the icon. See ThumbImageBuffer ."
            },
            {
                "processPath": "The process to which the window belongs. UTF-8 encoding."
            },
            {
                "sourceTitle": "The title of the window. UTF-8 encoding."
            },
            {
                "primaryMonitor": "Determines whether the screen is the primary display:\n true: The screen is the primary display.\n false: The screen is not the primary display.\n "
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "api_joinchannelwithuseraccountex",
        "name": "joinChannelWithUserAccountEx",
        "description": "Joins the channel with a user account, and configures whether to automatically subscribe to audio or video streams after joining the channel.\nTo ensure smooth communication, use the same parameter type to identify the user. For example, if a user joins the channel with a user ID, then ensure all the other users use the user ID too. The same applies to the user account. If a user joins the channel with the Agora Web SDK, ensure that the ID of the user is set to the same parameter type. Once a user joins the channel, the user subscribes to the audio and video streams of all the other users in the channel by default, giving rise to usage and billing calculation. To stop subscribing to a specified stream or all remote streams, call the corresponding mute methods.\n This method allows a user to join the channel with the user account. After the user successfully joins the channel, the SDK triggers the following callbacks:\n The local client: onLocalUserRegistered , onJoinChannelSuccess and onConnectionStateChanged callbacks.\n The remote client: The onUserJoined callback if the user is in the COMMUNICATION profile, and the onUserInfoUpdated callback if the user is a host in the LIVE_BROADCASTING profile.",
        "parameters": [
            {
                "userAccount": "The user account. This parameter is used to identify the user in the channel for real-time audio and video engagement. You need to set and manage user accounts yourself and ensure that each user account in the same channel is unique. The maximum length of this parameter is 255 bytes. Ensure that you set this parameter and do not set it as NULL. Supported characters are (89 in total):\n The 26 lowercase English letters: a to z.\n The 26 uppercase English letters: A to Z.\n All numeric characters: 0 to 9.\n Space\n \"!\", \"#\", \"$\", \"%\", \"&amp;\", \"(\", \")\", \"+\", \"-\", \":\", \";\", \"&lt;\", \"= \", \".\", \"&gt;\", \"?\", \"@\", \"[\", \"]\", \"^\", \"_\", \"{\", \"}\", \"|\", \"~\", \",\" "
            },
            {
                "token": "The token generated on your server for authentication. See \n "
            },
            {
                "channelId": "The channel name. This parameter signifies the channel in which users engage in real-time audio and video interaction. Under the premise of the same App ID, users who fill in the same channel ID enter the same channel for audio and video interaction. The string length must be less than 64 bytes. Supported characters:\n All lowercase English letters: a to z.\n All uppercase English letters: A to Z.\n All numeric characters: 0 to 9.\n Space\n \"!\", \"#\", \"$\", \"%\", \"&amp;\", \"(\", \")\", \"+\", \"-\", \":\", \";\", \"&lt;\", \"= \", \".\", \"&gt;\", \"?\", \"@\", \"[\", \"]\", \"^\", \"_\", \"{\", \"}\", \"|\", \"~\", \",\" "
            },
            {
                "options": "The channel media options. See ChannelMediaOptions .\n "
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "enum_videocapturetype",
        "name": "VIDEO_CAPTURE_TYPE",
        "description": "The capture type of the custom video source.\n",
        "parameters": [
            {
                "VIDEO_CAPTURE_UNKNOWN": "Unknown type."
            },
            {
                "VIDEO_CAPTURE_CAMERA": "(Default) Video captured by the camera."
            },
            {
                "VIDEO_CAPTURE_SCREEN": "Video for screen sharing."
            }
        ],
        "returns": "",
        "is_hide": true
    },
    {
        "id": "callback_ivideoframeobserver_onframe",
        "name": "onFrame",
        "description": "Occurs each time the player receives a video frame.\nAfter registering the video frame observer, the callback occurs every time the player receives a video frame, reporting the detailed information of the video frame.",
        "parameters": [],
        "returns": "",
        "is_hide": true
    },
    {
        "id": "callback_onvideosubscribestatechanged",
        "name": "onVideoSubscribeStateChanged",
        "description": "Occurs when the video subscribing state changes.\n",
        "parameters": [
            {
                "channel": "The channel name."
            },
            {
                "uid": "The user ID of the remote user."
            },
            {
                "oldState": "The previous subscribing status. See StreamSubscribeState ."
            },
            {
                "newState": "The current subscribing status. See StreamSubscribeState."
            },
            {
                "elapseSinceLastState": "The time elapsed (ms) from the previous state to the current state."
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "api_setlogfilesize_ng",
        "name": "setLogFileSize",
        "description": "Sets the log file size.\nDeprecated:\n Use the logConfig parameter in initialize instead. By default, the SDK generates five SDK log files and five API call log files with the following rules: The SDK log files are: agorasdk.log, agorasdk.1.log, agorasdk.2.log, agorasdk.3.log, and agorasdk.4.log.\n The API call log files are: agoraapi.log, agoraapi.1.log, agoraapi.2.log, agoraapi.3.log, and agoraapi.4.log.\n The default size for each SDK log file is 1,024 KB; the default size for each API call log file is 2,048 KB. These log files are encoded in UTF-8.\n The SDK writes the latest logs in agorasdk.log or agoraapi.log.\n When agorasdk.log is full, the SDK processes the log files in the following order:\n Delete the agorasdk.4.log file (if any).\n Rename agorasdk.3.log to agorasdk.4.log.\n Rename agorasdk.2.log to agorasdk.3.log.\n Rename agorasdk.1.log to agorasdk.2.log.\n Create a new agorasdk.log file. The overwrite rules for the agoraapi.log file are the same as for agorasdk.log. This method is used to set the size of the agorasdk.log file only and does not effect the agoraapi.log file.",
        "parameters": [
            {
                "fileSizeInKBytes": "The size (KB) of an agorasdk.log file. The value range is [128,20480]. The default value is 1,024 KB. If you set fileSizeInKByte smaller than 128 KB, the SDK automatically adjusts it to 128 KB; if you set fileSizeInKByte greater than 20,480 KB, the SDK automatically adjusts it to 20,480 KB.\n "
            }
        ],
        "returns": "",
        "is_hide": false
    },
    {
        "id": "enum_errorcode",
        "name": "ErrorCode",
        "description": "Error codes. See https://docs.agora.io/en/Interactive%20Broadcast/error_rtc.",
        "parameters": [],
        "returns": ""
    },
    {
        "id": "enum_warningcode",
        "name": "WarningCode",
        "description": "Warning codes. See https://docs.agora.io/en/Interactive%20Broadcast/error_rtc.",
        "parameters": [],
        "returns": ""
    }
]